{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff05dc12",
   "metadata": {},
   "source": [
    "# Investment Simulation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81fbe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfc3d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_ta_classic in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.3.36)\n",
      "Requirement already satisfied: numpy>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.1.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2025.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (1.16.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas>=2.0.0->pandas_ta_classic) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas_ta_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3585fefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cdfa47",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca09993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03634598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, matthews_corrcoef,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed36532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f745496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the problematic import line in squeeze_pro.py\n"
     ]
    }
   ],
   "source": [
    "pandas_ta_classic_path = None\n",
    "for sp in site.getsitepackages():\n",
    "    pandas_ta_classic_path = os.path.join(sp, 'pandas_ta_classic')\n",
    "    if os.path.exists(pandas_ta_classic_path):\n",
    "        break\n",
    "\n",
    "if pandas_ta_classic_path:\n",
    "    squeeze_pro_path = os.path.join(pandas_ta_classic_path, 'momentum', 'squeeze_pro.py')\n",
    "    if os.path.exists(squeeze_pro_path):\n",
    "        try:\n",
    "            with open(squeeze_pro_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            new_lines = []\n",
    "            fixed = False\n",
    "            for line in lines:\n",
    "                if \"from numpy import NaN as npNaN\" in line:\n",
    "                    new_lines.append(line.replace(\"from numpy import NaN as npNaN\", \"# from numpy import NaN as npNaN\\nimport numpy as np\\n\"))\n",
    "                    fixed = True\n",
    "                    print(\"Modified import statement in squeeze_pro.py\")\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "\n",
    "            if fixed:\n",
    "                with open(squeeze_pro_path, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "                print(\"Successfully patched pandas_ta_classic/momentum/squeeze_pro.py\")\n",
    "            else:\n",
    "                print(\"Could not find the problematic import line in squeeze_pro.py\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error modifying squeeze_pro.py: {e}\")\n",
    "    else:\n",
    "        print(f\"Could not find squeeze_pro.py at {squeeze_pro_path}\")\n",
    "else:\n",
    "    print(\"Could not find the pandas_ta_classic library installation path.\")\n",
    "\n",
    "import pandas_ta_classic as ta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1ecbb",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60191d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"trained_models/\"\n",
    "MIN_SEQUENCE_LENGTH = 12  # Minimum sequence length for any company\n",
    "MAX_SEQUENCE_LENGTH = 12  # Maximum sequence length to cap computational cost\n",
    "INITIAL_TRAINING_DAYS = 1100  # Number of days to use for initial training only\n",
    "KELLY_FRACTION = 0.1\n",
    "SECTOR_CONFIDENCE_THRESHOLD = 0.30\n",
    "RETRAIN_INTERVAL = 200\n",
    "MAX_DAY_GAP = 5  # Maximum allowed gap in trading days (to account for weekends/holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade05e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOTAL_UTILIZATION      = 0.70   # never deploy more than 70% of capital at once\n",
    "MAX_NEW_DAILY_UTILIZATION  = 0.30   # new trades today ≤ 30% of capital\n",
    "MAX_TICKER_UTILIZATION     = 0.20   # per ticker cap (sum of all overlapping trades)\n",
    "MAX_SECTOR_UTILIZATION     = 0.45   # per sector cap (sum across its tickers)\n",
    "MAX_POSITION_SIZE          = 0.1   # single-trade cap\n",
    "\n",
    "MAX_POSITIONS_PER_TICKER   = 3      # ladder depth per ticker\n",
    "MIN_TRADE_DOLLARS          = 100.0  # skip dust trades\n",
    "\n",
    "KELLY_TRADE_CAP            = 0.30   # cap raw Kelly per trade (before other caps)\n",
    "DRAWDOWN_THROTTLE_LEVELS   = [(0.05, 0.50), (0.10, 0.25)]  # (peak_dd, utilization_multiplier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f113a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def set_global_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0c573",
   "metadata": {},
   "source": [
    "#### Define Horizon Target\n",
    "For a horizon H, compute both the direction and H-day return off the same base day t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7665bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_horizon_targets(df: pd.DataFrame, H: int, price_col='close_price') -> pd.DataFrame:\n",
    "    df = df.sort_values(['ticker','date']).copy()\n",
    "    df[f'ret_{H}d']    = df.groupby('ticker')[price_col].shift(-H) / df[price_col] - 1.0\n",
    "    df[f'target_{H}d'] = (df[f'ret_{H}d'] > 0).astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153b3ef",
   "metadata": {},
   "source": [
    "#### Identify Contiguous Periods\n",
    "Groups rows for a ticker into blocks where data gaps never exceed MAX_DAY_GAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088bb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_contiguous_periods(df: pd.DataFrame, max_gap_days: int = MAX_DAY_GAP) -> list:\n",
    "\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    dates = pd.to_datetime(df['date'])\n",
    "\n",
    "    contiguous_periods = []\n",
    "    start_idx = 0\n",
    "\n",
    "    for i in range(1, len(dates)):\n",
    "        gap = (dates[i] - dates[i-1]).days\n",
    "        if gap > max_gap_days:\n",
    "            # End current period and start new one\n",
    "            contiguous_periods.append((start_idx, i-1))\n",
    "            start_idx = i\n",
    "\n",
    "\n",
    "    contiguous_periods.append((start_idx, len(dates)-1))\n",
    "\n",
    "\n",
    "    contiguous_periods = [(s, e) for s, e in contiguous_periods if e - s >= MIN_SEQUENCE_LENGTH]\n",
    "\n",
    "    return contiguous_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c42a1",
   "metadata": {},
   "source": [
    "#### Calculate Dynamic Sequence Length \n",
    "Pick a sequence length tailored to the amount of history available for a ticker.\n",
    "Avoids hard-coding a single window size for short vs long histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac04ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dynamic_sequence_length(company_data_df: pd.DataFrame,\n",
    "                                     min_length: int = MIN_SEQUENCE_LENGTH,\n",
    "                                     max_length: int = MAX_SEQUENCE_LENGTH,\n",
    "                                     target_fraction: float = 0.15) -> int:\n",
    "\n",
    "    total_days = len(company_data_df)\n",
    "\n",
    "\n",
    "    dynamic_length = int(total_days * target_fraction)\n",
    "    dynamic_length = max(min_length, min(dynamic_length, max_length))\n",
    "\n",
    "    return dynamic_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e35512",
   "metadata": {},
   "source": [
    "#### Create Contiguous Sequences\n",
    "Turn each contiguous period into overlapping (sequence_length) windows (X) and next-step labels (y).\n",
    "Produces the actual training/validation tensors for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24c2a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contiguous_sequences(data: np.ndarray, targets: np.ndarray,\n",
    "                               contiguous_periods: list, sequence_length: int, H: int):\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for start_idx, end_idx in contiguous_periods:\n",
    "        usable_end = end_idx - H\n",
    "        period_length = usable_end - start_idx + 1\n",
    "        if period_length < sequence_length + 1:\n",
    "            continue\n",
    "\n",
    "        for i in range(start_idx + sequence_length -1 , usable_end + 1):\n",
    "            X.append(data[i-sequence_length + 1 : i +1])\n",
    "            y.append(targets[i + H])\n",
    "\n",
    "    return np.array(X) if X else np.array([]), np.array(y) if y else np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb35cd",
   "metadata": {},
   "source": [
    "#### Create Target Variable\n",
    "Build the binary classification target per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0802af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variable(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    print(\"Creating target variable...\")\n",
    "    df = df.sort_values(by=['ticker', 'date']).copy()\n",
    "    df['next_day_close'] = df.groupby('ticker')['close_price'].shift(-1)\n",
    "    df['target'] = (df['next_day_close'] > df['close_price']).astype(int)\n",
    "    df.dropna(subset=['next_day_close'], inplace=True)\n",
    "    df['target'] = df['target'].astype(int)\n",
    "    print(\"Target variable created.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec4141",
   "metadata": {},
   "source": [
    "#### Track Current Exposures from Open Trades\n",
    "Settle exits first each day, then compute exposure books before opening new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a94acbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_exposures(open_positions, sectors_by_ticker):\n",
    "    expo_ticker = defaultdict(float)\n",
    "    expo_sector = defaultdict(float)\n",
    "    total = 0.0\n",
    "    for pos in open_positions:\n",
    "        amt = pos['invest']\n",
    "        tkr = pos['ticker']\n",
    "        sec = sectors_by_ticker.get(tkr, 'UNKNOWN')\n",
    "        expo_ticker[tkr] += amt\n",
    "        expo_sector[sec] += amt\n",
    "        total += amt\n",
    "    return total, dict(expo_ticker), dict(expo_sector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b9c88",
   "metadata": {},
   "source": [
    "#### Throttle Book When in Drawdown \n",
    "Apply multiplicative haircut to utilisation caps based on current peak to trough drawdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "516f715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utilization_throttle(equity_curve):\n",
    "    if not equity_curve:\n",
    "        return 1.0\n",
    "    peak = max(equity_curve)\n",
    "    curr = equity_curve[-1]\n",
    "    dd = 0.0 if peak == 0 else (peak - curr)/peak\n",
    "    mult = 1.0\n",
    "    for level, m in DRAWDOWN_THROTTLE_LEVELS:\n",
    "        if dd >= level:\n",
    "            mult = min(mult, m)\n",
    "    return mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dacbec2",
   "metadata": {},
   "source": [
    "#### Cap Each Proposed Trade\n",
    "When sizing a new trade, clamp it by all remaining budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdeea8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_by_caps(proposed_amt, capital, \n",
    "                  total_open, new_today_open, expo_ticker, expo_sector,\n",
    "                  ticker, sector,\n",
    "                  util_mult=1.0):\n",
    "    # remaining dollar budgets\n",
    "    rem_total  = MAX_TOTAL_UTILIZATION*util_mult*capital - total_open\n",
    "    rem_new    = MAX_NEW_DAILY_UTILIZATION*util_mult*capital - new_today_open\n",
    "    rem_ticker = MAX_TICKER_UTILIZATION*util_mult*capital - expo_ticker.get(ticker, 0.0)\n",
    "    rem_sector = MAX_SECTOR_UTILIZATION*util_mult*capital - expo_sector.get(sector, 0.0)\n",
    "    per_trade  = MAX_POSITION_SIZE*util_mult*capital\n",
    "\n",
    "    # clamp\n",
    "    max_affordable = max(0.0, min(proposed_amt, rem_total, rem_new, rem_ticker, rem_sector, per_trade))\n",
    "    return max_affordable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f740f8",
   "metadata": {},
   "source": [
    "#### Calculate Historical Payouts\n",
    "Estimate average upside “b” per winning trade for Kelly sizing. \n",
    "\n",
    "Only looks at wins; losses are implicit in Kelly’s (1-p)/b term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6935f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_historical_payouts(df):\n",
    "    \"\"\"\n",
    "    Returns a dict per ticker with:\n",
    "      - 'b': avg_win / avg_loss_abs  (Kelly 'odds' ratio)\n",
    "      - 'avg_win': mean positive next-day return when target==1\n",
    "      - 'avg_loss_abs': mean absolute negative next-day return when target==0\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for t, g in df.groupby('ticker'):\n",
    "        # next_day return already implicit in your create_target_variable\n",
    "        r = (g['next_day_close'] - g['close_price']) / g['close_price']\n",
    "        wins = r[g['target'] == 1]\n",
    "        losses = r[g['target'] == 0]\n",
    "        avg_win = wins[wins > 0].mean() if (wins > 0).any() else np.nan\n",
    "        avg_loss_abs = (-losses[losses < 0]).mean() if (losses < 0).any() else np.nan\n",
    "\n",
    "        if np.isfinite(avg_win) and np.isfinite(avg_loss_abs) and avg_loss_abs > 0:\n",
    "            b = avg_win / avg_loss_abs\n",
    "        else:\n",
    "            b = np.nan\n",
    "\n",
    "        out[t] = {'b': b, 'avg_win': avg_win, 'avg_loss_abs': avg_loss_abs}\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fd47a",
   "metadata": {},
   "source": [
    "##### PyTorch Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e8b7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, n_features, hidden1=128, hidden2=64, fc=32, dropout=0.3, inter_dropout=0.1, use_layernorm=False):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=n_features, hidden_size=hidden1,\n",
    "            num_layers=1, batch_first=True, bidirectional=False, dropout=0.1\n",
    "        )\n",
    "        self.inter_drop = nn.Dropout(p=inter_dropout)  # proxy for recurrent_dropout\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=hidden1, hidden_size=hidden2,\n",
    "            num_layers=1, batch_first=True, bidirectional=False, dropout=0.1\n",
    "        )\n",
    "\n",
    "        # Normalization after temporal pooling\n",
    "        self.use_layernorm = bool(use_layernorm)\n",
    "        if self.use_layernorm:\n",
    "            self.norm = nn.LayerNorm(normalized_shape=hidden2)\n",
    "        else:\n",
    "            self.norm = nn.BatchNorm1d(num_features=hidden2)\n",
    "\n",
    "        # Head\n",
    "        self.fc1 = nn.Linear(hidden2, fc)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.fc_out = nn.Linear(fc, 1)  # logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out1, _ = self.lstm1(x)     # (B, T, hidden1)\n",
    "        out1 = self.inter_drop(out1)\n",
    "        out2, _ = self.lstm2(out1)  # (B, T, hidden2)\n",
    "\n",
    "        last = out2[:, -1, :]       # (B, hidden2)\n",
    "        if isinstance(self.norm, nn.BatchNorm1d):\n",
    "            last = self.norm(last)  # BN expects (B, C)\n",
    "        else:\n",
    "            last = self.norm(last)  # LN expects (B, C)\n",
    "\n",
    "        z = self.fc1(last)\n",
    "        z = self.relu(z)\n",
    "        z = self.drop(z)\n",
    "        logits = self.fc_out(z).squeeze(-1)  # (B,)\n",
    "        return logits\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=15, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_metric = None\n",
    "        self.best_state_dict = None\n",
    "        self.mode = mode  # 'min' for val_loss\n",
    "    def step(self, metric, model):\n",
    "        if self.best_metric is None:\n",
    "            improved = True\n",
    "        else:\n",
    "            improved = (metric < self.best_metric) if self.mode == 'min' else (metric > self.best_metric)\n",
    "        if improved:\n",
    "            self.best_metric = metric\n",
    "            self.best_state_dict = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return improved\n",
    "\n",
    "@torch.no_grad()\n",
    "def _evaluate(model, loader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_logits = []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits))  # sigmoid\n",
    "    return avg_loss, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15476a3",
   "metadata": {},
   "source": [
    "#### Train Company Models\n",
    "Train, calibrate, and persist a per-ticker classifier.\n",
    "This encapsulates per-asset modelling with proper scaling and probability calibration, which is crucial since Kelly needs probabilities (not raw logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3f408a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_company_models(company_data_df: pd.DataFrame,\n",
    "                         ticker: str,\n",
    "                         feature_cols: list,\n",
    "                         model_save_path: str,\n",
    "                         sequence_length: int = None,\n",
    "                         H = 1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #sequence length & guards\n",
    "    if sequence_length is None:\n",
    "        sequence_length = calculate_dynamic_sequence_length(company_data_df)\n",
    "\n",
    "    if len(company_data_df) < sequence_length + H + 10:\n",
    "        print(f\"Not enough data for {ticker}.\")\n",
    "        return False, sequence_length\n",
    "\n",
    "    contiguous_periods = identify_contiguous_periods(company_data_df)\n",
    "    if not contiguous_periods:\n",
    "        print(f\"No contiguous periods found for {ticker}.\")\n",
    "        return False, sequence_length\n",
    "\n",
    "    #build sequences\n",
    "    targets = company_data_df[f'target_{H}d'].values\n",
    "    X, y = create_contiguous_sequences(\n",
    "        company_data_df[feature_cols].values,\n",
    "        targets,\n",
    "        contiguous_periods,\n",
    "        sequence_length,\n",
    "        H\n",
    "    )\n",
    "    print(f\"Created {len(X)} sequences for {ticker}.\")\n",
    "    if len(X) < 2:\n",
    "        return False, sequence_length\n",
    "\n",
    "    #chronological split (no shuffle)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False\n",
    "    )\n",
    "\n",
    "    #Scale features (fit on train, transform both)\n",
    "    scaler = MinMaxScaler()\n",
    "    F = X_train.shape[-1]\n",
    "    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, F)).reshape(X_train.shape)\n",
    "    X_val_scaled   = scaler.transform(  X_val.reshape(-1, F)).reshape(X_val.shape)\n",
    "\n",
    "    # Keras fit used the scaled data\n",
    "    train_ds = SequenceDataset(X_train_scaled, y_train)\n",
    "    val_ds   = SequenceDataset(X_val_scaled,   y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  drop_last=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "    n_features = X_train.shape[-1]\n",
    "    model = LSTMClassifier(n_features=n_features, dropout=0.3, inter_dropout=0.1).to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # clipnorm via grad clipping below\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-7)\n",
    "    early = EarlyStopper(patience=15, mode='min')\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clipnorm=1.0\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        val_loss, _ = _evaluate(model, val_loader, device, loss_fn)\n",
    "\n",
    "        scheduler.step(val_loss)  # ReduceLROnPlateau on val_loss\n",
    "\n",
    "        # EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        _ = early.step(val_loss, model)\n",
    "        if early.counter >= early.patience:  # stop after 'patience' non-improve epochs\n",
    "            break\n",
    "\n",
    "    # Restore best weights\n",
    "    if early.best_state_dict is not None:\n",
    "        model.load_state_dict(early.best_state_dict)\n",
    "\n",
    "    #final validation pass for isotonic calibration (on scaled val data)\n",
    "    _, validation_predictions = _evaluate(model, val_loader, device, loss_fn)\n",
    "\n",
    "    # Keras code used 10% buffer (comment said 5%)\n",
    "    pred_min, pred_max = validation_predictions.min(), validation_predictions.max()\n",
    "    buffer = 0.05 * (pred_max - pred_min)\n",
    "    y_min_dynamic = max(0.0, pred_min - buffer)\n",
    "    y_max_dynamic = min(1.0, pred_max + buffer)\n",
    "\n",
    "    calibrator = IsotonicRegression(y_min=y_min_dynamic, y_max=y_max_dynamic, out_of_bounds='clip')\n",
    "    calibrator.fit(validation_predictions, y_val.astype(float))\n",
    "\n",
    "    # inside train_company_models(..., H=H)\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_path, f\"{ticker}_lstm_H{H}.pt\"))\n",
    "    joblib.dump(calibrator,      os.path.join(model_save_path, f\"{ticker}_calibrator_H{H}.pkl\"))\n",
    "    joblib.dump(scaler,          os.path.join(model_save_path, f\"{ticker}_scaler_H{H}.pkl\"))\n",
    "    joblib.dump(sequence_length, os.path.join(model_save_path, f\"{ticker}_seq_length_H{H}.pkl\"))\n",
    "\n",
    "\n",
    "    del model, scaler, calibrator, train_loader, val_loader, train_ds, val_ds\n",
    "    del X_train, X_val, y_train, y_val, X_train_scaled, X_val_scaled\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return True, sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38900d2",
   "metadata": {},
   "source": [
    "#### Predict Next Day Performance \n",
    "One-step-ahead inference for a ticker on a given day.\n",
    "\n",
    "Feeds calibrated p(up) into the portfolio/Kelly step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0cc2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_horizon(company_data_df: pd.DataFrame,\n",
    "                                 ticker: str,\n",
    "                                 feature_cols: list,\n",
    "                                 model_save_path: str,\n",
    "                                 H: int) -> dict | None:\n",
    "\n",
    "    try:\n",
    "        # Load artifacts\n",
    "        state_dict_path = os.path.join(model_save_path, f\"{ticker}_lstm_H{H}.pt\")\n",
    "        calibrator = joblib.load(os.path.join(model_save_path, f\"{ticker}_calibrator_H{H}.pkl\"))\n",
    "        scaler     = joblib.load(os.path.join(model_save_path, f\"{ticker}_scaler_H{H}.pkl\"))\n",
    "        sequence_length    = joblib.load(os.path.join(model_save_path, f\"{ticker}_seq_length_H{H}.pkl\"))\n",
    "\n",
    "\n",
    "        if not os.path.isfile(state_dict_path):\n",
    "            return None\n",
    "        \n",
    "        if hasattr(scaler, \"n_features_in_\") and scaler.n_features_in_ != len(feature_cols):\n",
    "            return None\n",
    "\n",
    "        # Build model skeleton with correct input dim, then load weights\n",
    "        n_features = len(feature_cols)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = LSTMClassifier(n_features=n_features).to(device)\n",
    "        model.load_state_dict(torch.load(state_dict_path, map_location=device))\n",
    "        model.eval()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # Find the last contiguous block and take the most recent sequence_length rows\n",
    "    contiguous_periods = identify_contiguous_periods(company_data_df)\n",
    "    if not contiguous_periods:\n",
    "        return None\n",
    "\n",
    "    last_start, last_end = contiguous_periods[-1]\n",
    "    period_data = company_data_df.iloc[last_start:last_end + 1]\n",
    "    if len(period_data) < sequence_length:\n",
    "        return None\n",
    "\n",
    "    last_sequence = period_data.tail(sequence_length)\n",
    "\n",
    "    # Scale using the saved scaler (fit  train only)\n",
    "    # Ensure column order matches training\n",
    "    try:\n",
    "        scaled_features = scaler.transform(last_sequence[feature_cols])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # Shape: (1, T, F) as float32 tensor\n",
    "    x = torch.from_numpy(np.asarray([scaled_features], dtype=np.float32)).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)                  # shape (1,)\n",
    "        prob = torch.sigmoid(logits).item()  # scalar in [0,1]\n",
    "\n",
    "    # Isotonic calibration (expects iterable)\n",
    "    calibrated = float(calibrator.predict([prob])[0])\n",
    "\n",
    "    print(f\"Raw prediction for {ticker}: {prob}\")\n",
    "    print(f\"Calibrated prediction for {ticker}: {calibrated}\")\n",
    "\n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'raw_prediction': prob,\n",
    "        'calibrated_prediction': calibrated\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ddf66a",
   "metadata": {},
   "source": [
    "#### Select and Size Portfolio\n",
    "Turn a cross-section of calibrated predictions into position sizes.\n",
    "\n",
    "Enforces diversification (one per sector) and risk discipline (Fractional Kelly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "242c2207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_size_portfolio(daily_predictions_df: pd.DataFrame, payout_map: dict,\n",
    "                            total_capital: float, sector_threshold: float,\n",
    "                            kelly_fraction: float) -> pd.DataFrame:\n",
    "\n",
    "    investment_decisions = []\n",
    "\n",
    "    if daily_predictions_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for sector, group in daily_predictions_df.groupby('sector'):\n",
    "        avg_sector_score = group['calibrated_prediction'].mean()\n",
    "        if avg_sector_score < sector_threshold:\n",
    "            continue\n",
    "\n",
    "        best_stock_in_sector = group.loc[group['calibrated_prediction'].idxmax()]\n",
    "        ticker = best_stock_in_sector['ticker']\n",
    "        p = best_stock_in_sector['calibrated_prediction']\n",
    "        b_info = payout_map.get(ticker, 0.0)\n",
    "        b = (b_info['b'] if isinstance(b_info, dict) else float(b_info))\n",
    "        if not np.isfinite(b) or b <= 0: \n",
    "            continue\n",
    "        \n",
    "        if not b_info or not np.isfinite(b_info.get('b', np.nan)): \n",
    "            continue\n",
    "        b = max(b_info['b'], 1e-6)  # avoid div-by-zero\n",
    "        kelly_percentage = p - (1 - p) / b\n",
    "\n",
    "        if kelly_percentage > 0:\n",
    "            investment_fraction = kelly_percentage * kelly_fraction\n",
    "            investment_amount = total_capital * investment_fraction\n",
    "\n",
    "            print(f\"Investing {investment_amount} in {ticker}\")\n",
    "\n",
    "            investment_decisions.append({\n",
    "                'ticker': ticker,\n",
    "                'investment_fraction': investment_fraction,\n",
    "                'investment_amount': investment_amount,\n",
    "                'predicted_prob': p\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(investment_decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dfa9e",
   "metadata": {},
   "source": [
    "#### Models Exist for Ticker\n",
    "Quick guard to avoid retraining if a ticker’s artifacts already exist.\n",
    "\n",
    "Decide whether to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ec476fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_exist_for_ticker(ticker: str, model_path: str, H: int = 1) -> bool:\n",
    "    files = [\n",
    "        f\"{ticker}_lstm_H{H}.pt\",\n",
    "        f\"{ticker}_calibrator_H{H}.pkl\",\n",
    "        f\"{ticker}_scaler_H{H}.pkl\",\n",
    "        f\"{ticker}_seq_length_H{H}.pkl\",\n",
    "    ]\n",
    "    return all(os.path.exists(os.path.join(model_path, f)) for f in files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea644bc8",
   "metadata": {},
   "source": [
    "#### Run Simulation\n",
    "The “engine” — walks forward over dates, trains as needed, predicts, sizes, books PnL, and updates capital.\n",
    "\n",
    "Enforces chronological integrity, periodic retraining, diversification, and proper capital tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f51d7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(master_df: pd.DataFrame, payout_map: dict, feature_cols: list,\n",
    "                   initial_capital: float, initial_training_days: int = INITIAL_TRAINING_DAYS,\n",
    "                   H: int = 1, allow_overlap: bool = True):\n",
    "\n",
    "    print(f\"\\nStarting simulation with {initial_training_days} initial training days... (H={H})\")\n",
    "    capital = initial_capital\n",
    "    equity_curve = [capital]\n",
    "    simulation_log = []\n",
    "\n",
    "    # Precompute horizon returns if not already present\n",
    "    ret_col = f\"ret_{H}d\"\n",
    "    tgt_col = f\"target_{H}d\"\n",
    "    if ret_col not in master_df.columns or tgt_col not in master_df.columns:\n",
    "        print(\"Adding horizon targets to master_df...\")\n",
    "        master_df = add_horizon_targets(master_df, H=H, price_col='close_price')\n",
    "\n",
    "    sectors_by_ticker = master_df.groupby('ticker')['sector'].first().to_dict()\n",
    "\n",
    "    all_tickers = master_df['ticker'].unique()\n",
    "    retrain_counter = {ticker: RETRAIN_INTERVAL for ticker in all_tickers}\n",
    "    ticker_sequence_lengths = {}\n",
    "\n",
    "    unique_dates = sorted(master_df['date'].unique())\n",
    "    start_index = min(initial_training_days, len(unique_dates) - 1)\n",
    "    print(f\"Starting predictions from day {start_index} (after {initial_training_days} training days)\")\n",
    "\n",
    "    # Overlapping book\n",
    "    open_positions = []  # each: {'ticker','invest','entry_date','exit_date'}\n",
    "    prev_equity_end = capital\n",
    "\n",
    "    for i in tqdm(range(start_index, len(unique_dates)), desc=\"Simulating Trading Days\"):\n",
    "        current_date = unique_dates[i]\n",
    "\n",
    "        # settle positions exiting today\n",
    "        total_pnl_today = 0.0\n",
    "        still_open = []\n",
    "        for pos in open_positions:\n",
    "            if pos['exit_date'] == current_date:\n",
    "                # realize H-day return measured from entry date\n",
    "                row = master_df[(master_df['date'] == pos['entry_date']) & (master_df['ticker'] == pos['ticker'])]\n",
    "                if not row.empty:\n",
    "                    rH = row.iloc[0][ret_col]\n",
    "                    if pd.notna(rH):\n",
    "                        pnl = pos['invest'] * rH\n",
    "                        total_pnl_today += pnl\n",
    "                        capital += pos['invest'] + pnl  # release capital\n",
    "                # else: silently drop if missing\n",
    "            else:\n",
    "                still_open.append(pos)\n",
    "        open_positions = still_open\n",
    "\n",
    "        # equity at START (after settlements, before new entries)\n",
    "        open_notional_start = sum(p['invest'] for p in open_positions)\n",
    "        equity_start = capital + open_notional_start\n",
    "        daily_return = (equity_start / prev_equity_end) - 1.0 if prev_equity_end > 0 else 0.0\n",
    "\n",
    "        # build prediction universe using info strictly before current_date\n",
    "        historical_data = master_df[master_df['date'] < current_date]\n",
    "        if i == 0:  # no \"yesterday\" slice for display\n",
    "            todays_data_for_prediction = pd.DataFrame(columns=master_df.columns)\n",
    "        else:\n",
    "            todays_data_for_prediction = master_df[master_df['date'] == unique_dates[i-1]]\n",
    "\n",
    "        daily_predictions = []\n",
    "        for ticker in todays_data_for_prediction['ticker'].unique():\n",
    "            company_hist_data = historical_data[historical_data['ticker'] == ticker]\n",
    "            if company_hist_data.empty:\n",
    "                continue\n",
    "\n",
    "            need_retrain = retrain_counter.get(ticker, 0) >= RETRAIN_INTERVAL\n",
    "            have_models  = models_exist_for_ticker(ticker, MODEL_SAVE_PATH, H=H)\n",
    "            if need_retrain or not have_models:\n",
    "                training_success, seq_length = train_company_models(\n",
    "                    company_hist_data, ticker, feature_cols, MODEL_SAVE_PATH, H=H\n",
    "                )\n",
    "                if training_success:\n",
    "                    ticker_sequence_lengths[ticker] = seq_length\n",
    "                    retrain_counter[ticker] = 0\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            pred = predict_next_horizon(company_hist_data, ticker, feature_cols, MODEL_SAVE_PATH, H=H)\n",
    "            if pred:\n",
    "                info = todays_data_for_prediction[todays_data_for_prediction['ticker'] == ticker].iloc[0]\n",
    "                pred.update({'company_name': info['company_name'], 'sector': info['sector']})\n",
    "                daily_predictions.append(pred)\n",
    "                retrain_counter[ticker] += 1\n",
    "\n",
    "        daily_predictions_df = pd.DataFrame(daily_predictions)\n",
    "\n",
    "        # size raw trades via Kelly\n",
    "        investment_decision_df = select_and_size_portfolio(\n",
    "            daily_predictions_df, payout_map, capital, SECTOR_CONFIDENCE_THRESHOLD, KELLY_FRACTION\n",
    "        )\n",
    "\n",
    "        # entry logic (allow_overlap controls laddering)\n",
    "        exit_idx = i + H\n",
    "        if exit_idx < len(unique_dates):\n",
    "            exit_date = unique_dates[exit_idx]\n",
    "        else:\n",
    "            exit_date = None  # can't open new trades that we cannot exit within data\n",
    "\n",
    "        entries_today = []\n",
    "        if exit_date is not None and not investment_decision_df.empty:\n",
    "            # strongest first to grab budgets\n",
    "            investment_decision_df = investment_decision_df.sort_values('predicted_prob', ascending=False)\n",
    "\n",
    "            # current exposures (post-settlement)\n",
    "            util_mult = utilization_throttle(equity_curve)\n",
    "            total_open, expo_ticker, expo_sector = compute_exposures(open_positions, sectors_by_ticker)\n",
    "            new_today_open = 0.0\n",
    "\n",
    "            for _, trade in investment_decision_df.iterrows():\n",
    "                tkr = trade['ticker']\n",
    "                sec = sectors_by_ticker.get(tkr, 'UNKNOWN')\n",
    "\n",
    "                # per-ticker ladder depth\n",
    "                open_count = sum(1 for p in open_positions if p['ticker'] == tkr)\n",
    "                if allow_overlap:\n",
    "                    if open_count >= MAX_POSITIONS_PER_TICKER:\n",
    "                        continue\n",
    "                else:\n",
    "                    if open_count >= 1:\n",
    "                        continue  # no overlapping allowed\n",
    "\n",
    "                # base size from Kelly (already scaled by KELLY_FRACTION)\n",
    "                base_amt = float(trade['investment_amount'])\n",
    "                base_amt = min(base_amt, KELLY_TRADE_CAP * (capital + sum(p['invest'] for p in open_positions)))  # vs equity\n",
    "\n",
    "                # clamp by remaining budgets\n",
    "                amt = clamp_by_caps(base_amt, (capital + sum(p['invest'] for p in open_positions)),\n",
    "                                    total_open, new_today_open, expo_ticker, expo_sector,\n",
    "                                    tkr, sec, util_mult)\n",
    "                if amt < MIN_TRADE_DOLLARS:\n",
    "                    continue\n",
    "\n",
    "                # lock capital and record position\n",
    "                capital -= amt\n",
    "                total_open += amt\n",
    "                new_today_open += amt\n",
    "                expo_ticker[tkr] = expo_ticker.get(tkr, 0.0) + amt\n",
    "                expo_sector[sec] = expo_sector.get(sec, 0.0) + amt\n",
    "\n",
    "                pos = {'ticker': tkr, 'invest': amt, 'entry_date': current_date, 'exit_date': exit_date}\n",
    "                open_positions.append(pos)\n",
    "                entries_today.append({**trade.to_dict(), 'allocated_amount': amt, 'exit_date': exit_date})\n",
    "\n",
    "        # equity at END (after entries)\n",
    "        open_notional_end = sum(p['invest'] for p in open_positions)\n",
    "        equity_end = capital + open_notional_end\n",
    "\n",
    "        # log the day\n",
    "        simulation_log.append({\n",
    "            'date': current_date,\n",
    "            'capital_start': equity_start - open_notional_start,  # cash only\n",
    "            'capital_end': capital,                               # cash only\n",
    "            'equity_start': equity_start,\n",
    "            'equity_end': equity_end,\n",
    "            'daily_pnl_realized': total_pnl_today,                # realized PnL today\n",
    "            'daily_return': daily_return,\n",
    "            'investments_made': entries_today\n",
    "        })\n",
    "\n",
    "        equity_curve.append(equity_end)\n",
    "        prev_equity_end = equity_end\n",
    "\n",
    "    return pd.DataFrame(simulation_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71833949",
   "metadata": {},
   "source": [
    "#### Calculate Final Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8ec6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_results(simulation_log: pd.DataFrame, initial_capital: float, rf_annual: float = 0.0):\n",
    "    if simulation_log.empty:\n",
    "        print(\"Simulation log is empty. No results to calculate.\")\n",
    "        return\n",
    "\n",
    "    use_equity = {'equity_start', 'equity_end'}.issubset(simulation_log.columns)\n",
    "\n",
    "    # precomputed daily_return if available and sane\n",
    "    if 'daily_return' in simulation_log.columns:\n",
    "        daily_returns = simulation_log['daily_return'].astype(float).values\n",
    "    elif use_equity:\n",
    "        eq_start = simulation_log['equity_start'].astype(float).values\n",
    "        eq_end   = simulation_log['equity_end'].astype(float).values\n",
    "        # r_t = equity_start_t / equity_end_{t-1} - 1\n",
    "        prev_end = np.roll(eq_end, 1)\n",
    "        prev_end[0] = eq_end[0] if eq_end[0] != 0 else eq_start[0]\n",
    "        daily_returns = (eq_start / prev_end) - 1.0\n",
    "    else:\n",
    "        # cash-based (not ideal for H>0)\n",
    "        print(\"Warning: Using cash-based returns\")\n",
    "        cap_start = simulation_log['capital_start'].astype(float).values\n",
    "        cap_end   = simulation_log['capital_end'].astype(float).values\n",
    "        prev_end  = np.roll(cap_end, 1)\n",
    "        prev_end[0] = cap_end[0] if cap_end[0] != 0 else cap_start[0]\n",
    "        daily_returns = (cap_start / prev_end) - 1.0\n",
    "\n",
    "    daily_returns = np.where(np.isfinite(daily_returns), daily_returns, 0.0)\n",
    "\n",
    "    # ROI from final equity if present, else cash\n",
    "    end_col = 'equity_end' if use_equity else 'capital_end'\n",
    "    final_value = float(simulation_log[end_col].iloc[-1])\n",
    "    total_roi = (final_value / initial_capital) - 1.0\n",
    "\n",
    "    # Sharpe\n",
    "    excess = daily_returns - (rf_annual / 252.0)\n",
    "    vol = excess.std(ddof=1)\n",
    "    sharpe_ratio = (excess.mean() / vol * np.sqrt(252.0)) if vol > 1e-12 else 0.0\n",
    "\n",
    "    print(\"\\n--- Simulation Results ---\")\n",
    "    print(f\"Initial Capital: ${initial_capital:,.2f}\")\n",
    "    print(f\"Final Capital:   ${final_value:,.2f}\")\n",
    "    print(f\"Total Return on Investment (ROI): {total_roi:.2%}\")\n",
    "    print(f\"Annualized Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(\"--------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "027ac6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of master_df before dropping NaNs: (108592, 21)\n",
      "Shape of master_df after dropping NaNs: (108592, 21)\n"
     ]
    }
   ],
   "source": [
    "companies = pd.read_parquet('stocknet-dataset/stock_table.parquet')\n",
    "tweets = pd.read_parquet('stocknet-dataset/stock_tweets_withsentiment_withemotion_withstance_nomerge.parquet')\n",
    "stocks = pd.read_parquet('stocknet-dataset/stock_prices.parquet')\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "companies.columns = [x.lower() for x in companies.columns]\n",
    "tweets.columns = [x.lower() for x in tweets.columns]\n",
    "stocks.columns = [x.lower() for x in stocks.columns]\n",
    "\n",
    "tweets['stance_positive'] = (tweets['stance_label'] == 'Positive').astype(int)\n",
    "tweets['stance_negative'] = (tweets['stance_label'] == 'Negative').astype(int)\n",
    "\n",
    "tweets_merged = tweets.groupby(['date', 'ticker'], as_index=False).agg({\n",
    "    'text': lambda x: ' '.join(x),\n",
    "    'sentiment': lambda x: x.mean(),\n",
    "    'emotion_anger': 'sum',\n",
    "    'emotion_disgust': 'sum',\n",
    "    'emotion_fear': 'sum',\n",
    "    'emotion_joy': 'sum',\n",
    "    'emotion_neutral': 'sum',\n",
    "    'emotion_sadness': 'sum',\n",
    "    'emotion_surprize': 'sum',\n",
    "    'stance_positive': 'sum',\n",
    "    'stance_negative': 'sum'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweets_merged['date'] = pd.to_datetime(tweets_merged['date'])\n",
    "stocks['date'] = pd.to_datetime(stocks['date'])\n",
    "\n",
    "\"\"\"\n",
    "master_df = stocks.merge(\n",
    "    tweets_merged,\n",
    "    on=[\"date\", \"ticker\"]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "master_df = pd.merge(\n",
    "    stocks,\n",
    "    tweets_merged,\n",
    "    on=[\"date\", \"ticker\"],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "tweet_feature_cols = ['sentiment', 'emotion_anger', 'emotion_disgust', 'emotion_fear', 'emotion_joy', 'emotion_neutral', 'emotion_sadness', 'emotion_surprize', 'stance_positive', 'stance_negative']\n",
    "for col in tweet_feature_cols:\n",
    "    if col in master_df.columns:\n",
    "        master_df[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "master_df = pd.merge(master_df, companies[['ticker', 'sector', 'company']], on='ticker', how='left')\n",
    "\n",
    "\n",
    "feature_cols = ['open','high','low','volume']\n",
    "\n",
    "master_df = master_df.rename(columns={'close': 'close_price', 'company': 'company_name'})\n",
    "\n",
    "\n",
    "print(f\"Shape of master_df before dropping NaNs: {master_df.shape}\")\n",
    "#master_df.dropna(inplace=True)\n",
    "print(f\"Shape of master_df after dropping NaNs: {master_df.shape}\")\n",
    "\n",
    "master_df.rename(columns={'close_price': 'close'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "master_df.sort_values(by=['ticker', 'date'], inplace=True)\n",
    "\n",
    "\n",
    "def apply_ta_indicators(df_group):\n",
    "    df_group.set_index(pd.DatetimeIndex(df_group['date']), inplace=True)\n",
    "    #Trend\n",
    "    df_group.ta.ema(length=12, append=True)\n",
    "    df_group.ta.ema(length=26, append=True)\n",
    "    df_group.ta.ema(length=50, append=True)\n",
    "\n",
    "    df_group.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "\n",
    "\n",
    "\n",
    "    df_group.ta.rsi(length=14, append=True)\n",
    "    df_group.ta.stochrsi(length=14, append=True)\n",
    "\n",
    "\n",
    "    df_group.ta.atr(length=14, append=True)\n",
    "\n",
    "    bb = ta.bbands(df_group['close'], length=20, std=2)\n",
    "    df_group['BB_upper'] = bb['BBU_20_2.0']\n",
    "    df_group['BB_middle'] = bb['BBM_20_2.0']\n",
    "    df_group['BB_lower'] = bb['BBL_20_2.0']\n",
    "\n",
    "\n",
    "    df_group.ta.obv(append=True)\n",
    "    return df_group.reset_index(drop=True)\n",
    "\n",
    "master_df = master_df.groupby('ticker').apply(apply_ta_indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44548929",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.drop(columns=['text','adj close','sentiment','emotion_anger','emotion_disgust','emotion_fear','emotion_joy','emotion_neutral','emotion_sadness','emotion_surprize'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78ea1351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>stance_positive</th>\n",
       "      <th>stance_negative</th>\n",
       "      <th>sector</th>\n",
       "      <th>...</th>\n",
       "      <th>MACDh_12_26_9</th>\n",
       "      <th>MACDs_12_26_9</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>STOCHRSIk_14_14_3_3</th>\n",
       "      <th>STOCHRSId_14_14_3_3</th>\n",
       "      <th>ATRr_14</th>\n",
       "      <th>BB_upper</th>\n",
       "      <th>BB_middle</th>\n",
       "      <th>BB_lower</th>\n",
       "      <th>OBV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AAPL</th>\n",
       "      <th>0</th>\n",
       "      <td>2012-09-04</td>\n",
       "      <td>95.108574</td>\n",
       "      <td>96.448570</td>\n",
       "      <td>94.928574</td>\n",
       "      <td>96.424286</td>\n",
       "      <td>91973000.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91973000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-09-05</td>\n",
       "      <td>96.510002</td>\n",
       "      <td>96.621429</td>\n",
       "      <td>95.657143</td>\n",
       "      <td>95.747147</td>\n",
       "      <td>84093800.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7879200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-09-06</td>\n",
       "      <td>96.167145</td>\n",
       "      <td>96.898575</td>\n",
       "      <td>95.828575</td>\n",
       "      <td>96.610001</td>\n",
       "      <td>97799100.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105678300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-09-07</td>\n",
       "      <td>96.864288</td>\n",
       "      <td>97.497147</td>\n",
       "      <td>96.538574</td>\n",
       "      <td>97.205711</td>\n",
       "      <td>82416600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188094900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-09-10</td>\n",
       "      <td>97.207146</td>\n",
       "      <td>97.612854</td>\n",
       "      <td>94.585716</td>\n",
       "      <td>94.677139</td>\n",
       "      <td>121999500.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66095400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">XOM</th>\n",
       "      <th>1253</th>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>76.940002</td>\n",
       "      <td>76.260002</td>\n",
       "      <td>76.470001</td>\n",
       "      <td>8229700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107548</td>\n",
       "      <td>-0.972858</td>\n",
       "      <td>31.975492</td>\n",
       "      <td>35.117121</td>\n",
       "      <td>31.775404</td>\n",
       "      <td>0.786087</td>\n",
       "      <td>81.525829</td>\n",
       "      <td>78.2435</td>\n",
       "      <td>74.961171</td>\n",
       "      <td>-268825100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>76.209999</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.080002</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>7060400.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069077</td>\n",
       "      <td>-0.990127</td>\n",
       "      <td>31.851847</td>\n",
       "      <td>48.597552</td>\n",
       "      <td>38.712818</td>\n",
       "      <td>0.759224</td>\n",
       "      <td>81.303475</td>\n",
       "      <td>78.0575</td>\n",
       "      <td>74.811525</td>\n",
       "      <td>-275885500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>76.239998</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>76.059998</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>8218000.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054652</td>\n",
       "      <td>-1.003790</td>\n",
       "      <td>29.688704</td>\n",
       "      <td>55.025431</td>\n",
       "      <td>46.246701</td>\n",
       "      <td>0.732850</td>\n",
       "      <td>80.964170</td>\n",
       "      <td>77.8325</td>\n",
       "      <td>74.700830</td>\n",
       "      <td>-284103500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>76.269997</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.050003</td>\n",
       "      <td>76.330002</td>\n",
       "      <td>15641700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018917</td>\n",
       "      <td>-1.008519</td>\n",
       "      <td>32.913052</td>\n",
       "      <td>73.940933</td>\n",
       "      <td>59.187972</td>\n",
       "      <td>0.711932</td>\n",
       "      <td>80.569554</td>\n",
       "      <td>77.6245</td>\n",
       "      <td>74.679446</td>\n",
       "      <td>-268461800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>76.849998</td>\n",
       "      <td>76.320000</td>\n",
       "      <td>76.570000</td>\n",
       "      <td>7340800.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028984</td>\n",
       "      <td>-1.001273</td>\n",
       "      <td>36.200731</td>\n",
       "      <td>85.986580</td>\n",
       "      <td>71.650981</td>\n",
       "      <td>0.698937</td>\n",
       "      <td>80.167620</td>\n",
       "      <td>77.4425</td>\n",
       "      <td>74.717380</td>\n",
       "      <td>-261121000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108592 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date       open       high        low      close  \\\n",
       "ticker                                                               \n",
       "AAPL   0    2012-09-04  95.108574  96.448570  94.928574  96.424286   \n",
       "       1    2012-09-05  96.510002  96.621429  95.657143  95.747147   \n",
       "       2    2012-09-06  96.167145  96.898575  95.828575  96.610001   \n",
       "       3    2012-09-07  96.864288  97.497147  96.538574  97.205711   \n",
       "       4    2012-09-10  97.207146  97.612854  94.585716  94.677139   \n",
       "...                ...        ...        ...        ...        ...   \n",
       "XOM    1253 2017-08-28  76.900002  76.940002  76.260002  76.470001   \n",
       "       1254 2017-08-29  76.209999  76.489998  76.080002  76.449997   \n",
       "       1255 2017-08-30  76.239998  76.449997  76.059998  76.099998   \n",
       "       1256 2017-08-31  76.269997  76.489998  76.050003  76.330002   \n",
       "       1257 2017-09-01  76.370003  76.849998  76.320000  76.570000   \n",
       "\n",
       "                  volume ticker  stance_positive  stance_negative  \\\n",
       "ticker                                                              \n",
       "AAPL   0      91973000.0   AAPL              0.0              0.0   \n",
       "       1      84093800.0   AAPL              0.0              0.0   \n",
       "       2      97799100.0   AAPL              0.0              0.0   \n",
       "       3      82416600.0   AAPL              0.0              0.0   \n",
       "       4     121999500.0   AAPL              0.0              0.0   \n",
       "...                  ...    ...              ...              ...   \n",
       "XOM    1253    8229700.0    XOM              0.0              0.0   \n",
       "       1254    7060400.0    XOM              0.0              0.0   \n",
       "       1255    8218000.0    XOM              0.0              0.0   \n",
       "       1256   15641700.0    XOM              0.0              0.0   \n",
       "       1257    7340800.0    XOM              0.0              0.0   \n",
       "\n",
       "                       sector  ... MACDh_12_26_9  MACDs_12_26_9     RSI_14  \\\n",
       "ticker                         ...                                           \n",
       "AAPL   0       Consumer Goods  ...           NaN            NaN        NaN   \n",
       "       1       Consumer Goods  ...           NaN            NaN        NaN   \n",
       "       2       Consumer Goods  ...           NaN            NaN        NaN   \n",
       "       3       Consumer Goods  ...           NaN            NaN        NaN   \n",
       "       4       Consumer Goods  ...           NaN            NaN        NaN   \n",
       "...                       ...  ...           ...            ...        ...   \n",
       "XOM    1253  Basic Matierials  ...     -0.107548      -0.972858  31.975492   \n",
       "       1254  Basic Matierials  ...     -0.069077      -0.990127  31.851847   \n",
       "       1255  Basic Matierials  ...     -0.054652      -1.003790  29.688704   \n",
       "       1256  Basic Matierials  ...     -0.018917      -1.008519  32.913052   \n",
       "       1257  Basic Matierials  ...      0.028984      -1.001273  36.200731   \n",
       "\n",
       "             STOCHRSIk_14_14_3_3  STOCHRSId_14_14_3_3   ATRr_14   BB_upper  \\\n",
       "ticker                                                                       \n",
       "AAPL   0                     NaN                  NaN       NaN        NaN   \n",
       "       1                     NaN                  NaN       NaN        NaN   \n",
       "       2                     NaN                  NaN       NaN        NaN   \n",
       "       3                     NaN                  NaN       NaN        NaN   \n",
       "       4                     NaN                  NaN       NaN        NaN   \n",
       "...                          ...                  ...       ...        ...   \n",
       "XOM    1253            35.117121            31.775404  0.786087  81.525829   \n",
       "       1254            48.597552            38.712818  0.759224  81.303475   \n",
       "       1255            55.025431            46.246701  0.732850  80.964170   \n",
       "       1256            73.940933            59.187972  0.711932  80.569554   \n",
       "       1257            85.986580            71.650981  0.698937  80.167620   \n",
       "\n",
       "             BB_middle   BB_lower          OBV  \n",
       "ticker                                          \n",
       "AAPL   0           NaN        NaN   91973000.0  \n",
       "       1           NaN        NaN    7879200.0  \n",
       "       2           NaN        NaN  105678300.0  \n",
       "       3           NaN        NaN  188094900.0  \n",
       "       4           NaN        NaN   66095400.0  \n",
       "...                ...        ...          ...  \n",
       "XOM    1253    78.2435  74.961171 -268825100.0  \n",
       "       1254    78.0575  74.811525 -275885500.0  \n",
       "       1255    77.8325  74.700830 -284103500.0  \n",
       "       1256    77.6245  74.679446 -268461800.0  \n",
       "       1257    77.4425  74.717380 -261121000.0  \n",
       "\n",
       "[108592 rows x 25 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5e11eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['EMA_12', 'EMA_26','EMA_50','MACD_12_26_9','MACDh_12_26_9','MACDs_12_26_9','RSI_14','ATRr_14','STOCHRSIk_14_14_3_3','STOCHRSId_14_14_3_3','ATRr_14','BB_upper','BB_middle','BB_lower','OBV']\n",
    "master_df = master_df.dropna(subset=columns_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cb9cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature columns: ['open', 'high', 'low', 'volume', 'stance_positive', 'stance_negative', 'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3', 'BB_upper', 'BB_middle', 'BB_lower', 'OBV']\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['open','high','low','volume',\n",
    "                'stance_positive','stance_negative'\n",
    "                ]\n",
    "\n",
    "new_indicator_columns = [\n",
    "    'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',\n",
    "    'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3',\n",
    "    'BB_upper', 'BB_middle', 'BB_lower', 'OBV'\n",
    "]\n",
    "feature_cols.extend(new_indicator_columns)\n",
    "print(f\"Final feature columns: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6153429",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = master_df.rename(columns={'close': 'close_price', 'company': 'company_name'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e26ecd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close_price</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>stance_positive</th>\n",
       "      <th>stance_negative</th>\n",
       "      <th>sector</th>\n",
       "      <th>...</th>\n",
       "      <th>MACDh_12_26_9</th>\n",
       "      <th>MACDs_12_26_9</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>STOCHRSIk_14_14_3_3</th>\n",
       "      <th>STOCHRSId_14_14_3_3</th>\n",
       "      <th>ATRr_14</th>\n",
       "      <th>BB_upper</th>\n",
       "      <th>BB_middle</th>\n",
       "      <th>BB_lower</th>\n",
       "      <th>OBV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-11-14</td>\n",
       "      <td>77.928574</td>\n",
       "      <td>78.207146</td>\n",
       "      <td>76.597145</td>\n",
       "      <td>76.697144</td>\n",
       "      <td>119292600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538077</td>\n",
       "      <td>-3.703588</td>\n",
       "      <td>25.771012</td>\n",
       "      <td>21.054629</td>\n",
       "      <td>19.582354</td>\n",
       "      <td>2.377852</td>\n",
       "      <td>94.648550</td>\n",
       "      <td>84.401357</td>\n",
       "      <td>74.154164</td>\n",
       "      <td>-1.014356e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-11-15</td>\n",
       "      <td>76.790001</td>\n",
       "      <td>77.071426</td>\n",
       "      <td>74.660004</td>\n",
       "      <td>75.088570</td>\n",
       "      <td>197477700.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537725</td>\n",
       "      <td>-3.838019</td>\n",
       "      <td>23.573491</td>\n",
       "      <td>15.792949</td>\n",
       "      <td>19.993462</td>\n",
       "      <td>2.380310</td>\n",
       "      <td>93.761634</td>\n",
       "      <td>83.514428</td>\n",
       "      <td>73.267223</td>\n",
       "      <td>-1.211834e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>75.028572</td>\n",
       "      <td>75.714287</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>75.382858</td>\n",
       "      <td>316723400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455544</td>\n",
       "      <td>-3.951905</td>\n",
       "      <td>24.836267</td>\n",
       "      <td>12.243346</td>\n",
       "      <td>16.363641</td>\n",
       "      <td>2.459547</td>\n",
       "      <td>92.716200</td>\n",
       "      <td>82.679214</td>\n",
       "      <td>72.642228</td>\n",
       "      <td>-8.951103e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-19</td>\n",
       "      <td>77.244286</td>\n",
       "      <td>81.071426</td>\n",
       "      <td>77.125717</td>\n",
       "      <td>80.818573</td>\n",
       "      <td>205829400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>-3.951213</td>\n",
       "      <td>43.429047</td>\n",
       "      <td>39.692073</td>\n",
       "      <td>22.576123</td>\n",
       "      <td>2.695187</td>\n",
       "      <td>91.617665</td>\n",
       "      <td>82.201285</td>\n",
       "      <td>72.784906</td>\n",
       "      <td>-6.892809e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-11-20</td>\n",
       "      <td>81.701431</td>\n",
       "      <td>81.707146</td>\n",
       "      <td>79.225716</td>\n",
       "      <td>80.129997</td>\n",
       "      <td>160688500.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281964</td>\n",
       "      <td>-3.880722</td>\n",
       "      <td>42.011353</td>\n",
       "      <td>69.373201</td>\n",
       "      <td>40.436207</td>\n",
       "      <td>2.679612</td>\n",
       "      <td>91.027780</td>\n",
       "      <td>81.851785</td>\n",
       "      <td>72.675790</td>\n",
       "      <td>-8.499694e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104215</th>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>76.940002</td>\n",
       "      <td>76.260002</td>\n",
       "      <td>76.470001</td>\n",
       "      <td>8229700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107548</td>\n",
       "      <td>-0.972858</td>\n",
       "      <td>31.975492</td>\n",
       "      <td>35.117121</td>\n",
       "      <td>31.775404</td>\n",
       "      <td>0.786087</td>\n",
       "      <td>81.525829</td>\n",
       "      <td>78.243500</td>\n",
       "      <td>74.961171</td>\n",
       "      <td>-2.688251e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104216</th>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>76.209999</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.080002</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>7060400.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069077</td>\n",
       "      <td>-0.990127</td>\n",
       "      <td>31.851847</td>\n",
       "      <td>48.597552</td>\n",
       "      <td>38.712818</td>\n",
       "      <td>0.759224</td>\n",
       "      <td>81.303475</td>\n",
       "      <td>78.057500</td>\n",
       "      <td>74.811525</td>\n",
       "      <td>-2.758855e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104217</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>76.239998</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>76.059998</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>8218000.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054652</td>\n",
       "      <td>-1.003790</td>\n",
       "      <td>29.688704</td>\n",
       "      <td>55.025431</td>\n",
       "      <td>46.246701</td>\n",
       "      <td>0.732850</td>\n",
       "      <td>80.964170</td>\n",
       "      <td>77.832500</td>\n",
       "      <td>74.700830</td>\n",
       "      <td>-2.841035e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104218</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>76.269997</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.050003</td>\n",
       "      <td>76.330002</td>\n",
       "      <td>15641700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018917</td>\n",
       "      <td>-1.008519</td>\n",
       "      <td>32.913052</td>\n",
       "      <td>73.940933</td>\n",
       "      <td>59.187972</td>\n",
       "      <td>0.711932</td>\n",
       "      <td>80.569554</td>\n",
       "      <td>77.624500</td>\n",
       "      <td>74.679446</td>\n",
       "      <td>-2.684618e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104219</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>76.849998</td>\n",
       "      <td>76.320000</td>\n",
       "      <td>76.570000</td>\n",
       "      <td>7340800.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Basic Matierials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028984</td>\n",
       "      <td>-1.001273</td>\n",
       "      <td>36.200731</td>\n",
       "      <td>85.986580</td>\n",
       "      <td>71.650981</td>\n",
       "      <td>0.698937</td>\n",
       "      <td>80.167620</td>\n",
       "      <td>77.442500</td>\n",
       "      <td>74.717380</td>\n",
       "      <td>-2.611210e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104220 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date       open       high        low  close_price       volume  \\\n",
       "0      2012-11-14  77.928574  78.207146  76.597145    76.697144  119292600.0   \n",
       "1      2012-11-15  76.790001  77.071426  74.660004    75.088570  197477700.0   \n",
       "2      2012-11-16  75.028572  75.714287  72.250000    75.382858  316723400.0   \n",
       "3      2012-11-19  77.244286  81.071426  77.125717    80.818573  205829400.0   \n",
       "4      2012-11-20  81.701431  81.707146  79.225716    80.129997  160688500.0   \n",
       "...           ...        ...        ...        ...          ...          ...   \n",
       "104215 2017-08-28  76.900002  76.940002  76.260002    76.470001    8229700.0   \n",
       "104216 2017-08-29  76.209999  76.489998  76.080002    76.449997    7060400.0   \n",
       "104217 2017-08-30  76.239998  76.449997  76.059998    76.099998    8218000.0   \n",
       "104218 2017-08-31  76.269997  76.489998  76.050003    76.330002   15641700.0   \n",
       "104219 2017-09-01  76.370003  76.849998  76.320000    76.570000    7340800.0   \n",
       "\n",
       "       ticker  stance_positive  stance_negative            sector  ...  \\\n",
       "0        AAPL              0.0              0.0    Consumer Goods  ...   \n",
       "1        AAPL              0.0              0.0    Consumer Goods  ...   \n",
       "2        AAPL              0.0              0.0    Consumer Goods  ...   \n",
       "3        AAPL              0.0              0.0    Consumer Goods  ...   \n",
       "4        AAPL              0.0              0.0    Consumer Goods  ...   \n",
       "...       ...              ...              ...               ...  ...   \n",
       "104215    XOM              0.0              0.0  Basic Matierials  ...   \n",
       "104216    XOM              0.0              0.0  Basic Matierials  ...   \n",
       "104217    XOM              0.0              0.0  Basic Matierials  ...   \n",
       "104218    XOM              0.0              0.0  Basic Matierials  ...   \n",
       "104219    XOM              0.0              0.0  Basic Matierials  ...   \n",
       "\n",
       "       MACDh_12_26_9  MACDs_12_26_9     RSI_14  STOCHRSIk_14_14_3_3  \\\n",
       "0          -0.538077      -3.703588  25.771012            21.054629   \n",
       "1          -0.537725      -3.838019  23.573491            15.792949   \n",
       "2          -0.455544      -3.951905  24.836267            12.243346   \n",
       "3           0.002769      -3.951213  43.429047            39.692073   \n",
       "4           0.281964      -3.880722  42.011353            69.373201   \n",
       "...              ...            ...        ...                  ...   \n",
       "104215     -0.107548      -0.972858  31.975492            35.117121   \n",
       "104216     -0.069077      -0.990127  31.851847            48.597552   \n",
       "104217     -0.054652      -1.003790  29.688704            55.025431   \n",
       "104218     -0.018917      -1.008519  32.913052            73.940933   \n",
       "104219      0.028984      -1.001273  36.200731            85.986580   \n",
       "\n",
       "        STOCHRSId_14_14_3_3   ATRr_14   BB_upper  BB_middle   BB_lower  \\\n",
       "0                 19.582354  2.377852  94.648550  84.401357  74.154164   \n",
       "1                 19.993462  2.380310  93.761634  83.514428  73.267223   \n",
       "2                 16.363641  2.459547  92.716200  82.679214  72.642228   \n",
       "3                 22.576123  2.695187  91.617665  82.201285  72.784906   \n",
       "4                 40.436207  2.679612  91.027780  81.851785  72.675790   \n",
       "...                     ...       ...        ...        ...        ...   \n",
       "104215            31.775404  0.786087  81.525829  78.243500  74.961171   \n",
       "104216            38.712818  0.759224  81.303475  78.057500  74.811525   \n",
       "104217            46.246701  0.732850  80.964170  77.832500  74.700830   \n",
       "104218            59.187972  0.711932  80.569554  77.624500  74.679446   \n",
       "104219            71.650981  0.698937  80.167620  77.442500  74.717380   \n",
       "\n",
       "                 OBV  \n",
       "0      -1.014356e+09  \n",
       "1      -1.211834e+09  \n",
       "2      -8.951103e+08  \n",
       "3      -6.892809e+08  \n",
       "4      -8.499694e+08  \n",
       "...              ...  \n",
       "104215 -2.688251e+08  \n",
       "104216 -2.758855e+08  \n",
       "104217 -2.841035e+08  \n",
       "104218 -2.684618e+08  \n",
       "104219 -2.611210e+08  \n",
       "\n",
       "[104220 rows x 25 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.reset_index(drop=True, inplace=True)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eeb47b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating target variable...\n",
      "Target variable created.\n",
      "\n",
      "Starting simulation with 365 initial training days... (H=3)\n",
      "Adding horizon targets to master_df...\n",
      "Starting predictions from day 365 (after 365 training days)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Trading Days:   0%|          | 0/843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 351 sequences for AAPL.\n",
      "Raw prediction for AAPL: 0.14428144693374634\n",
      "Calibrated prediction for AAPL: 0.19344249367713928\n",
      "Created 351 sequences for ABB.\n",
      "Raw prediction for ABB: 0.4676075875759125\n",
      "Calibrated prediction for ABB: 0.43724560737609863\n",
      "Created 270 sequences for ABBV.\n",
      "Raw prediction for ABBV: 0.4413568377494812\n",
      "Calibrated prediction for ABBV: 0.4337449073791504\n",
      "Created 351 sequences for AEP.\n",
      "Raw prediction for AEP: 0.4799320697784424\n",
      "Calibrated prediction for AEP: 0.5453822612762451\n",
      "Created 351 sequences for AMGN.\n",
      "Raw prediction for AMGN: 0.5599236488342285\n",
      "Calibrated prediction for AMGN: 0.5749375820159912\n",
      "Created 351 sequences for AMZN.\n",
      "Raw prediction for AMZN: 0.5295031666755676\n",
      "Calibrated prediction for AMZN: 0.5245636105537415\n",
      "Created 351 sequences for BA.\n",
      "Raw prediction for BA: 0.5095901489257812\n",
      "Calibrated prediction for BA: 0.5075602531433105\n",
      "Created 351 sequences for BAC.\n",
      "Raw prediction for BAC: 0.5424584746360779\n",
      "Calibrated prediction for BAC: 0.5833333134651184\n",
      "Created 351 sequences for BBL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating Trading Days:   0%|          | 0/843 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m payout_map \u001b[38;5;241m=\u001b[39m calculate_historical_payouts(master_df_with_target)\n\u001b[1;32m      5\u001b[0m initial_capital \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100_000.0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m simulation_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaster_df_with_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpayout_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_capital\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_training_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINITIAL_TRAINING_DAYS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m display(simulation_results\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m     17\u001b[0m calculate_final_results(simulation_results, initial_capital)\n",
      "Cell \u001b[0;32mIn[25], line 73\u001b[0m, in \u001b[0;36mrun_simulation\u001b[0;34m(master_df, payout_map, feature_cols, initial_capital, initial_training_days, H, allow_overlap)\u001b[0m\n\u001b[1;32m     71\u001b[0m have_models  \u001b[38;5;241m=\u001b[39m models_exist_for_ticker(ticker, MODEL_SAVE_PATH, H\u001b[38;5;241m=\u001b[39mH)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_retrain \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_models:\n\u001b[0;32m---> 73\u001b[0m     training_success, seq_length \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_company_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompany_hist_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m training_success:\n\u001b[1;32m     77\u001b[0m         ticker_sequence_lengths[ticker] \u001b[38;5;241m=\u001b[39m seq_length\n",
      "Cell \u001b[0;32mIn[21], line 66\u001b[0m, in \u001b[0;36mtrain_company_models\u001b[0;34m(company_data_df, ticker, feature_cols, model_save_path, sequence_length, H)\u001b[0m\n\u001b[1;32m     64\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 66\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, yb)\n\u001b[1;32m     68\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/micromamba/envs/df_ae2/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/df_ae2/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[20], line 36\u001b[0m, in \u001b[0;36mLSTMClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# x: (B, T, F)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     out1, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# (B, T, hidden1)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minter_drop(out1)\n\u001b[1;32m     38\u001b[0m     out2, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm2(out1)  \u001b[38;5;66;03m# (B, T, hidden2)\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/df_ae2/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/df_ae2/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/micromamba/envs/df_ae2/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1138\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1146\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "master_df_with_target = create_target_variable(master_df)\n",
    "master_df_with_target = add_horizon_targets(master_df_with_target, H=1)\n",
    "payout_map = calculate_historical_payouts(master_df_with_target)\n",
    "\n",
    "initial_capital = 100_000.0\n",
    "simulation_results = run_simulation(\n",
    "    master_df_with_target,\n",
    "    payout_map,\n",
    "    feature_cols,\n",
    "    initial_capital,\n",
    "    initial_training_days=INITIAL_TRAINING_DAYS,\n",
    "    H=3,\n",
    "    allow_overlap=True\n",
    ")\n",
    "\n",
    "display(simulation_results.head(100))\n",
    "calculate_final_results(simulation_results, initial_capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "363f9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of trained_models/ after deletion attempt:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'trained_models/'\n",
    "if os.path.exists(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "else:\n",
    "    print(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "print(f\"Contents of {folder_path} after deletion attempt:\")\n",
    "if os.path.exists(folder_path):\n",
    "    print(os.listdir(folder_path))\n",
    "else:\n",
    "    print(\"Folder does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
