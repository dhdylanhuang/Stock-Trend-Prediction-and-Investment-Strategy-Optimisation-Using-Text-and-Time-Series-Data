{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Results from Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgZ_ylAADNb6",
    "outputId": "6134cc02-4bf2-4dbb-a201-b8fd4d0df03c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_ta_classic in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.3.36)\n",
      "Requirement already satisfied: numpy>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.1.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2025.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (1.16.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas>=2.0.0->pandas_ta_classic) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas_ta_classic in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.3.36)\n",
      "Requirement already satisfied: numpy>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.1.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2025.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (1.16.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas>=2.0.0->pandas_ta_classic) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (21.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas_ta_classic\n",
    "%pip install --upgrade pandas_ta_classic\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "befeuglqHy8f",
    "outputId": "a2bcece3-930c-4c77-c6ff-3473d1fde791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the problematic import line in squeeze_pro.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import site\n",
    "import os\n",
    "\n",
    "# Find the path to the pandas_ta_classic library and patch it\n",
    "pandas_ta_classic_path = None\n",
    "for sp in site.getsitepackages():\n",
    "    pandas_ta_classic_path = os.path.join(sp, 'pandas_ta_classic')\n",
    "    if os.path.exists(pandas_ta_classic_path):\n",
    "        break\n",
    "\n",
    "if pandas_ta_classic_path:\n",
    "    squeeze_pro_path = os.path.join(pandas_ta_classic_path, 'momentum', 'squeeze_pro.py')\n",
    "    if os.path.exists(squeeze_pro_path):\n",
    "        try:\n",
    "            with open(squeeze_pro_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            new_lines = []\n",
    "            fixed = False\n",
    "            for line in lines:\n",
    "                if \"from numpy import NaN as npNaN\" in line:\n",
    "                    new_lines.append(line.replace(\"from numpy import NaN as npNaN\", \"# from numpy import NaN as npNaN\\nimport numpy as np\\n\"))\n",
    "                    fixed = True\n",
    "                    print(\"Modified import statement in squeeze_pro.py\")\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "\n",
    "            if fixed:\n",
    "                with open(squeeze_pro_path, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "                print(\"Successfully patched pandas_ta_classic/momentum/squeeze_pro.py\")\n",
    "            else:\n",
    "                print(\"Could not find the problematic import line in squeeze_pro.py\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error modifying squeeze_pro.py: {e}\")\n",
    "    else:\n",
    "        print(f\"Could not find squeeze_pro.py at {squeeze_pro_path}\")\n",
    "else:\n",
    "    print(\"Could not find the pandas_ta_classic library installation path.\")\n",
    "\n",
    "import pandas_ta_classic as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: torch==2.8.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torchvision) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install scikit-learn\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, matthews_corrcoef,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import scipy\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unicodedata import bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion(y_true, y_pred, labels=None, normalize=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize=normalize)\n",
    "    return cm\n",
    "\n",
    "def plot_confusion(cm, labels, title=\"Confusion Matrix\"):\n",
    "    fig, ax = plt.subplots(figsize=(8,10))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(ax=ax, values_format=\".2f\" if cm.dtype.kind=='f' else \"d\", cmap='Blues', colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def pct_str(x, decimals=1):\n",
    "    return f\"{x*100:.{decimals}f}%\"    \n",
    "    \n",
    "def edge_labels_from_edges(edges, decimals=1):\n",
    "    \"\"\"\n",
    "    Turn monotonic edges (len = C+1) into C human-readable labels, e.g.\n",
    "    [-0.023, -0.006, 0.001, 0.008] -> [\"≤ -2.3%\", \"(-2.3%,-0.6%]\", \"(-0.6%,0.1%]\"]\n",
    "    Uses left-closed, right-open for inner, with pretty endpoints.\n",
    "    \"\"\"\n",
    "    C = len(edges) - 1\n",
    "    labels = []\n",
    "    for i in range(C):\n",
    "        lo, hi = edges[i], edges[i+1]\n",
    "        if i == 0:\n",
    "            labels.append(f\"≤ {pct_str(hi, decimals)}\")\n",
    "        elif i == C-1:\n",
    "            labels.append(f\"> {pct_str(lo, decimals)}\")\n",
    "        else:\n",
    "            labels.append(f\"({pct_str(lo, decimals)},{pct_str(hi, decimals)}]\")\n",
    "    return labels\n",
    "    \n",
    "\n",
    "def aggregate_confusions_with_median_edges(results_df, n_classes=None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      agg_cm: (C x C) summed confusion matrix over companies\n",
    "      q_labels: ['Q1',...,'QC']\n",
    "      interval_labels: human-friendly labels built from *median* edges across companies\n",
    "      display_labels: ['Q1 ≤ -0.9%', 'Q2 (-0.9%,-0.2%]', ...]\n",
    "    \"\"\"\n",
    "    if results_df is None or results_df.empty:\n",
    "        return None, None, None, None\n",
    "\n",
    "    cm_list, edges_list = [], []\n",
    "    for _, row in results_df.iterrows():\n",
    "        cm = row.get('confusion_matrix', None)\n",
    "        edges = row.get('bucket_edges', None)\n",
    "        if cm is not None:\n",
    "            cm_arr = np.array(cm, dtype=float)\n",
    "            cm_list.append(cm_arr)\n",
    "        if edges is not None:\n",
    "            edges_arr = np.array(edges, dtype=float)\n",
    "            # Expect length C+1\n",
    "            edges_list.append(edges_arr)\n",
    "\n",
    "    if not cm_list:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Sum all confusion matrices\n",
    "    agg_cm = np.sum(cm_list, axis=0)\n",
    "\n",
    "    # Infer C if not provided\n",
    "    C_from_cm = agg_cm.shape[0]\n",
    "    C_from_edges = (edges_list[0].shape[0] - 1) if edges_list else None\n",
    "    if n_classes is None:\n",
    "        n_classes = C_from_edges if C_from_edges is not None else C_from_cm\n",
    "    else:\n",
    "        # sanity: if mismatch, trust agg_cm\n",
    "        if n_classes != C_from_cm:\n",
    "            n_classes = C_from_cm\n",
    "\n",
    "    # Median edges if available; otherwise a symmetric fallback around 0\n",
    "    if edges_list:\n",
    "        E = np.stack(edges_list, axis=0)         # [N, C+1]\n",
    "        median_edges = np.median(E, axis=0)\n",
    "    else:\n",
    "        # crude fallback: equal bins on a small range, just for labeling\n",
    "        lo, hi = -0.10, 0.10\n",
    "        median_edges = np.linspace(lo, hi, n_classes + 1)\n",
    "\n",
    "    # Build human labels and Q-labels\n",
    "    interval_labels = edge_labels_from_edges(median_edges, decimals=1)  # uses your helper\n",
    "    q_labels = [f\"Q{i}\" for i in range(1, n_classes + 1)]\n",
    "    display_labels = [f\"{q_labels[i]} {interval_labels[i]}\" for i in range(n_classes)]\n",
    "\n",
    "    return agg_cm, q_labels, interval_labels, display_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect logits for any loader (no labels) \n",
    "@torch.no_grad()\n",
    "def collect_logits(model, loader, device):\n",
    "    model.eval()\n",
    "    chunks = []\n",
    "    for xb, _ in loader:\n",
    "        xb = xb.to(device)\n",
    "        z  = model(xb)           # [B, K-1]\n",
    "        chunks.append(z.detach().cpu())\n",
    "    return torch.cat(chunks, dim=0)   # [N, K-1]\n",
    "\n",
    "# learn per-threshold taus on VAL (maximize macro-F1 over ordinal-decoded labels) \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_taus_per_threshold(Z_val, y_val_idx, grid=np.linspace(0.2, 0.8, 61)):\n",
    "    \"\"\"\n",
    "    Z_val: torch/numpy [N, K-1] logits from ordinal head on VAL\n",
    "    y_val_idx: numpy int [N] class indices 0..K-1\n",
    "    Returns: taus np.float32 [K-1]\n",
    "    \"\"\"\n",
    "    if isinstance(Z_val, torch.Tensor):\n",
    "        Z_val = Z_val.numpy()\n",
    "    P_val = 1.0 / (1.0 + np.exp(-Z_val))  # sigmoid -> P(y>k)\n",
    "\n",
    "    # monotone repair for each sample: enforce P_k >= P_{k+1}\n",
    "    P_rep = monotone_repair_numpy(P_val)\n",
    "\n",
    "    K_1 = P_rep.shape[1]\n",
    "    best_taus = np.full(K_1, 0.5, dtype=np.float32)\n",
    "\n",
    "    # simple independent sweep per head, decode with all taus applied\n",
    "    for k in range(K_1):\n",
    "        best_f1, best_tau = -1.0, 0.5\n",
    "        for tau in grid:\n",
    "            y_hat = decode_ordinal_with_taus(P_rep, taus_override={k: tau})\n",
    "            f1 = f1_score(y_val_idx, y_hat, average='macro', zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_tau = f1, tau\n",
    "        best_taus[k] = best_tau\n",
    "    return best_taus\n",
    "\n",
    "# -mnotone repair: cumulative max from right to left \n",
    "def monotone_repair_numpy(P):\n",
    "    \"\"\"\n",
    "    P: [N, K-1] with P[:,k] = P(y>k)\n",
    "    Ensures P[:,k] >= P[:,k+1] for all k by cumulative max from right.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P).copy()\n",
    "    for k in range(P.shape[1] - 2, -1, -1):\n",
    "        P[:, k] = np.maximum(P[:, k], P[:, k+1])\n",
    "    return P\n",
    "\n",
    "def monotone_repair_torch(P):\n",
    "    \"\"\"\n",
    "    P: torch [N, K-1]\n",
    "    \"\"\"\n",
    "    out = P.clone()\n",
    "    for k in range(out.shape[1] - 2, -1, -1):\n",
    "        out[:, k] = torch.maximum(out[:, k], out[:, k+1])\n",
    "    return out\n",
    "\n",
    "# decode class indices from ordinal probabilities + per-head taus \n",
    "def decode_ordinal_with_taus(P_rep, taus=None, taus_override=None):\n",
    "    \"\"\"\n",
    "    P_rep: [N, K-1] repaired cumulative probs (numpy)\n",
    "    taus: np.array [K-1] default thresholds; optional\n",
    "    taus_override: dict {k: tau_k} to try alternative per-head tau during search\n",
    "    Returns y_hat: np.int64 [N] in 0..K-1\n",
    "    \"\"\"\n",
    "    N, K_1 = P_rep.shape\n",
    "    if taus is None:\n",
    "        taus = np.full(K_1, 0.5, dtype=np.float32)\n",
    "    if taus_override:\n",
    "        taus = taus.copy()\n",
    "        for k, v in taus_override.items():\n",
    "            taus[k] = v\n",
    "\n",
    "    # class = number of thresholds surpassed\n",
    "    comp = (P_rep >= taus.reshape(1, -1)).astype(np.int32)  # [N, K-1]\n",
    "    y_hat = comp.sum(axis=1).astype(np.int64)\n",
    "    return y_hat  # 0..K-1\n",
    "\n",
    "# from ordinal (cumulative) probs to class probs, then expected class\n",
    "def ordinal_to_class_probs(P_rep):\n",
    "    \"\"\"\n",
    "    P_rep: [N, K-1] repaired P(y>k)\n",
    "    Returns: Pc [N, K] with Pc[:,c] = P(y=c)\n",
    "    \"\"\"\n",
    "    N, K_1 = P_rep.shape\n",
    "    K = K_1 + 1\n",
    "    Pc = np.empty((N, K), dtype=np.float32)\n",
    "    # P(y=0) = 1 - P(y>0?)\n",
    "    Pc[:, 0] = 1.0 - P_rep[:, 0]\n",
    "    for c in range(1, K-1):\n",
    "        Pc[:, c] = np.clip(P_rep[:, c-1] - P_rep[:, c], 0.0, 1.0)\n",
    "    Pc[:, K-1] = P_rep[:, K_1-1]\n",
    "    # small numerical cleanup\n",
    "    s = Pc.sum(axis=1, keepdims=True)\n",
    "    Pc = Pc / np.maximum(s, 1e-8)\n",
    "    return Pc\n",
    "\n",
    "def expected_class_from_ordinal(P_rep):\n",
    "    Pc = ordinal_to_class_probs(P_rep)\n",
    "    K = Pc.shape[1]\n",
    "    idx = np.arange(K, dtype=np.float32).reshape(1, -1)\n",
    "    return (Pc * idx).sum(axis=1)  # [N]\n",
    "\n",
    "# expected return using train bucket means μ_c \n",
    "def expected_return_from_ordinal(P_rep, mu_c):\n",
    "    \"\"\"\n",
    "    P_rep: [N, K-1] repaired P(y>k)\n",
    "    mu_c: dict or array of per-class mean returns length K\n",
    "    \"\"\"\n",
    "    Pc = ordinal_to_class_probs(P_rep)  # [N, K]\n",
    "    if isinstance(mu_c, dict):\n",
    "        mu = np.array([mu_c[c] for c in range(len(mu_c))], dtype=np.float32)\n",
    "    else:\n",
    "        mu = np.asarray(mu_c, dtype=np.float32)\n",
    "    return (Pc * mu.reshape(1, -1)).sum(axis=1)   # [N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class OrdinalSequenceDataset(Dataset):\n",
    "    def __init__(self, X, T):  # T: float32 matrix [N, K-1] with 0/1\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.T = torch.tensor(T, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.T[i]\n",
    "\n",
    "def make_cumulative_targets(y_int, K):\n",
    "    y = y_int.reshape(-1, 1)\n",
    "    ks = np.arange(K-1).reshape(1, -1)\n",
    "    return (y > ks).astype(np.float32)   # [N, K-1]\n",
    "\n",
    "def decode_ordinal(probs, thr=0.5):\n",
    "    return (probs >= thr).sum(axis=1)    # [N] in 0..K-1\n",
    "\n",
    "def ordinal_threshold_weights(T_train):\n",
    "    pos = T_train.mean(axis=0)                   # [K-1]\n",
    "    w = (1.0 / np.clip(pos, 1e-6, 1.0))\n",
    "    w = w / w.mean()\n",
    "    return torch.tensor(w, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalHeadCORN(nn.Module):\n",
    "    def __init__(self, in_dim, K):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, K-1)   # independent thresholds\n",
    "\n",
    "    def forward(self, h):                  # h: [B, H]\n",
    "        return self.fc(h)                  # logits: [B, K-1]\n",
    "\n",
    "\n",
    "class OrdinalHeadCORAL(nn.Module):\n",
    "    def __init__(self, in_dim, K):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(in_dim, 1, bias=False)   # shared direction\n",
    "        # unconstrained params → cumulative biases ensure b1<=b2<=... via cumsum of positives\n",
    "        self._beta = nn.Parameter(torch.zeros(K-1)) # unconstrained\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, h):\n",
    "        base = self.w(h)                  # [B,1]\n",
    "        deltas = self.softplus(self._beta)  # ≥0\n",
    "        b = torch.cumsum(deltas, dim=0)     # monotone biases [K-1]\n",
    "        return base - b                     # broadcast to [B,K-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class RNNHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared head:\n",
    "      - RNN stack (LSTM/GRU, uni/bi)\n",
    "      - BatchNorm + Dense(32, ReLU) + Dropout(0.3)\n",
    "      - Output layer (1 unit): linear (regression) or logits (classification)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, rnn_type='LSTM', bidirectional=False, problem_type='regression', output_dim=1):\n",
    "        super().__init__()\n",
    "        hidden1 = 128\n",
    "        hidden2 = 64\n",
    "        self.problem_type = problem_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "\n",
    "        rnn_cls = {'LSTM': nn.LSTM, 'GRU': nn.GRU}[('GRU' if 'GRU' in self.rnn_type else 'LSTM')]\n",
    "\n",
    "        self.rnn1 = rnn_cls(\n",
    "            input_size=input_size, hidden_size=hidden1, num_layers=1,\n",
    "            batch_first=True, dropout=0.0, bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        self.inter_rnn_drop = nn.Dropout(0.1)\n",
    "        \n",
    "        self.rnn2 = rnn_cls(\n",
    "            input_size=hidden1*(2 if bidirectional else 1), hidden_size=hidden2, num_layers=1,\n",
    "            batch_first=True, dropout=0.0, bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        feat_dim = hidden2*(2 if bidirectional else 1)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(feat_dim)\n",
    "        self.fc = nn.Linear(feat_dim, 32)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        if self.problem_type == 'multiclass':\n",
    "            self.out = OrdinalHeadCORAL(32, output_dim)\n",
    "        else:\n",
    "            self.out = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, F]\n",
    "        out, _ = self.rnn1(x)\n",
    "        out = self.inter_rnn_drop(out)   # inter-layer dropout (sequence-wise)\n",
    "        out, _ = self.rnn2(out)\n",
    "        # take last timestep: [B, T, H] -> [B, H]\n",
    "        out = out[:, -1, :]\n",
    "        out = self.bn(out)\n",
    "        out = F.relu(self.fc(out))\n",
    "        out = self.drop(out)\n",
    "        out = self.out(out)  # shape [B, 1] or [B, output_dim]\n",
    "        return out  # regression: raw; classification: logits\n",
    "\n",
    "def build_model(input_shape, model_type='LSTM', problem_type='regression'):\n",
    "    seq_len, n_features = input_shape\n",
    "    output_dim = 6 if problem_type == 'multiclass' else 1\n",
    "    model_type = model_type.upper()\n",
    "    if model_type == 'LSTM':\n",
    "        return RNNHead(n_features, rnn_type='LSTM', bidirectional=False, problem_type=problem_type, output_dim=output_dim)\n",
    "    elif model_type == 'BILSTM':\n",
    "        return RNNHead(n_features, rnn_type='LSTM', bidirectional=True, problem_type=problem_type, output_dim=output_dim)\n",
    "    elif model_type == 'GRU':\n",
    "        return RNNHead(n_features, rnn_type='GRU', bidirectional=False, problem_type=problem_type, output_dim=output_dim)\n",
    "    elif model_type == 'BIGRU':\n",
    "        return RNNHead(n_features, rnn_type='GRU', bidirectional=True, problem_type=problem_type, output_dim=output_dim)\n",
    "    else:\n",
    "        raise ValueError(\"Model type must be one of: ['LSTM','BiLSTM','GRU','BiGRU']\")\n",
    "\n",
    "# Early Stopping (PyTorch)\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=15, min_delta=0.0, restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best = restore_best\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        improved = (self.best_loss - val_loss) > self.min_delta\n",
    "        if improved:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best:\n",
    "                # Deep copy state dict\n",
    "                self.best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)\n",
    "            \n",
    "def best_threshold_from_val(val_logits, val_labels, metric='mcc'):\n",
    "    p = 1.0 / (1.0 + np.exp(-val_logits))\n",
    "    grid = np.linspace(0.2, 0.8, 61)\n",
    "    best_t, best_s = 0.5, -1.0\n",
    "    for t in grid:\n",
    "        yhat = (p >= t).astype(int)\n",
    "        if metric == 'f1':\n",
    "            s = f1_score(val_labels, yhat, zero_division=0)\n",
    "        else:\n",
    "            s = matthews_corrcoef(val_labels, yhat) if len(np.unique(yhat)) > 1 else -1\n",
    "        if s > best_s:\n",
    "            best_s, best_t = s, t\n",
    "    return best_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_return(series, h):\n",
    "    return series.shift(-h) / series - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_quantile_edges(x, n_classes=6):\n",
    "    qs = np.linspace(0, 1, n_classes+1)\n",
    "    edges = np.quantile(x, qs)\n",
    "    for i in range(1, len(edges)):\n",
    "        if edges[i] <= edges[i-1]:\n",
    "            edges[i] = np.nextafter(edges[i-1], np.inf)\n",
    "    return edges  # length n_classes+1\n",
    "\n",
    "def bucketize_with_edges(x, edges):\n",
    "    inner = edges[1:-1]\n",
    "    labels = np.digitize(x, inner, right=True)\n",
    "    return labels.astype(int)  # 0..C-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fGvgtiPTiBQ4"
   },
   "outputs": [],
   "source": [
    "class StockPredictionPipeline:\n",
    "    def __init__(self, df, feature_columns, model_type='LSTM', sequence_length=30, problem_type='regression', horizon_steps=1):\n",
    "        self.df = df.copy()\n",
    "        self.feature_columns = feature_columns\n",
    "        self.model_type = model_type\n",
    "        self.sequence_length = sequence_length\n",
    "        self.problem_type = problem_type\n",
    "        self.horizon_steps = horizon_steps\n",
    "        self.results = []\n",
    "        self.loss_curves = []\n",
    "\n",
    "        # Validate\n",
    "        self._validate_inputs()\n",
    "\n",
    "        # Device & precision\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mixed_precision = torch.cuda.is_available()\n",
    "\n",
    "        print(f\"Pipeline initialized for a '{self.problem_type}' problem \"\n",
    "              f\"with horizon {self.horizon_steps} steps. Device: {self.device}\")\n",
    "\n",
    "    def _validate_inputs(self):\n",
    "        missing_cols = [col for col in self.feature_columns if col not in self.df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_cols}\")\n",
    "\n",
    "        if 'close' not in self.df.columns and 'close_price' not in self.df.columns:\n",
    "            raise ValueError(\"No 'close' or 'close_price' column found in data\")\n",
    "\n",
    "        valid_models = ['LSTM', 'BiLSTM', 'GRU', 'BiGRU']\n",
    "        if self.model_type not in valid_models:\n",
    "            raise ValueError(f\"Model type must be one of: {valid_models}\")\n",
    "\n",
    "        if self.problem_type not in ['regression', 'classification', 'multiclass']:\n",
    "            raise ValueError(\"Problem type must be 'regression', 'classification', or 'multiclass'\")\n",
    "\n",
    "    def create_target_variable(self, company_data):\n",
    "        company_data = company_data.copy()\n",
    "        price_col = 'close' if 'close' in company_data.columns else 'close_price'\n",
    "        if 'date' in company_data.columns:\n",
    "            company_data = company_data.sort_values('date')\n",
    "            \n",
    "        h = self.horizon_steps\n",
    "\n",
    "        company_data['target_regression'] = (\n",
    "            np.log(company_data[price_col].shift(-h)) - np.log(company_data[price_col])\n",
    "        )\n",
    "        company_data['target_direction'] = (company_data['target_regression'] > 0).astype(int)        \n",
    "\n",
    "        if self.problem_type == 'regression':\n",
    "            company_data = company_data.dropna()\n",
    "        elif self.problem_type == 'classification':\n",
    "            company_data = company_data.dropna()\n",
    "        else:\n",
    "            company_data['ret_h'] = pct_return(company_data[price_col], h)\n",
    "            subset = ['ret_h']\n",
    "            company_data = company_data.dropna(subset=subset)        \n",
    "            \n",
    "        return company_data\n",
    "\n",
    "    def create_sequences(self, features, *targets):\n",
    "        X = []\n",
    "        y_sequences = [[] for _ in targets]\n",
    "        for i in range(self.sequence_length, len(features)):\n",
    "            X.append(features[i-self.sequence_length:i])\n",
    "            for j, target in enumerate(targets):\n",
    "                y_sequences[j].append(target[i])\n",
    "        return (np.array(X),) + tuple(np.array(y) for y in y_sequences)\n",
    "\n",
    "    def _train_one_epoch_multiclass(self, model, loader, optimizer, loss_fn, scaler, *,\n",
    "                        problem_type, pos_weight=None):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)             # [B,T,F]\n",
    "            yb = yb.to(self.device)             # ordinal: [B,K-1] float\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            ctx = torch.amp.autocast('cuda') if self.mixed_precision else nullcontext()\n",
    "            with ctx:\n",
    "                logits = model(xb)              # [B, K-1] for ordinal\n",
    "                if problem_type == 'multiclass':\n",
    "                    # BCE with per-head pos_weight\n",
    "                    # compute weighted BCE per column, then mean over columns\n",
    "                    bces = []\n",
    "                    for k in range(logits.shape[1]):\n",
    "                        w = None if pos_weight is None else pos_weight[k]\n",
    "                        bce_k = F.binary_cross_entropy_with_logits(\n",
    "                            logits[:, k], yb[:, k], pos_weight=w\n",
    "                        )\n",
    "                        bces.append(bce_k)\n",
    "                    loss = torch.stack(bces).mean()\n",
    "                else:\n",
    "                    loss = loss_fn(logits, yb)\n",
    "\n",
    "            if self.mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            total += float(loss.item()) * xb.size(0)\n",
    "        return total / len(loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_one_epoch_multiclass(self, model, loader, loss_fn, *, problem_type, pos_weight=None):\n",
    "        model.eval()\n",
    "        total = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device)\n",
    "            logits = model(xb)\n",
    "            if problem_type == 'multiclass':\n",
    "                bces = []\n",
    "                for k in range(logits.shape[1]):\n",
    "                    w = None if pos_weight is None else pos_weight[k]\n",
    "                    bce_k = F.binary_cross_entropy_with_logits(\n",
    "                        logits[:, k], yb[:, k], pos_weight=w\n",
    "                    )\n",
    "                    bces.append(bce_k)\n",
    "                loss = torch.stack(bces).mean()\n",
    "            else:\n",
    "                loss = loss_fn(logits, yb)\n",
    "            total += float(loss.item()) * xb.size(0)\n",
    "        return total / len(loader.dataset)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_one_epoch(self, model, loader, loss_fn):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device)\n",
    "            \n",
    "            if self.problem_type in ['regression', 'classification']:\n",
    "                yb = yb.view(-1, 1) #single output heads\n",
    "                \n",
    "            logits = model(xb)\n",
    "            if self.problem_type == 'multiclass':\n",
    "                loss = loss_fn(logits, yb)\n",
    "            else:\n",
    "                loss = loss_fn(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        return total_loss / len(loader.dataset)\n",
    "\n",
    "    def _train_one_epoch(self, model, loader, optimizer, loss_fn, scaler):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device)\n",
    "            if self.problem_type in ['regression', 'classification']:\n",
    "                yb = yb.view(-1, 1) #single output heads\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            ctx = torch.amp.autocast('cuda') if self.mixed_precision else nullcontext()\n",
    "            with ctx:\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "\n",
    "            if self.mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        return total_loss / len(loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_one_epoch(self, model, loader, loss_fn):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device)\n",
    "            \n",
    "            if self.problem_type in ['regression', 'classification']:\n",
    "                yb = yb.view(-1, 1) #single output heads\n",
    "                \n",
    "            logits = model(xb)\n",
    "            if self.problem_type == 'multiclass':\n",
    "                loss = loss_fn(logits, yb.long())\n",
    "            else:\n",
    "                loss = loss_fn(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        return total_loss / len(loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _predict(self, model, loader):\n",
    "        model.eval()\n",
    "        outs = []\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            if self.problem_type == 'multiclass':\n",
    "                logits = model(xb).detach().cpu().numpy()\n",
    "            else:\n",
    "                logits = model(xb).squeeze(1).detach().cpu().numpy()\n",
    "            outs.append(logits)\n",
    "        return np.concatenate(outs, axis=0)\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        model = build_model(input_shape, model_type=self.model_type, problem_type=self.problem_type)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def process_company(self, company_name, company_data, sector):\n",
    "        print(f\"\\nProcessing {company_name} ({sector})...\")\n",
    "        try:\n",
    "            company_data = self.create_target_variable(company_data)\n",
    "\n",
    "            # Min samples requirement (same heuristic)\n",
    "            min_samples = self.sequence_length + 150 + self.horizon_steps\n",
    "            if len(company_data) < min_samples:\n",
    "                print(f\"Insufficient data for {company_name} ({len(company_data)} < {min_samples}). Skipping...\")\n",
    "                return None\n",
    "\n",
    "            if company_data[self.feature_columns].isnull().any().any():\n",
    "                print(f\"Missing values in features for {company_name}. Skipping...\")\n",
    "                return None\n",
    "\n",
    "            features = company_data[self.feature_columns].values\n",
    "            target_reg = company_data['target_regression'].values\n",
    "            target_dir = company_data['target_direction'].values\n",
    "\n",
    "            # Create sequences\n",
    "            if self.problem_type == 'multiclass':\n",
    "                X_raw, y_reg, y_dir = self.create_sequences(features, target_reg, target_dir)\n",
    "            else:\n",
    "                X_raw, y_reg, y_dir = self.create_sequences(features, target_reg, target_dir)\n",
    "\n",
    "            # TimeSeriesSplit\n",
    "            n_splits = min(5, len(X_raw) // 50)\n",
    "            if n_splits < 3:\n",
    "                print(f\"Insufficient data for proper time series validation for {company_name}. Skipping...\")\n",
    "                return None\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "            splits = list(tscv.split(X_raw))\n",
    "            train_idx, test_idx = splits[-1]\n",
    "\n",
    "            # Train/Val split (last 20% of train for val)\n",
    "            val_size = int(0.2 * len(train_idx))\n",
    "            final_train_idx = train_idx[:-val_size]\n",
    "            val_idx = train_idx[-val_size:]\n",
    "            \n",
    "            if self.horizon_steps > 1:\n",
    "                print(\"Adjusting for multi-step horizon...\")\n",
    "                gap = self.horizon_steps\n",
    "                if len(final_train_idx) > gap:\n",
    "                    final_train_idx = final_train_idx[:-gap]  # drop last h labels from train\n",
    "                if len(val_idx) > gap:\n",
    "                    val_idx = val_idx[gap:]  # drop last h labels from val\n",
    "\n",
    "            X_train_raw, X_val_raw, X_test_raw = X_raw[final_train_idx], X_raw[val_idx], X_raw[test_idx]\n",
    "            \n",
    "            F = X_raw.shape[-1]\n",
    "            feat_scaler = StandardScaler()\n",
    "            X_train = feat_scaler.fit_transform(X_train_raw.reshape(-1, F)).reshape(X_train_raw.shape)\n",
    "            X_val   = feat_scaler.transform(X_val_raw.reshape(-1, F)).reshape(X_val_raw.shape)\n",
    "            X_test  = feat_scaler.transform(X_test_raw.reshape(-1, F)).reshape(X_test_raw.shape)\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                y_train, y_val, y_test = y_reg[final_train_idx], y_reg[val_idx], y_reg[test_idx]\n",
    "                target_scaler = StandardScaler()\n",
    "                y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "                y_val_scaled   = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "                train_target, val_target = y_train_scaled, y_val_scaled\n",
    "\n",
    "            elif self.problem_type == 'classification':\n",
    "                y_train, y_val, y_test = y_dir[final_train_idx], y_dir[val_idx], y_dir[test_idx]\n",
    "                train_target, val_target = y_train, y_val\n",
    "                target_scaler = None\n",
    "                class_ratio = np.mean(y_train)\n",
    "                if class_ratio < 0.1 or class_ratio > 0.9:\n",
    "                    print(f\"Severe class imbalance for {company_name} ({class_ratio:.3f}). Consider using class weights.\")\n",
    "\n",
    "            elif self.problem_type == 'multiclass':\n",
    "                # 1) returns aligned with sequence targets\n",
    "                if 'ret_h' not in company_data.columns:\n",
    "                    raise RuntimeError(\"Expected 'ret_h' for ordinal but not found. create_target_variable must set it.\")\n",
    "                ret_full     = company_data['ret_h'].values\n",
    "                ret_seq_full = ret_full[self.sequence_length:]\n",
    "            \n",
    "                # 2) split returns\n",
    "                ret_train = ret_seq_full[final_train_idx]\n",
    "                ret_val   = ret_seq_full[val_idx]\n",
    "                ret_test  = ret_seq_full[test_idx]\n",
    "            \n",
    "                # 3) TRAIN-only edges (K classes)\n",
    "                K = getattr(self, 'n_classes', 6)\n",
    "                edges = safe_quantile_edges(ret_train, n_classes=K)\n",
    "                label_names = edge_labels_from_edges(edges, decimals=1)\n",
    "                K = int(edges.shape[0] - 1)\n",
    "            \n",
    "                # 4) bucketize using TRAIN edges\n",
    "                y_bucket = bucketize_with_edges(ret_seq_full, edges)\n",
    "                y_train, y_val, y_test = y_bucket[final_train_idx], y_bucket[val_idx], y_bucket[test_idx]\n",
    "            \n",
    "                # 5) cumulative targets \n",
    "                T_train = make_cumulative_targets(y_train, K)   # [Ntr, K-1]\n",
    "                T_val   = make_cumulative_targets(y_val,   K)\n",
    "                T_test  = make_cumulative_targets(y_test,  K)   # for completeness\n",
    "            \n",
    "                train_target, val_target = T_train, T_val\n",
    "                target_scaler = None\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unknown problem type.\")\n",
    "            \n",
    "\n",
    "            # Datasets & loaders\n",
    "            if self.problem_type == 'multiclass':\n",
    "                train_ds = OrdinalSequenceDataset(X_train, T_train)\n",
    "                val_ds   = OrdinalSequenceDataset(X_val,   T_val)\n",
    "                test_ds  = OrdinalSequenceDataset(X_test,  T_test)\n",
    "            else:\n",
    "                train_ds = SequenceDataset(X_train, train_target)\n",
    "                val_ds   = SequenceDataset(X_val,   val_target)\n",
    "                test_ds  = SequenceDataset(X_test,  y_test)\n",
    "\n",
    "            train_loader = DataLoader(train_ds, batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "            val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "            test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "\n",
    "            # Build model\n",
    "            model = self.build_model((self.sequence_length, len(self.feature_columns)))\n",
    "\n",
    "            # Loss functions\n",
    "            if self.problem_type == 'regression':\n",
    "                loss_fn = nn.HuberLoss(delta=1.0)\n",
    "            elif self.problem_type == 'classification':\n",
    "                # Use BCEWithLogitsLoss for numerical stability (logits input)\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "            elif self.problem_type == 'multiclass':\n",
    "                pos_rate = T_train.mean(axis=0)   # [K-1]\n",
    "                pos_weight = (1.0 - pos_rate) / np.clip(pos_rate, 1e-6, 1.0)\n",
    "                pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float32).to(self.device)\n",
    "                loss_fn = None #will handle this in _train_one_epoch\n",
    "\n",
    "            # Optimizer & scheduler\n",
    "            optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-7, weight_decay=1e-5)\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-7)\n",
    "            early_stopper = EarlyStopper(patience=15, min_delta=0.0, restore_best=True)\n",
    "            scaler = torch.amp.GradScaler('cuda', enabled=self.mixed_precision)\n",
    "\n",
    "            # Training loop\n",
    "            max_epochs = 100\n",
    "            best_val = float('inf')\n",
    "            epochs_trained = 0\n",
    "            company_loss_rows = []  \n",
    "\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                if self.problem_type == 'multiclass':\n",
    "                    train_loss = self._train_one_epoch_multiclass(model, train_loader, optimizer, loss_fn, scaler,\n",
    "                                        problem_type=self.problem_type, pos_weight=pos_weight_tensor)\n",
    "                    val_loss   = self._eval_one_epoch_multiclass(model, val_loader, loss_fn,\n",
    "                                        problem_type=self.problem_type, pos_weight=pos_weight_tensor)\n",
    "                else:\n",
    "                    train_loss = self._train_one_epoch(model, train_loader, optimizer, loss_fn, scaler)\n",
    "                    val_loss   = self._eval_one_epoch(model, val_loader, loss_fn)\n",
    "                scheduler.step(val_loss)\n",
    "                stop = early_stopper.step(val_loss, model)\n",
    "                epochs_trained = epoch\n",
    "                \n",
    "                row = {\n",
    "                    'company': company_name,\n",
    "                    'sector': sector,\n",
    "                    'model_type': self.model_type,\n",
    "                    'problem_type': self.problem_type,\n",
    "                    'sequence_length': self.sequence_length,\n",
    "                    'horizon_steps': self.horizon_steps,\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': float(train_loss),\n",
    "                    'val_loss': float(val_loss),\n",
    "                    'train_samples': len(X_train),\n",
    "                    'val_samples': len(X_val),\n",
    "                    'test_samples': len(X_test),\n",
    "                }\n",
    "                \n",
    "                company_loss_rows.append(row)\n",
    "                self.loss_curves.append(row)\n",
    "\n",
    "                if epoch % 10 == 0 or stop:\n",
    "                    print(f\"  Epoch {epoch:03d} - train {train_loss:.5f} | val {val_loss:.5f}\")\n",
    "\n",
    "                if stop:\n",
    "                    break\n",
    "\n",
    "            # Restore best model weights (like Keras restore_best_weights=True)\n",
    "            early_stopper.restore(model)\n",
    "\n",
    "            # Predictions\n",
    "            if self.problem_type == 'multiclass':\n",
    "                # CALIBRATE ON VAL\n",
    "                # Collect VAL logits (shape [Nval, K-1]) and learn per-head taus\n",
    "                Z_val = collect_logits(model, val_loader, self.device)       # torch [Nval, K-1]\n",
    "                taus  = find_taus_per_threshold(Z_val, y_val)            # np [K-1]\n",
    "\n",
    "                # RUN ON TEST WITH REPAIR + DECODE \n",
    "                Z_test = collect_logits(model, test_loader, self.device)     # torch [Nte, K-1]\n",
    "                P_test = torch.sigmoid(Z_test).cpu().numpy()                 # np [Nte, K-1] with P(y>k)\n",
    "                P_rep  = monotone_repair_numpy(P_test)                       # enforce P[:,k] >= P[:,k+1]\n",
    "\n",
    "                # Decode calibrated class indices using learned taus\n",
    "                y_pred_labels = decode_ordinal_with_taus(P_rep, taus=taus)   # np [Nte], values 0..K-1\n",
    "                y_true_labels = y_test                                   # ensure you use *_idx from bucketization\n",
    "\n",
    "                # CONFUSION & METRICS\n",
    "                labels = list(range(K))\n",
    "                cm_counts = compute_confusion(y_true_labels, y_pred_labels, labels=labels, normalize=None)\n",
    "                cm_norm   = compute_confusion(y_true_labels, y_pred_labels, labels=labels, normalize='true')\n",
    "\n",
    "                micro_acc = (y_true_labels == y_pred_labels).mean()\n",
    "                macro_f1  = f1_score(y_true_labels, y_pred_labels, average='macro', zero_division=0)\n",
    "\n",
    "                # EXPECTED RETURN (ordinal → class probs)\n",
    "                # Use TRAIN means μ_c computed from train split only\n",
    "                ret_seq_train = ret_seq_full[final_train_idx]\n",
    "                mu_c = {c: (ret_seq_train[y_train == c].mean() if np.any(y_train == c) else 0.0)\n",
    "                        for c in range(K)}\n",
    "\n",
    "                # convert repaired cumulative probs -> class probs, then expected return\n",
    "                Pc_test     = ordinal_to_class_probs(P_rep)                   # [Nte, K]\n",
    "                expected_ret = (Pc_test * np.array([mu_c[c] for c in range(K)], dtype=np.float32)).sum(axis=1)\n",
    "                expected_ret_mean = float(expected_ret.mean())\n",
    "\n",
    "                #binary collapse for comparability\n",
    "                mid_cut = (K // 2)\n",
    "                y_true_dir = (y_true_labels >= mid_cut).astype(int)\n",
    "                y_pred_dir = (y_pred_labels >= mid_cut).astype(int)\n",
    "                precision = precision_score(y_true_dir, y_pred_dir, zero_division=0)\n",
    "                recall    = recall_score(y_true_dir, y_pred_dir, zero_division=0)\n",
    "                f1        = f1_score(y_true_dir, y_pred_dir, zero_division=0)\n",
    "                mcc       = matthews_corrcoef(y_true_dir, y_pred_dir)\n",
    "                directional_accuracy = (y_true_dir == y_pred_dir).mean()\n",
    "\n",
    "                # PACK RESULT + SAVE EDGES/TAUS/LABELS FOR PLOTTING \n",
    "                result = {\n",
    "                    'company': company_name,\n",
    "                    'sector': sector,\n",
    "                    'model_type': self.model_type,\n",
    "                    'problem_type': self.problem_type,\n",
    "                    'horizon_steps': self.horizon_steps,\n",
    "                    'macro_f1': macro_f1,\n",
    "                    'micro_accuracy': micro_acc,\n",
    "                    'expected_return_mean': expected_ret_mean,\n",
    "                    'mse': np.nan, 'mae': np.nan, 'r2': np.nan,\n",
    "                    'mcc': mcc, 'f1': f1, 'precision': precision, 'recall': recall,\n",
    "                    'directional_accuracy': directional_accuracy,\n",
    "                    'n_samples': int(X_raw.shape[0]),\n",
    "                    'train_samples': int(X_train.shape[0]),\n",
    "                    'val_samples': int(X_val.shape[0]),\n",
    "                    'test_samples': int(X_test.shape[0]),\n",
    "                    'epochs_trained': epochs_trained\n",
    "                }\n",
    "                result['confusion_matrix'] = cm_counts.tolist()\n",
    "                result['confusion_matrix_normalized'] = cm_norm.tolist()\n",
    "                result['bucket_edges']  = edges.tolist()        # from TRAIN quantiles\n",
    "                result['bucket_labels'] = label_names           # pretty strings for plotting\n",
    "                result['taus']          = taus.astype(float).tolist()\n",
    "\n",
    "\n",
    "            elif self.problem_type == 'regression':\n",
    "                y_pred_raw = self._predict(model, test_loader)  # [N]\n",
    "                y_pred_unscaled = target_scaler.inverse_transform(y_pred_raw.reshape(-1,1)).flatten() if target_scaler is not None else y_pred_raw\n",
    "                mse = mean_squared_error(y_test, y_pred_unscaled)\n",
    "                mae = mean_absolute_error(y_test, y_pred_unscaled)\n",
    "                r2  = r2_score(y_test, y_pred_unscaled)\n",
    "\n",
    "                # Directional metrics (derived from sign)\n",
    "                y_test_dir = (y_reg[test_idx] > 0).astype(int)\n",
    "                y_pred_dir = (y_pred_unscaled > 0).astype(int)\n",
    "                precision = precision_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "                recall    = recall_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "                f1        = f1_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "                mcc       = matthews_corrcoef(y_test_dir, y_pred_dir)\n",
    "                directional_accuracy = (y_test_dir == y_pred_dir).mean()\n",
    "\n",
    "                result = {\n",
    "                    'company': company_name,\n",
    "                    'sector': sector,\n",
    "                    'model_type': self.model_type,\n",
    "                    'problem_type': self.problem_type,\n",
    "                    'horizon_steps': self.horizon_steps,\n",
    "                    'macro_f1': np.nan,\n",
    "                    'micro_accuracy': np.nan,\n",
    "                    'expected_return_mean': np.nan,\n",
    "                    'mse': mse,\n",
    "                    'mae': mae,\n",
    "                    'r2': r2,\n",
    "                    'mcc': mcc,\n",
    "                    'f1': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'directional_accuracy': directional_accuracy,\n",
    "                    'n_samples': int(X_raw.shape[0]),\n",
    "                    'train_samples': int(X_train.shape[0]),\n",
    "                    'val_samples': int(X_val.shape[0]),\n",
    "                    'test_samples': int(X_test.shape[0]),\n",
    "                    'epochs_trained': epochs_trained\n",
    "                }\n",
    "\n",
    "            else:  # binary classification\n",
    "                val_logits = self._predict(model, val_loader) # logits [N]\n",
    "                thr = best_threshold_from_val(val_logits, y_val, metric='mcc')\n",
    "                \n",
    "                test_logits = self._predict(model, test_loader)  \n",
    "                probs = 1.0 / (1.0 + np.exp(-test_logits))\n",
    "                y_pred_dir = (probs >= thr).astype(int)\n",
    "                y_test_dir = y_test\n",
    "\n",
    "                labels = [0, 1]\n",
    "                cm_counts = compute_confusion(y_test_dir, y_pred_dir, labels=labels, normalize=None)\n",
    "                cm_norm   = compute_confusion(y_test_dir, y_pred_dir, labels=labels, normalize='true')\n",
    "\n",
    "                mse = mae = r2 = np.nan\n",
    "                precision = precision_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "                recall    = recall_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "                f1        = f1_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "                mcc       = matthews_corrcoef(y_test_dir, y_pred_dir) if len(np.unique(y_pred_dir)) > 1 else 0.0\n",
    "                directional_accuracy = (y_test_dir == y_pred_dir).mean()\n",
    "\n",
    "                result = {\n",
    "                    'company': company_name,\n",
    "                    'sector': sector,\n",
    "                    'model_type': self.model_type,\n",
    "                    'problem_type': self.problem_type,\n",
    "                    'horizon_steps': self.horizon_steps,\n",
    "                    'macro_f1': np.nan,\n",
    "                    'micro_accuracy': np.nan,\n",
    "                    'expected_return_mean': np.nan,\n",
    "                    'mse': mse,\n",
    "                    'mae': mae,\n",
    "                    'r2': r2,\n",
    "                    'mcc': mcc,\n",
    "                    'f1': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'directional_accuracy': directional_accuracy,\n",
    "                    'n_samples': int(X_raw.shape[0]),\n",
    "                    'train_samples': int(X_train.shape[0]),\n",
    "                    'val_samples': int(X_val.shape[0]),\n",
    "                    'test_samples': int(X_test.shape[0]),\n",
    "                    'epochs_trained': epochs_trained\n",
    "                }\n",
    "                result['confusion_matrix'] = cm_counts.tolist()\n",
    "                result['confusion_matrix_normalized'] = cm_norm.tolist()\n",
    "\n",
    "            # Logging\n",
    "            if self.problem_type == 'regression':\n",
    "                print(f\"  Regression -> MSE: {mse:.6f}, MAE: {mae:.6f}, R²: {r2:.4f}\")\n",
    "            elif self.problem_type == 'multiclass':\n",
    "                print(f\"  Multiclass -> Micro Acc: {micro_acc:.4f}, Macro F1: {macro_f1:.4f}, Expected Return: {expected_ret_mean:.6f}\")\n",
    "            else:\n",
    "                print(f\"  Directional -> Accuracy: {directional_accuracy:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "            # Cleanup\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            return result\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {company_name}: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return None\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        company_col = None\n",
    "        for col_name in ['ticker', 'company', 'symbol']:\n",
    "            if col_name in self.df.columns:\n",
    "                company_col = col_name\n",
    "                break\n",
    "        if company_col is None:\n",
    "            company_col = self.df.columns[0]\n",
    "            print(f\"Warning: Using '{company_col}' as company identifier column\")\n",
    "\n",
    "        companies = self.df[company_col].unique()\n",
    "        print(f\"Processing {len(companies)} companies with {self.model_type} model...\")\n",
    "        print(f\"Problem type: {self.problem_type}\")\n",
    "        print(f\"Sequence length: {self.sequence_length}\")\n",
    "        print(f\"Features: {self.feature_columns}\")\n",
    "\n",
    "        successful_companies = 0\n",
    "        for i, company in enumerate(companies, 1):\n",
    "            print(f\"\\n[{i}/{len(companies)}] Processing {company}...\")\n",
    "            company_data = self.df[self.df[company_col] == company].copy()\n",
    "            sector = company_data['sector'].iloc[0] if 'sector' in company_data.columns else 'Unknown'\n",
    "            result = self.process_company(company, company_data, sector)\n",
    "            if result:\n",
    "                self.results.append(result)\n",
    "                successful_companies += 1\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Pipeline completed: {successful_companies}/{len(companies)} companies processed successfully\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        if self.results:\n",
    "            self.results_df = pd.DataFrame(self.results)\n",
    "            return self.results_df\n",
    "        else:\n",
    "            print(\"No companies were processed successfully!\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def analyze_results(self):\n",
    "        if not hasattr(self, 'results_df') or self.results_df.empty:\n",
    "            print(\"No results to analyze!\")\n",
    "            return None\n",
    "\n",
    "        df = self.results_df\n",
    "        analysis = {}\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STOCK PREDICTION PIPELINE RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: {self.model_type} | Problem: {self.problem_type}\")\n",
    "        print(f\"Companies analyzed: {len(df)}\")\n",
    "        print(f\"Average samples per company: {df['n_samples'].mean():.0f}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"OVERALL PERFORMANCE\")\n",
    "        print(\"=\"*50)\n",
    "        if self.problem_type == 'regression':\n",
    "            print(f\"Mean Squared Error:     {df['mse'].mean():.6f} (±{df['mse'].std():.6f})\")\n",
    "            print(f\"Mean Absolute Error:    {df['mae'].mean():.6f} (±{df['mae'].std():.6f})\")\n",
    "            print(f\"R² Score:              {df['r2'].mean():.4f} (±{df['r2'].std():.4f})\")\n",
    "\n",
    "        elif self.problem_type == 'multiclass':\n",
    "            print(f\"Micro Accuracy:         {df['micro_accuracy'].mean():.4f} (±{df['micro_accuracy'].std():.4f})\")\n",
    "            print(f\"Macro F1 Score:         {df['macro_f1'].mean():.4f} (±{df['macro_f1'].std():.4f})\")\n",
    "            print(f\"Expected Return:        {df['expected_return_mean'].mean():.6f} (±{df['expected_return_mean'].std():.4f})\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"TOP 10 BY EXPECTED RETURN (mean)\")\n",
    "            print(\"=\"*50)\n",
    "            top_er = df.nlargest(10, 'expected_return_mean')\n",
    "            for _, row in top_er.iterrows():\n",
    "                print(f\"{row['company']:<20} | {row['sector']:<15} | \"\n",
    "                      f\"E[r]_mean: {row['expected_return_mean']:.4e} | Macro-F1: {row['macro_f1']:.3f}\")\n",
    "            \n",
    "        print(f\"Directional Accuracy:   {df['directional_accuracy'].mean():.4f} (±{df['directional_accuracy'].std():.4f})\")\n",
    "        print(f\"Matthews Correlation:   {df['mcc'].mean():.4f} (±{df['mcc'].std():.4f})\")\n",
    "        print(f\"F1 Score:              {df['f1'].mean():.4f} (±{df['f1'].std():.4f})\")\n",
    "        print(f\"Precision:             {df['precision'].mean():.4f} (±{df['precision'].std():.4f})\")\n",
    "        print(f\"Recall:                {df['recall'].mean():.4f} (±{df['recall'].std():.4f})\")\n",
    "\n",
    "        if 'sector' in df.columns and df['sector'].nunique() > 1:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"PERFORMANCE BY SECTOR\")\n",
    "            print(\"=\"*50)\n",
    "            sector_stats = df.groupby('sector').agg({\n",
    "                'directional_accuracy': ['mean', 'std', 'count'],\n",
    "                'mcc': ['mean', 'std'],\n",
    "                'r2': 'mean' if self.problem_type == 'regression' else lambda x: np.nan,\n",
    "                'mae': 'mean' if self.problem_type == 'regression' else lambda x: np.nan\n",
    "            }).round(4)\n",
    "            sector_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in sector_stats.columns]\n",
    "            sector_stats = sector_stats.sort_values('directional_accuracy_mean', ascending=False)\n",
    "            for sector, row in sector_stats.iterrows():\n",
    "                print(f\"{sector:<20} | Acc: {row['directional_accuracy_mean']:.3f}±{row['directional_accuracy_std']:.3f} | \"\n",
    "                      f\"MCC: {row['mcc_mean']:.3f} | Companies: {int(row['directional_accuracy_count'])}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TOP 10 PERFORMERS (by Directional Accuracy)\")\n",
    "        print(\"=\"*50)\n",
    "        top_performers = df.nlargest(10, 'directional_accuracy')\n",
    "        for _, row in top_performers.iterrows():\n",
    "            print(f\"{row['company']:<20} | {row['sector']:<15} | \"\n",
    "                  f\"Acc: {row['directional_accuracy']:.3f} | MCC: {row['mcc']:.3f}\")\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def save_results(self, results, output_dir='results/benchmarking'):\n",
    "        if results is not None and not results.empty:\n",
    "            model_name = self.model_type\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                out_dir = os.path.join(output_dir, 'regression')\n",
    "            elif self.problem_type == 'classification':\n",
    "                out_dir = os.path.join(output_dir, 'classification')\n",
    "            else:\n",
    "                out_dir = os.path.join(output_dir, 'multiclass')\n",
    "\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            output_path = os.path.join(out_dir, f\"{model_name}.csv\")\n",
    "\n",
    "            results.to_csv(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        else:\n",
    "            print(\"No results to save.\")\n",
    "            \n",
    "        \n",
    "            \n",
    "    def get_loss_curves_df(self):\n",
    "        if not self.loss_curves:\n",
    "            print(\"No loss curves logged yet.\")\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(self.loss_curves)\n",
    "\n",
    "    def save_loss_curves(self, out_path='results/benchmarking/'):\n",
    "        df = self.get_loss_curves_df()\n",
    "        if df.empty:\n",
    "            print(\"No loss curves to save.\")\n",
    "            return\n",
    "        if self.problem_type == 'regression':\n",
    "            out_path = os.path.join(out_path, 'regression', f\"{self.model_type}_loss_curves.csv\")\n",
    "        else:\n",
    "            out_path = os.path.join(out_path, 'classification', f\"{self.model_type}_loss_curves.csv\")\n",
    "            \n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        \n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\"Loss curves saved to {out_path}\")\n",
    "\n",
    "    def get_feature_importance_analysis(self):\n",
    "        print(\"Feature importance analysis not implemented yet.\")\n",
    "        print(\"Consider implementing SHAP values or permutation importance for better insights.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlCKxsDxHHFe",
    "outputId": "36aa45f2-ea60-4af0-f4ea-d3804481245f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/mdk91y8925ncdd32prjkcl_c0000gn/T/ipykernel_97455/1242874406.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  master_df[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of master_df before dropping NaNs: (108592, 21)\n",
      "Shape of master_df after dropping NaNs: (108592, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/mdk91y8925ncdd32prjkcl_c0000gn/T/ipykernel_97455/1242874406.py:95: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  master_df = master_df.groupby('ticker').apply(apply_ta_indicators)\n"
     ]
    }
   ],
   "source": [
    "companies = pd.read_parquet('stocknet-dataset/stock_table.parquet')\n",
    "tweets = pd.read_parquet('stocknet-dataset/stock_tweets_withsentiment_withemotion_withstance_nomerge.parquet')\n",
    "stocks = pd.read_parquet('stocknet-dataset/stock_prices.parquet')\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "companies.columns = [x.lower() for x in companies.columns]\n",
    "tweets.columns = [x.lower() for x in tweets.columns]\n",
    "stocks.columns = [x.lower() for x in stocks.columns]\n",
    "\n",
    "tweets['stance_positive'] = (tweets['stance_label'] == 'Positive').astype(int)\n",
    "tweets['stance_negative'] = (tweets['stance_label'] == 'Negative').astype(int)\n",
    "\n",
    "tweets_merged = tweets.groupby(['date', 'ticker'], as_index=False).agg({\n",
    "    'text': lambda x: ' '.join(x),\n",
    "    'sentiment': lambda x: x.mean(),\n",
    "    'emotion_anger': 'sum',\n",
    "    'emotion_disgust': 'sum',\n",
    "    'emotion_fear': 'sum',\n",
    "    'emotion_joy': 'sum',\n",
    "    'emotion_neutral': 'sum',\n",
    "    'emotion_sadness': 'sum',\n",
    "    'emotion_surprize': 'sum',\n",
    "    'stance_positive': 'sum',\n",
    "    'stance_negative': 'sum'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweets_merged['date'] = pd.to_datetime(tweets_merged['date'])\n",
    "stocks['date'] = pd.to_datetime(stocks['date'])\n",
    "\n",
    "\n",
    "\n",
    "master_df = pd.merge(\n",
    "    stocks,\n",
    "    tweets_merged,\n",
    "    on=[\"date\", \"ticker\"],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing tweet features with 0\n",
    "tweet_feature_cols = ['sentiment', 'emotion_anger', 'emotion_disgust', 'emotion_fear', 'emotion_joy', 'emotion_neutral', 'emotion_sadness', 'emotion_surprize', 'stance_positive', 'stance_negative']\n",
    "for col in tweet_feature_cols:\n",
    "    if col in master_df.columns:\n",
    "        master_df[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "master_df = pd.merge(master_df, companies[['ticker', 'sector', 'company']], on='ticker', how='left')\n",
    "\n",
    "\n",
    "feature_cols = ['open','high','low','volume']\n",
    "\n",
    "master_df = master_df.rename(columns={'close': 'close_price', 'company': 'company_name'})\n",
    "\n",
    "\n",
    "print(f\"Shape of master_df before dropping NaNs: {master_df.shape}\")\n",
    "print(f\"Shape of master_df after dropping NaNs: {master_df.shape}\")\n",
    "\n",
    "master_df.rename(columns={'close_price': 'close'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "master_df.sort_values(by=['ticker', 'date'], inplace=True)\n",
    "\n",
    "\n",
    "def apply_ta_indicators(df_group):\n",
    "    df_group.set_index(pd.DatetimeIndex(df_group['date']), inplace=True)\n",
    "    df_group.ta.ema(length=12, append=True)\n",
    "    df_group.ta.ema(length=26, append=True)\n",
    "    df_group.ta.ema(length=50, append=True)\n",
    "\n",
    "    df_group.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "\n",
    "\n",
    "    df_group.ta.rsi(length=14, append=True)\n",
    "    df_group.ta.stochrsi(length=14, append=True)\n",
    "\n",
    "    df_group.ta.atr(length=14, append=True)\n",
    "\n",
    "    bb = ta.bbands(df_group['close'], length=20, std=2)\n",
    "    df_group['BB_upper'] = bb['BBU_20_2.0']\n",
    "    df_group['BB_middle'] = bb['BBM_20_2.0']\n",
    "    df_group['BB_lower'] = bb['BBL_20_2.0']\n",
    "\n",
    "    df_group.ta.obv(append=True)\n",
    "    return df_group.reset_index(drop=True)\n",
    "\n",
    "master_df = master_df.groupby('ticker').apply(apply_ta_indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "12_qUQQGGxKm",
    "outputId": "41eb4670-181a-465b-f565-290ccae2d22b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>MACDh_12_26_9</th>\n",
       "      <th>MACDs_12_26_9</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>STOCHRSIk_14_14_3_3</th>\n",
       "      <th>STOCHRSId_14_14_3_3</th>\n",
       "      <th>ATRr_14</th>\n",
       "      <th>BB_upper</th>\n",
       "      <th>BB_middle</th>\n",
       "      <th>BB_lower</th>\n",
       "      <th>OBV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-11-14</td>\n",
       "      <td>77.928574</td>\n",
       "      <td>78.207146</td>\n",
       "      <td>76.597145</td>\n",
       "      <td>76.697144</td>\n",
       "      <td>69.613815</td>\n",
       "      <td>119292600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538077</td>\n",
       "      <td>-3.703588</td>\n",
       "      <td>25.771012</td>\n",
       "      <td>21.054629</td>\n",
       "      <td>19.582354</td>\n",
       "      <td>2.377852</td>\n",
       "      <td>94.648550</td>\n",
       "      <td>84.401357</td>\n",
       "      <td>74.154164</td>\n",
       "      <td>-1.014356e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-11-15</td>\n",
       "      <td>76.790001</td>\n",
       "      <td>77.071426</td>\n",
       "      <td>74.660004</td>\n",
       "      <td>75.088570</td>\n",
       "      <td>68.153778</td>\n",
       "      <td>197477700.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537725</td>\n",
       "      <td>-3.838019</td>\n",
       "      <td>23.573491</td>\n",
       "      <td>15.792949</td>\n",
       "      <td>19.993462</td>\n",
       "      <td>2.380310</td>\n",
       "      <td>93.761634</td>\n",
       "      <td>83.514428</td>\n",
       "      <td>73.267223</td>\n",
       "      <td>-1.211834e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>75.028572</td>\n",
       "      <td>75.714287</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>75.382858</td>\n",
       "      <td>68.420891</td>\n",
       "      <td>316723400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455544</td>\n",
       "      <td>-3.951905</td>\n",
       "      <td>24.836267</td>\n",
       "      <td>12.243346</td>\n",
       "      <td>16.363641</td>\n",
       "      <td>2.459547</td>\n",
       "      <td>92.716200</td>\n",
       "      <td>82.679214</td>\n",
       "      <td>72.642228</td>\n",
       "      <td>-8.951103e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-19</td>\n",
       "      <td>77.244286</td>\n",
       "      <td>81.071426</td>\n",
       "      <td>77.125717</td>\n",
       "      <td>80.818573</td>\n",
       "      <td>73.354591</td>\n",
       "      <td>205829400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>-3.951213</td>\n",
       "      <td>43.429047</td>\n",
       "      <td>39.692073</td>\n",
       "      <td>22.576123</td>\n",
       "      <td>2.695187</td>\n",
       "      <td>91.617665</td>\n",
       "      <td>82.201285</td>\n",
       "      <td>72.784906</td>\n",
       "      <td>-6.892809e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-11-20</td>\n",
       "      <td>81.701431</td>\n",
       "      <td>81.707146</td>\n",
       "      <td>79.225716</td>\n",
       "      <td>80.129997</td>\n",
       "      <td>72.729614</td>\n",
       "      <td>160688500.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281964</td>\n",
       "      <td>-3.880722</td>\n",
       "      <td>42.011353</td>\n",
       "      <td>69.373201</td>\n",
       "      <td>40.436207</td>\n",
       "      <td>2.679612</td>\n",
       "      <td>91.027780</td>\n",
       "      <td>81.851785</td>\n",
       "      <td>72.675790</td>\n",
       "      <td>-8.499694e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104215</th>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>76.940002</td>\n",
       "      <td>76.260002</td>\n",
       "      <td>76.470001</td>\n",
       "      <td>76.470001</td>\n",
       "      <td>8229700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107548</td>\n",
       "      <td>-0.972858</td>\n",
       "      <td>31.975492</td>\n",
       "      <td>35.117121</td>\n",
       "      <td>31.775404</td>\n",
       "      <td>0.786087</td>\n",
       "      <td>81.525829</td>\n",
       "      <td>78.243500</td>\n",
       "      <td>74.961171</td>\n",
       "      <td>-2.688251e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104216</th>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>76.209999</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.080002</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>7060400.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069077</td>\n",
       "      <td>-0.990127</td>\n",
       "      <td>31.851847</td>\n",
       "      <td>48.597552</td>\n",
       "      <td>38.712818</td>\n",
       "      <td>0.759224</td>\n",
       "      <td>81.303475</td>\n",
       "      <td>78.057500</td>\n",
       "      <td>74.811525</td>\n",
       "      <td>-2.758855e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104217</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>76.239998</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>76.059998</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>8218000.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054652</td>\n",
       "      <td>-1.003790</td>\n",
       "      <td>29.688704</td>\n",
       "      <td>55.025431</td>\n",
       "      <td>46.246701</td>\n",
       "      <td>0.732850</td>\n",
       "      <td>80.964170</td>\n",
       "      <td>77.832500</td>\n",
       "      <td>74.700830</td>\n",
       "      <td>-2.841035e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104218</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>76.269997</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.050003</td>\n",
       "      <td>76.330002</td>\n",
       "      <td>76.330002</td>\n",
       "      <td>15641700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018917</td>\n",
       "      <td>-1.008519</td>\n",
       "      <td>32.913052</td>\n",
       "      <td>73.940933</td>\n",
       "      <td>59.187972</td>\n",
       "      <td>0.711932</td>\n",
       "      <td>80.569554</td>\n",
       "      <td>77.624500</td>\n",
       "      <td>74.679446</td>\n",
       "      <td>-2.684618e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104219</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>76.849998</td>\n",
       "      <td>76.320000</td>\n",
       "      <td>76.570000</td>\n",
       "      <td>76.570000</td>\n",
       "      <td>7340800.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028984</td>\n",
       "      <td>-1.001273</td>\n",
       "      <td>36.200731</td>\n",
       "      <td>85.986580</td>\n",
       "      <td>71.650981</td>\n",
       "      <td>0.698937</td>\n",
       "      <td>80.167620</td>\n",
       "      <td>77.442500</td>\n",
       "      <td>74.717380</td>\n",
       "      <td>-2.611210e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104220 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date       open       high        low      close  adj close  \\\n",
       "0      2012-11-14  77.928574  78.207146  76.597145  76.697144  69.613815   \n",
       "1      2012-11-15  76.790001  77.071426  74.660004  75.088570  68.153778   \n",
       "2      2012-11-16  75.028572  75.714287  72.250000  75.382858  68.420891   \n",
       "3      2012-11-19  77.244286  81.071426  77.125717  80.818573  73.354591   \n",
       "4      2012-11-20  81.701431  81.707146  79.225716  80.129997  72.729614   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "104215 2017-08-28  76.900002  76.940002  76.260002  76.470001  76.470001   \n",
       "104216 2017-08-29  76.209999  76.489998  76.080002  76.449997  76.449997   \n",
       "104217 2017-08-30  76.239998  76.449997  76.059998  76.099998  76.099998   \n",
       "104218 2017-08-31  76.269997  76.489998  76.050003  76.330002  76.330002   \n",
       "104219 2017-09-01  76.370003  76.849998  76.320000  76.570000  76.570000   \n",
       "\n",
       "             volume ticker text  sentiment  ...  MACDh_12_26_9  MACDs_12_26_9  \\\n",
       "0       119292600.0   AAPL  NaN        0.0  ...      -0.538077      -3.703588   \n",
       "1       197477700.0   AAPL  NaN        0.0  ...      -0.537725      -3.838019   \n",
       "2       316723400.0   AAPL  NaN        0.0  ...      -0.455544      -3.951905   \n",
       "3       205829400.0   AAPL  NaN        0.0  ...       0.002769      -3.951213   \n",
       "4       160688500.0   AAPL  NaN        0.0  ...       0.281964      -3.880722   \n",
       "...             ...    ...  ...        ...  ...            ...            ...   \n",
       "104215    8229700.0    XOM  NaN        0.0  ...      -0.107548      -0.972858   \n",
       "104216    7060400.0    XOM  NaN        0.0  ...      -0.069077      -0.990127   \n",
       "104217    8218000.0    XOM  NaN        0.0  ...      -0.054652      -1.003790   \n",
       "104218   15641700.0    XOM  NaN        0.0  ...      -0.018917      -1.008519   \n",
       "104219    7340800.0    XOM  NaN        0.0  ...       0.028984      -1.001273   \n",
       "\n",
       "           RSI_14  STOCHRSIk_14_14_3_3  STOCHRSId_14_14_3_3   ATRr_14  \\\n",
       "0       25.771012            21.054629            19.582354  2.377852   \n",
       "1       23.573491            15.792949            19.993462  2.380310   \n",
       "2       24.836267            12.243346            16.363641  2.459547   \n",
       "3       43.429047            39.692073            22.576123  2.695187   \n",
       "4       42.011353            69.373201            40.436207  2.679612   \n",
       "...           ...                  ...                  ...       ...   \n",
       "104215  31.975492            35.117121            31.775404  0.786087   \n",
       "104216  31.851847            48.597552            38.712818  0.759224   \n",
       "104217  29.688704            55.025431            46.246701  0.732850   \n",
       "104218  32.913052            73.940933            59.187972  0.711932   \n",
       "104219  36.200731            85.986580            71.650981  0.698937   \n",
       "\n",
       "         BB_upper  BB_middle   BB_lower           OBV  \n",
       "0       94.648550  84.401357  74.154164 -1.014356e+09  \n",
       "1       93.761634  83.514428  73.267223 -1.211834e+09  \n",
       "2       92.716200  82.679214  72.642228 -8.951103e+08  \n",
       "3       91.617665  82.201285  72.784906 -6.892809e+08  \n",
       "4       91.027780  81.851785  72.675790 -8.499694e+08  \n",
       "...           ...        ...        ...           ...  \n",
       "104215  81.525829  78.243500  74.961171 -2.688251e+08  \n",
       "104216  81.303475  78.057500  74.811525 -2.758855e+08  \n",
       "104217  80.964170  77.832500  74.700830 -2.841035e+08  \n",
       "104218  80.569554  77.624500  74.679446 -2.684618e+08  \n",
       "104219  80.167620  77.442500  74.717380 -2.611210e+08  \n",
       "\n",
       "[104220 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_check = ['EMA_12', 'EMA_26','EMA_50','MACD_12_26_9','MACDh_12_26_9','MACDs_12_26_9','RSI_14','ATRr_14','STOCHRSIk_14_14_3_3','STOCHRSId_14_14_3_3','ATRr_14','BB_upper','BB_middle','BB_lower','OBV']\n",
    "master_df = master_df.dropna(subset=columns_to_check)\n",
    "\n",
    "\n",
    "master_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4D6EfvoYDrGY",
    "outputId": "a09ae550-5787-45c6-81aa-4bf74311f09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'open', 'high', 'low', 'close', 'adj close', 'volume', 'ticker',\n",
      "       'text', 'sentiment', 'emotion_anger', 'emotion_disgust', 'emotion_fear',\n",
      "       'emotion_joy', 'emotion_neutral', 'emotion_sadness', 'emotion_surprize',\n",
      "       'stance_positive', 'stance_negative', 'sector', 'company_name',\n",
      "       'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9',\n",
      "       'MACDs_12_26_9', 'RSI_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3',\n",
      "       'ATRr_14', 'BB_upper', 'BB_middle', 'BB_lower', 'OBV'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(master_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ytoycwxUDtS3"
   },
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'open', 'high', 'low', 'close', 'volume'\n",
    "    # 'stance_positive', 'stance_negative',\n",
    "    # 'sentiment'\n",
    "]\n",
    "\n",
    "new_indicator_columns = [\n",
    "    'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',\n",
    "    'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3',\n",
    "    'BB_upper', 'BB_middle', 'BB_lower', 'OBV'\n",
    "]\n",
    "feature_columns.extend(new_indicator_columns)\n",
    "\n",
    "sequence_length=12\n",
    "\n",
    "\n",
    "\n",
    "all_pipelines = {}\n",
    "all_results_dfs = {}\n",
    "all_analyses = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RBV6cWZNDuCW",
    "outputId": "07c614de-47f3-4115-a4f2-0a8e11405f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "  RUNNING PIPELINE FOR: LSTM\n",
      "=========================\n",
      "\n",
      "Pipeline initialized for a 'regression' problem with horizon 1 steps. Device: cpu\n",
      "Processing 88 companies with LSTM model...\n",
      "Problem type: regression\n",
      "Sequence length: 12\n",
      "Features: ['open', 'high', 'low', 'close', 'volume', 'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3', 'BB_upper', 'BB_middle', 'BB_lower', 'OBV']\n",
      "\n",
      "[1/88] Processing AAPL...\n",
      "\n",
      "Processing AAPL (Consumer Goods)...\n",
      "  Epoch 010 - train 0.31088 | val 0.68258\n",
      "  Epoch 017 - train 0.31521 | val 0.75467\n",
      "  Regression -> MSE: 0.000252, MAE: 0.012781, R²: -0.0276\n",
      "\n",
      "[2/88] Processing ABB...\n",
      "\n",
      "Processing ABB (Industrial Goods)...\n",
      "Insufficient data for ABB (62 < 163). Skipping...\n",
      "\n",
      "[3/88] Processing ABBV...\n",
      "\n",
      "Processing ABBV (Healthcare)...\n",
      "  Epoch 010 - train 0.33628 | val 0.45298\n",
      "  Epoch 020 - train 0.30384 | val 0.40971\n",
      "  Regression -> MSE: 0.000680, MAE: 0.019337, R²: 0.0168\n",
      "\n",
      "[4/88] Processing AEP...\n",
      "\n",
      "Processing AEP (Utilities)...\n",
      "Insufficient data for AEP (109 < 163). Skipping...\n",
      "\n",
      "[5/88] Processing AGFS...\n",
      "\n",
      "Processing AGFS (Conglomerates)...\n",
      "Insufficient data for AGFS (2 < 163). Skipping...\n",
      "\n",
      "[6/88] Processing AMGN...\n",
      "\n",
      "Processing AMGN (Healthcare)...\n",
      "  Epoch 010 - train 0.35151 | val 0.75612\n",
      "  Epoch 020 - train 0.34021 | val 0.78764\n",
      "  Epoch 022 - train 0.34982 | val 0.80688\n",
      "  Regression -> MSE: 0.000359, MAE: 0.015157, R²: -0.0390\n",
      "\n",
      "[7/88] Processing AMZN...\n",
      "\n",
      "Processing AMZN (Services)...\n",
      "  Epoch 010 - train 0.29114 | val 0.41494\n",
      "  Epoch 017 - train 0.28119 | val 0.43341\n",
      "  Regression -> MSE: 0.000372, MAE: 0.014817, R²: -0.0776\n",
      "\n",
      "[8/88] Processing BA...\n",
      "\n",
      "Processing BA (Industrial Goods)...\n",
      "  Epoch 010 - train 0.31804 | val 0.38105\n",
      "  Epoch 020 - train 0.31314 | val 0.46229\n",
      "  Epoch 022 - train 0.31012 | val 0.52169\n",
      "  Regression -> MSE: 0.000197, MAE: 0.009347, R²: 0.0189\n",
      "\n",
      "[9/88] Processing BABA...\n",
      "\n",
      "Processing BABA (Services)...\n",
      "  Epoch 010 - train 0.31147 | val 0.90074\n",
      "  Epoch 020 - train 0.29344 | val 1.20392\n",
      "  Regression -> MSE: 0.000582, MAE: 0.018487, R²: -0.0164\n",
      "\n",
      "[10/88] Processing BAC...\n",
      "\n",
      "Processing BAC (Financial)...\n",
      "  Epoch 010 - train 0.36578 | val 0.95916\n",
      "  Epoch 019 - train 0.32910 | val 0.93166\n",
      "  Regression -> MSE: 0.000242, MAE: 0.012028, R²: 0.0156\n",
      "\n",
      "[11/88] Processing BBL...\n",
      "\n",
      "Processing BBL (Basic Matierials)...\n",
      "Insufficient data for BBL (69 < 163). Skipping...\n",
      "\n",
      "[12/88] Processing BCH...\n",
      "\n",
      "Processing BCH (Financial)...\n",
      "Insufficient data for BCH (5 < 163). Skipping...\n",
      "\n",
      "[13/88] Processing BHP...\n",
      "\n",
      "Processing BHP (Basic Matierials)...\n",
      "  Epoch 010 - train 0.32831 | val 0.74930\n",
      "  Epoch 016 - train 0.31650 | val 0.92505\n",
      "  Regression -> MSE: 0.000616, MAE: 0.019491, R²: -0.0537\n",
      "\n",
      "[14/88] Processing BP...\n",
      "\n",
      "Processing BP (Basic Matierials)...\n",
      "  Epoch 010 - train 0.32874 | val 0.74758\n",
      "  Epoch 020 - train 0.29794 | val 0.86220\n",
      "  Regression -> MSE: 0.000354, MAE: 0.015844, R²: -0.0480\n",
      "\n",
      "[15/88] Processing BRK-A...\n",
      "\n",
      "Processing BRK-A (Financial)...\n",
      "Insufficient data for BRK-A (13 < 163). Skipping...\n",
      "\n",
      "[16/88] Processing BSAC...\n",
      "\n",
      "Processing BSAC (Financial)...\n",
      "Insufficient data for BSAC (11 < 163). Skipping...\n",
      "\n",
      "[17/88] Processing BUD...\n",
      "\n",
      "Processing BUD (Consumer Goods)...\n",
      "Insufficient data for BUD (96 < 163). Skipping...\n",
      "\n",
      "[18/88] Processing C...\n",
      "\n",
      "Processing C (Financial)...\n",
      "  Epoch 010 - train 0.34870 | val 0.70923\n",
      "  Epoch 016 - train 0.32878 | val 0.70547\n",
      "  Regression -> MSE: 0.000241, MAE: 0.011509, R²: -0.0139\n",
      "\n",
      "[19/88] Processing CAT...\n",
      "\n",
      "Processing CAT (Industrial Goods)...\n",
      "  Epoch 010 - train 0.30482 | val 1.18409\n",
      "  Epoch 016 - train 0.29795 | val 1.10097\n",
      "  Regression -> MSE: 0.000311, MAE: 0.013527, R²: -0.0161\n",
      "\n",
      "[20/88] Processing CELG...\n",
      "\n",
      "Processing CELG (Healthcare)...\n",
      "  Epoch 010 - train 0.37293 | val 0.52473\n",
      "  Epoch 016 - train 0.33862 | val 0.53928\n",
      "  Regression -> MSE: 0.000667, MAE: 0.019405, R²: 0.0038\n",
      "\n",
      "[21/88] Processing CHL...\n",
      "\n",
      "Processing CHL (Technology)...\n",
      "Insufficient data for CHL (96 < 163). Skipping...\n",
      "\n",
      "[22/88] Processing CHTR...\n",
      "\n",
      "Processing CHTR (Services)...\n",
      "Insufficient data for CHTR (137 < 163). Skipping...\n",
      "\n",
      "[23/88] Processing CMCSA...\n",
      "\n",
      "Processing CMCSA (Services)...\n",
      "  Epoch 010 - train 0.33913 | val 0.47177\n",
      "  Epoch 016 - train 0.30562 | val 1.19393\n",
      "  Regression -> MSE: 0.000150, MAE: 0.009479, R²: 0.0005\n",
      "\n",
      "[24/88] Processing CODI...\n",
      "\n",
      "Processing CODI (Conglomerates)...\n",
      "Insufficient data for CODI (29 < 163). Skipping...\n",
      "\n",
      "[25/88] Processing CSCO...\n",
      "\n",
      "Processing CSCO (Technology)...\n",
      "  Epoch 010 - train 0.30221 | val 0.60322\n",
      "  Epoch 020 - train 0.27206 | val 0.65611\n",
      "  Regression -> MSE: 0.000225, MAE: 0.010799, R²: -0.0012\n",
      "\n",
      "[26/88] Processing CVX...\n",
      "\n",
      "Processing CVX (Basic Matierials)...\n",
      "  Epoch 010 - train 0.33126 | val 1.20769\n",
      "  Epoch 016 - train 0.29328 | val 1.60012\n",
      "  Regression -> MSE: 0.000376, MAE: 0.016451, R²: 0.0007\n",
      "\n",
      "[27/88] Processing D...\n",
      "\n",
      "Processing D (Utilities)...\n",
      "  Epoch 010 - train 0.33747 | val 0.62260\n",
      "  Epoch 018 - train 0.32072 | val 0.75540\n",
      "  Regression -> MSE: 0.000104, MAE: 0.007809, R²: 0.0182\n",
      "\n",
      "[28/88] Processing DHR...\n",
      "\n",
      "Processing DHR (Industrial Goods)...\n",
      "Insufficient data for DHR (91 < 163). Skipping...\n",
      "\n",
      "[29/88] Processing DIS...\n",
      "\n",
      "Processing DIS (Services)...\n",
      "  Epoch 010 - train 0.33456 | val 1.13763\n",
      "  Epoch 016 - train 0.33431 | val 1.33074\n",
      "  Regression -> MSE: 0.000163, MAE: 0.010225, R²: -0.0164\n",
      "\n",
      "[30/88] Processing DUK...\n",
      "\n",
      "Processing DUK (Utilities)...\n",
      "Insufficient data for DUK (117 < 163). Skipping...\n",
      "\n",
      "[31/88] Processing EXC...\n",
      "\n",
      "Processing EXC (Utilities)...\n",
      "Insufficient data for EXC (136 < 163). Skipping...\n",
      "\n",
      "[32/88] Processing FB...\n",
      "\n",
      "Processing FB (Technology)...\n",
      "  Epoch 010 - train 0.33794 | val 0.86132\n",
      "  Epoch 018 - train 0.31686 | val 1.25050\n",
      "  Regression -> MSE: 0.000272, MAE: 0.013008, R²: -0.0118\n",
      "\n",
      "[33/88] Processing GD...\n",
      "\n",
      "Processing GD (Industrial Goods)...\n",
      "Insufficient data for GD (153 < 163). Skipping...\n",
      "\n",
      "[34/88] Processing GE...\n",
      "\n",
      "Processing GE (Industrial Goods)...\n",
      "  Epoch 010 - train 0.30229 | val 0.57070\n",
      "  Epoch 020 - train 0.28448 | val 0.57599\n",
      "  Epoch 030 - train 0.27912 | val 0.55365\n",
      "  Epoch 040 - train 0.26021 | val 0.56882\n",
      "  Epoch 044 - train 0.24307 | val 0.57246\n",
      "  Regression -> MSE: 0.000253, MAE: 0.012292, R²: -0.7155\n",
      "\n",
      "[35/88] Processing GMRE...\n",
      "\n",
      "Processing GMRE (Conglomerates)...\n",
      "Insufficient data for GMRE (0 < 163). Skipping...\n",
      "\n",
      "[36/88] Processing GOOG...\n",
      "\n",
      "Processing GOOG (Technology)...\n",
      "  Epoch 010 - train 0.35086 | val 0.84256\n",
      "  Epoch 019 - train 0.31864 | val 0.74774\n",
      "  Regression -> MSE: 0.000232, MAE: 0.010737, R²: -0.0227\n",
      "\n",
      "[37/88] Processing HD...\n",
      "\n",
      "Processing HD (Services)...\n",
      "  Epoch 010 - train 0.35249 | val 0.59427\n",
      "  Epoch 020 - train 0.30059 | val 0.69348\n",
      "  Epoch 023 - train 0.28750 | val 0.71296\n",
      "  Regression -> MSE: 0.000214, MAE: 0.011265, R²: -0.4437\n",
      "\n",
      "[38/88] Processing HON...\n",
      "\n",
      "Processing HON (Industrial Goods)...\n",
      "Insufficient data for HON (142 < 163). Skipping...\n",
      "\n",
      "[39/88] Processing HRG...\n",
      "\n",
      "Processing HRG (Conglomerates)...\n",
      "Insufficient data for HRG (30 < 163). Skipping...\n",
      "\n",
      "[40/88] Processing HSBC...\n",
      "\n",
      "Processing HSBC (Financial)...\n",
      "Insufficient data for HSBC (71 < 163). Skipping...\n",
      "\n",
      "[41/88] Processing IEP...\n",
      "\n",
      "Processing IEP (Conglomerates)...\n",
      "Insufficient data for IEP (58 < 163). Skipping...\n",
      "\n",
      "[42/88] Processing INTC...\n",
      "\n",
      "Processing INTC (Technology)...\n",
      "  Epoch 010 - train 0.32268 | val 0.49246\n",
      "  Epoch 017 - train 0.32162 | val 0.50947\n",
      "  Regression -> MSE: 0.000179, MAE: 0.010580, R²: -0.0113\n",
      "\n",
      "[43/88] Processing JNJ...\n",
      "\n",
      "Processing JNJ (Healthcare)...\n",
      "  Epoch 010 - train 0.35003 | val 0.64887\n",
      "  Epoch 018 - train 0.33051 | val 0.79655\n",
      "  Regression -> MSE: 0.000083, MAE: 0.006877, R²: 0.0061\n",
      "\n",
      "[44/88] Processing JPM...\n",
      "\n",
      "Processing JPM (Financial)...\n",
      "  Epoch 010 - train 0.32830 | val 0.65109\n",
      "  Epoch 017 - train 0.30786 | val 0.64433\n",
      "  Regression -> MSE: 0.000224, MAE: 0.011305, R²: -0.0233\n",
      "\n",
      "[45/88] Processing KO...\n",
      "\n",
      "Processing KO (Consumer Goods)...\n",
      "  Epoch 010 - train 0.27176 | val 0.28425\n",
      "  Epoch 020 - train 0.25425 | val 0.31845\n",
      "  Epoch 022 - train 0.25427 | val 0.32791\n",
      "  Regression -> MSE: 0.000092, MAE: 0.007937, R²: -0.0032\n",
      "\n",
      "[46/88] Processing LMT...\n",
      "\n",
      "Processing LMT (Industrial Goods)...\n",
      "  Epoch 010 - train 0.35048 | val 0.21800\n",
      "  Epoch 020 - train 0.33886 | val 0.27485\n",
      "  Regression -> MSE: 0.000134, MAE: 0.009574, R²: -0.0659\n",
      "\n",
      "[47/88] Processing MA...\n",
      "\n",
      "Processing MA (Financial)...\n",
      "  Epoch 010 - train 0.35723 | val 0.57126\n",
      "  Epoch 016 - train 0.33740 | val 0.45372\n",
      "  Regression -> MSE: 0.000255, MAE: 0.013270, R²: -0.0735\n",
      "\n",
      "[48/88] Processing MCD...\n",
      "\n",
      "Processing MCD (Services)...\n",
      "  Epoch 010 - train 0.32431 | val 0.46054\n",
      "  Epoch 020 - train 0.33104 | val 0.48321\n",
      "  Epoch 024 - train 0.30050 | val 0.49337\n",
      "  Regression -> MSE: 0.000295, MAE: 0.011973, R²: -0.7998\n",
      "\n",
      "[49/88] Processing MDT...\n",
      "\n",
      "Processing MDT (Healthcare)...\n",
      "  Epoch 010 - train 0.39628 | val 0.23786\n",
      "  Epoch 020 - train 0.32867 | val 0.30651\n",
      "  Epoch 026 - train 0.32911 | val 0.51175\n",
      "  Regression -> MSE: 0.000230, MAE: 0.012050, R²: 0.0167\n",
      "\n",
      "[50/88] Processing MMM...\n",
      "\n",
      "Processing MMM (Industrial Goods)...\n",
      "  Epoch 010 - train 0.34839 | val 0.74078\n",
      "  Epoch 016 - train 0.35890 | val 0.82547\n",
      "  Regression -> MSE: 0.000251, MAE: 0.010547, R²: -0.0011\n",
      "\n",
      "[51/88] Processing MO...\n",
      "\n",
      "Processing MO (Consumer Goods)...\n",
      "  Epoch 010 - train 0.32976 | val 0.66535\n",
      "  Epoch 016 - train 0.30816 | val 0.86850\n",
      "  Regression -> MSE: 0.000133, MAE: 0.008191, R²: -0.0006\n",
      "\n",
      "[52/88] Processing MRK...\n",
      "\n",
      "Processing MRK (Healthcare)...\n",
      "  Epoch 010 - train 0.32990 | val 0.57819\n",
      "  Epoch 020 - train 0.30862 | val 0.59625\n",
      "  Epoch 021 - train 0.31333 | val 0.60973\n",
      "  Regression -> MSE: 0.000191, MAE: 0.010633, R²: -0.0122\n",
      "\n",
      "[53/88] Processing MSFT...\n",
      "\n",
      "Processing MSFT (Technology)...\n",
      "  Epoch 010 - train 0.30368 | val 0.52942\n",
      "  Epoch 020 - train 0.27865 | val 0.51817\n",
      "  Epoch 021 - train 0.28714 | val 0.51373\n",
      "  Regression -> MSE: 0.000264, MAE: 0.010440, R²: -0.0235\n",
      "\n",
      "[54/88] Processing NEE...\n",
      "\n",
      "Processing NEE (Utilities)...\n",
      "Insufficient data for NEE (119 < 163). Skipping...\n",
      "\n",
      "[55/88] Processing NGG...\n",
      "\n",
      "Processing NGG (Utilities)...\n",
      "Insufficient data for NGG (28 < 163). Skipping...\n",
      "\n",
      "[56/88] Processing NVS...\n",
      "\n",
      "Processing NVS (Healthcare)...\n",
      "  Epoch 010 - train 0.33265 | val 0.35707\n",
      "  Epoch 017 - train 0.32411 | val 0.39641\n",
      "  Regression -> MSE: 0.000163, MAE: 0.010173, R²: -0.0011\n",
      "\n",
      "[57/88] Processing ORCL...\n",
      "\n",
      "Processing ORCL (Technology)...\n",
      "  Epoch 010 - train 0.26041 | val 0.59936\n",
      "  Epoch 016 - train 0.28712 | val 0.81100\n",
      "  Regression -> MSE: 0.000183, MAE: 0.010451, R²: 0.0005\n",
      "\n",
      "[58/88] Processing PCG...\n",
      "\n",
      "Processing PCG (Utilities)...\n",
      "Insufficient data for PCG (101 < 163). Skipping...\n",
      "\n",
      "[59/88] Processing PCLN...\n",
      "\n",
      "Processing PCLN (Services)...\n",
      "  Epoch 010 - train 0.30452 | val 0.45260\n",
      "  Epoch 019 - train 0.27322 | val 0.47163\n",
      "  Regression -> MSE: 0.000378, MAE: 0.013768, R²: -0.0843\n",
      "\n",
      "[60/88] Processing PEP...\n",
      "\n",
      "Processing PEP (Consumer Goods)...\n",
      "  Epoch 010 - train 0.31107 | val 0.24643\n",
      "  Epoch 020 - train 0.27514 | val 0.29490\n",
      "  Epoch 023 - train 0.29444 | val 0.33663\n",
      "  Regression -> MSE: 0.000123, MAE: 0.007878, R²: -0.1905\n",
      "\n",
      "[61/88] Processing PFE...\n",
      "\n",
      "Processing PFE (Healthcare)...\n",
      "  Epoch 010 - train 0.34039 | val 0.78921\n",
      "  Epoch 017 - train 0.32311 | val 0.88724\n",
      "  Regression -> MSE: 0.000216, MAE: 0.011177, R²: -0.0039\n",
      "\n",
      "[62/88] Processing PG...\n",
      "\n",
      "Processing PG (Consumer Goods)...\n",
      "  Epoch 010 - train 0.32494 | val 0.99338\n",
      "  Epoch 016 - train 0.30199 | val 1.33433\n",
      "  Regression -> MSE: 0.000093, MAE: 0.007599, R²: -0.1745\n",
      "\n",
      "[63/88] Processing PICO...\n",
      "\n",
      "Processing PICO (Conglomerates)...\n",
      "Insufficient data for PICO (16 < 163). Skipping...\n",
      "\n",
      "[64/88] Processing PM...\n",
      "\n",
      "Processing PM (Consumer Goods)...\n",
      "Insufficient data for PM (145 < 163). Skipping...\n",
      "\n",
      "[65/88] Processing PPL...\n",
      "\n",
      "Processing PPL (Utilities)...\n",
      "Insufficient data for PPL (108 < 163). Skipping...\n",
      "\n",
      "[66/88] Processing PTR...\n",
      "\n",
      "Processing PTR (Basic Matierials)...\n",
      "Insufficient data for PTR (44 < 163). Skipping...\n",
      "\n",
      "[67/88] Processing RDS-B...\n",
      "\n",
      "Processing RDS-B (Basic Matierials)...\n",
      "Insufficient data for RDS-B (3 < 163). Skipping...\n",
      "\n",
      "[68/88] Processing REX...\n",
      "\n",
      "Processing REX (Conglomerates)...\n",
      "Insufficient data for REX (70 < 163). Skipping...\n",
      "\n",
      "[69/88] Processing SLB...\n",
      "\n",
      "Processing SLB (Basic Matierials)...\n",
      "  Epoch 010 - train 0.31239 | val 0.31313\n",
      "  Epoch 016 - train 0.30370 | val 0.34296\n",
      "  Regression -> MSE: 0.000344, MAE: 0.015473, R²: -0.0472\n",
      "\n",
      "[70/88] Processing SNP...\n",
      "\n",
      "Processing SNP (Basic Matierials)...\n",
      "Insufficient data for SNP (23 < 163). Skipping...\n",
      "\n",
      "[71/88] Processing SNY...\n",
      "\n",
      "Processing SNY (Healthcare)...\n",
      "Insufficient data for SNY (143 < 163). Skipping...\n",
      "\n",
      "[72/88] Processing SO...\n",
      "\n",
      "Processing SO (Utilities)...\n",
      "  Epoch 010 - train 0.32514 | val 0.22560\n",
      "  Epoch 020 - train 0.29388 | val 0.25660\n",
      "  Epoch 024 - train 0.29784 | val 0.32269\n",
      "  Regression -> MSE: 0.000247, MAE: 0.011147, R²: -0.0366\n",
      "\n",
      "[73/88] Processing SPLP...\n",
      "\n",
      "Processing SPLP (Conglomerates)...\n",
      "Insufficient data for SPLP (5 < 163). Skipping...\n",
      "\n",
      "[74/88] Processing SRE...\n",
      "\n",
      "Processing SRE (Utilities)...\n",
      "Insufficient data for SRE (69 < 163). Skipping...\n",
      "\n",
      "[75/88] Processing T...\n",
      "\n",
      "Processing T (Technology)...\n",
      "  Epoch 010 - train 0.30641 | val 0.91733\n",
      "  Epoch 018 - train 0.29766 | val 1.02634\n",
      "  Regression -> MSE: 0.000079, MAE: 0.007032, R²: 0.0103\n",
      "\n",
      "[76/88] Processing TM...\n",
      "\n",
      "Processing TM (Consumer Goods)...\n",
      "Insufficient data for TM (117 < 163). Skipping...\n",
      "\n",
      "[77/88] Processing TOT...\n",
      "\n",
      "Processing TOT (Basic Matierials)...\n",
      "Insufficient data for TOT (93 < 163). Skipping...\n",
      "\n",
      "[78/88] Processing TSM...\n",
      "\n",
      "Processing TSM (Technology)...\n",
      "Insufficient data for TSM (87 < 163). Skipping...\n",
      "\n",
      "[79/88] Processing UL...\n",
      "\n",
      "Processing UL (Consumer Goods)...\n",
      "Insufficient data for UL (61 < 163). Skipping...\n",
      "\n",
      "[80/88] Processing UN...\n",
      "\n",
      "Processing UN (Consumer Goods)...\n",
      "Insufficient data for UN (54 < 163). Skipping...\n",
      "\n",
      "[81/88] Processing UNH...\n",
      "\n",
      "Processing UNH (Healthcare)...\n",
      "  Epoch 010 - train 0.35350 | val 0.44818\n",
      "  Epoch 020 - train 0.30905 | val 0.70540\n",
      "  Epoch 021 - train 0.32981 | val 0.70597\n",
      "  Regression -> MSE: 0.000359, MAE: 0.015268, R²: 0.0027\n",
      "\n",
      "[82/88] Processing UPS...\n",
      "\n",
      "Processing UPS (Services)...\n",
      "  Epoch 010 - train 0.30635 | val 0.47860\n",
      "  Epoch 020 - train 0.29221 | val 0.45072\n",
      "  Epoch 030 - train 0.28623 | val 0.45707\n",
      "  Epoch 032 - train 0.29643 | val 0.45927\n",
      "  Regression -> MSE: 0.000104, MAE: 0.007252, R²: -0.0791\n",
      "\n",
      "[83/88] Processing UTX...\n",
      "\n",
      "Processing UTX (Industrial Goods)...\n",
      "  Epoch 010 - train 0.34346 | val 0.75967\n",
      "  Epoch 016 - train 0.32483 | val 1.07538\n",
      "  Regression -> MSE: 0.000183, MAE: 0.010465, R²: -0.0138\n",
      "\n",
      "[84/88] Processing V...\n",
      "\n",
      "Processing V (Financial)...\n",
      "  Epoch 010 - train 0.30852 | val 0.95938\n",
      "  Epoch 016 - train 0.32561 | val 0.73051\n",
      "  Regression -> MSE: 0.000220, MAE: 0.011633, R²: -0.0025\n",
      "\n",
      "[85/88] Processing VZ...\n",
      "\n",
      "Processing VZ (Technology)...\n",
      "  Epoch 010 - train 0.32308 | val 0.84013\n",
      "  Epoch 016 - train 0.31726 | val 0.93560\n",
      "  Regression -> MSE: 0.000092, MAE: 0.007684, R²: -0.0122\n",
      "\n",
      "[86/88] Processing WFC...\n",
      "\n",
      "Processing WFC (Financial)...\n",
      "  Epoch 010 - train 0.36997 | val 1.40831\n",
      "  Epoch 016 - train 0.36321 | val 1.81473\n",
      "  Regression -> MSE: 0.000161, MAE: 0.010061, R²: 0.0003\n",
      "\n",
      "[87/88] Processing WMT...\n",
      "\n",
      "Processing WMT (Services)...\n",
      "  Epoch 010 - train 0.32997 | val 0.88333\n",
      "  Epoch 017 - train 0.30478 | val 0.82875\n",
      "  Regression -> MSE: 0.000117, MAE: 0.008855, R²: -0.0269\n",
      "\n",
      "[88/88] Processing XOM...\n",
      "\n",
      "Processing XOM (Basic Matierials)...\n",
      "  Epoch 010 - train 0.34319 | val 0.70835\n",
      "  Epoch 016 - train 0.33851 | val 0.74689\n",
      "  Regression -> MSE: 0.000233, MAE: 0.012327, R²: 0.0010\n",
      "\n",
      "================================================================================\n",
      "Pipeline completed: 50/88 companies processed successfully\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STOCK PREDICTION PIPELINE RESULTS\n",
      "================================================================================\n",
      "Model: LSTM | Problem: regression\n",
      "Companies analyzed: 50\n",
      "Average samples per company: 320\n",
      "\n",
      "==================================================\n",
      "OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Mean Squared Error:     0.000250 (±0.000142)\n",
      "Mean Absolute Error:    0.011708 (±0.003265)\n",
      "R² Score:              -0.0616 (±0.1611)\n",
      "Directional Accuracy:   0.4978 (±0.0711)\n",
      "Matthews Correlation:   0.0316 (±0.0945)\n",
      "F1 Score:              0.4560 (±0.2887)\n",
      "Precision:             0.3996 (±0.2422)\n",
      "Recall:                0.6294 (±0.4322)\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE BY SECTOR\n",
      "==================================================\n",
      "Utilities            | Acc: 0.530±0.012 | MCC: 0.001 | Companies: 2\n",
      "Technology           | Acc: 0.528±0.056 | MCC: 0.088 | Companies: 8\n",
      "Consumer Goods       | Acc: 0.520±0.116 | MCC: 0.099 | Companies: 5\n",
      "Healthcare           | Acc: 0.520±0.032 | MCC: -0.005 | Companies: 9\n",
      "Financial            | Acc: 0.516±0.054 | MCC: 0.016 | Companies: 6\n",
      "Industrial Goods     | Acc: 0.481±0.105 | MCC: 0.053 | Companies: 6\n",
      "Services             | Acc: 0.470±0.054 | MCC: 0.001 | Companies: 9\n",
      "Basic Matierials     | Acc: 0.422±0.057 | MCC: 0.000 | Companies: 5\n",
      "\n",
      "==================================================\n",
      "TOP 10 PERFORMERS (by Directional Accuracy)\n",
      "==================================================\n",
      "PEP                  | Consumer Goods  | Acc: 0.682 | MCC: 0.390\n",
      "BA                   | Industrial Goods | Acc: 0.654 | MCC: 0.354\n",
      "GOOG                 | Technology      | Acc: 0.632 | MCC: 0.300\n",
      "MSFT                 | Technology      | Acc: 0.581 | MCC: 0.220\n",
      "JNJ                  | Healthcare      | Acc: 0.576 | MCC: 0.000\n",
      "C                    | Financial       | Acc: 0.563 | MCC: 0.000\n",
      "KO                   | Consumer Goods  | Acc: 0.561 | MCC: 0.106\n",
      "FB                   | Technology      | Acc: 0.558 | MCC: 0.181\n",
      "UPS                  | Services        | Acc: 0.556 | MCC: 0.000\n",
      "ABBV                 | Healthcare      | Acc: 0.553 | MCC: 0.048\n",
      "Results saved to results/benchmarking/regression/LSTM.csv\n",
      "\n",
      "Displaying first 5 rows of LSTM results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>sector</th>\n",
       "      <th>model_type</th>\n",
       "      <th>problem_type</th>\n",
       "      <th>horizon_steps</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>micro_accuracy</th>\n",
       "      <th>expected_return_mean</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>...</th>\n",
       "      <th>mcc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>directional_accuracy</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>train_samples</th>\n",
       "      <th>val_samples</th>\n",
       "      <th>test_samples</th>\n",
       "      <th>epochs_trained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.012781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619469</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>468</td>\n",
       "      <td>312</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.019337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048457</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>283</td>\n",
       "      <td>189</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMGN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.015157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018214</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>293</td>\n",
       "      <td>196</td>\n",
       "      <td>49</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Services</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.014817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460526</td>\n",
       "      <td>458</td>\n",
       "      <td>306</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BA</td>\n",
       "      <td>Industrial Goods</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353649</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>315</td>\n",
       "      <td>211</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  company            sector model_type problem_type  horizon_steps  macro_f1  \\\n",
       "0    AAPL    Consumer Goods       LSTM   regression              1       NaN   \n",
       "1    ABBV        Healthcare       LSTM   regression              1       NaN   \n",
       "2    AMGN        Healthcare       LSTM   regression              1       NaN   \n",
       "3    AMZN          Services       LSTM   regression              1       NaN   \n",
       "4      BA  Industrial Goods       LSTM   regression              1       NaN   \n",
       "\n",
       "   micro_accuracy  expected_return_mean       mse       mae  ...       mcc  \\\n",
       "0             NaN                   NaN  0.000252  0.012781  ...  0.000000   \n",
       "1             NaN                   NaN  0.000680  0.019337  ...  0.048457   \n",
       "2             NaN                   NaN  0.000359  0.015157  ... -0.018214   \n",
       "3             NaN                   NaN  0.000372  0.014817  ...  0.000000   \n",
       "4             NaN                   NaN  0.000197  0.009347  ...  0.353649   \n",
       "\n",
       "         f1  precision    recall  directional_accuracy  n_samples  \\\n",
       "0  0.619469   0.448718  1.000000              0.448718        468   \n",
       "1  0.676923   0.564103  0.846154              0.553191        283   \n",
       "2  0.418605   0.529412  0.346154              0.479167        293   \n",
       "3  0.000000   0.000000  0.000000              0.460526        458   \n",
       "4  0.750000   0.613636  0.964286              0.653846        315   \n",
       "\n",
       "   train_samples  val_samples  test_samples  epochs_trained  \n",
       "0            312           78            78              17  \n",
       "1            189           47            47              20  \n",
       "2            196           49            48              22  \n",
       "3            306           76            76              17  \n",
       "4            211           52            52              22  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'agg_cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline_LSTM\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     28\u001b[0m    agg_cm, q_labels, interval_labels, display_labels \u001b[38;5;241m=\u001b[39m aggregate_confusions_with_median_edges(results_LSTM)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43magg_cm\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     plot_confusion(agg_cm, display_labels, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAggregated Confusion (Quantile buckets)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     agg_norm \u001b[38;5;241m=\u001b[39m agg_cm \u001b[38;5;241m/\u001b[39m agg_cm\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agg_cm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n{'='*25}\\n  RUNNING PIPELINE FOR: LSTM\\n{'='*25}\\n\")\n",
    "\n",
    "pipeline_LSTM = StockPredictionPipeline(\n",
    "    df=master_df,\n",
    "    feature_columns=feature_columns,\n",
    "    model_type='LSTM',\n",
    "    sequence_length=sequence_length,\n",
    "    problem_type='regression',\n",
    "    horizon_steps=1\n",
    ")\n",
    "\n",
    "results_LSTM = pipeline_LSTM.run_pipeline()\n",
    "\n",
    "if results_LSTM is not None and not results_LSTM.empty:\n",
    "    analysis_LSTM = pipeline_LSTM.analyze_results()\n",
    "    pipeline_LSTM.save_results(results_LSTM, output_dir='results/benchmarking/')\n",
    "\n",
    "    all_pipelines[\"LSTM\"] = pipeline_LSTM\n",
    "    all_results_dfs[\"LSTM\"] = results_LSTM\n",
    "    all_analyses[\"LSTM\"] = analysis_LSTM\n",
    "\n",
    "    print(\"\\nDisplaying first 5 rows of LSTM results:\")\n",
    "    display(results_LSTM.head())\n",
    "else:\n",
    "    print(f\"\\n[FAILED] Pipeline for LSTM did not produce any results.\")\n",
    "\n",
    "if pipeline_LSTM.problem_type in ['classification', 'multiclass']:\n",
    "   agg_cm, q_labels, interval_labels, display_labels = aggregate_confusions_with_median_edges(results_LSTM)\n",
    "if agg_cm is not None:\n",
    "    plot_confusion(agg_cm, display_labels, title=\"Aggregated Confusion (Quantile buckets)\")\n",
    "    agg_norm = agg_cm / agg_cm.sum(axis=1, keepdims=True).clip(min=1)\n",
    "    plot_confusion(agg_norm, display_labels, title=\"Aggregated Confusion (Row-normalized)\")\n",
    "\n",
    "\n",
    "    \n",
    "loss_df = pipeline_LSTM.get_loss_curves_df()\n",
    "\n",
    "pipeline_LSTM.save_loss_curves('results/benchmarking/')\n",
    "\n",
    "del pipeline_LSTM"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
