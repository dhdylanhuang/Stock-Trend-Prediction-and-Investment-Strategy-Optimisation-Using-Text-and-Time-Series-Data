{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Results from Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgZ_ylAADNb6",
    "outputId": "6134cc02-4bf2-4dbb-a201-b8fd4d0df03c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_ta_classic in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.3.37)\n",
      "Requirement already satisfied: numpy>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.1.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2025.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (1.16.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas>=2.0.0->pandas_ta_classic) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas_ta_classic in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.3.37)\n",
      "Requirement already satisfied: numpy>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.1.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2025.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (1.16.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas>=2.0.0->pandas_ta_classic) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (21.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas_ta_classic\n",
    "%pip install --upgrade pandas_ta_classic\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "befeuglqHy8f",
    "outputId": "a2bcece3-930c-4c77-c6ff-3473d1fde791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the problematic import line in squeeze_pro.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import site\n",
    "import os\n",
    "\n",
    "# Find the path to the pandas_ta_classic library and patch it\n",
    "pandas_ta_classic_path = None\n",
    "for sp in site.getsitepackages():\n",
    "    pandas_ta_classic_path = os.path.join(sp, 'pandas_ta_classic')\n",
    "    if os.path.exists(pandas_ta_classic_path):\n",
    "        break\n",
    "\n",
    "if pandas_ta_classic_path:\n",
    "    squeeze_pro_path = os.path.join(pandas_ta_classic_path, 'momentum', 'squeeze_pro.py')\n",
    "    if os.path.exists(squeeze_pro_path):\n",
    "        try:\n",
    "            with open(squeeze_pro_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            new_lines = []\n",
    "            fixed = False\n",
    "            for line in lines:\n",
    "                if \"from numpy import NaN as npNaN\" in line:\n",
    "                    new_lines.append(line.replace(\"from numpy import NaN as npNaN\", \"# from numpy import NaN as npNaN\\nimport numpy as np\\n\"))\n",
    "                    fixed = True\n",
    "                    print(\"Modified import statement in squeeze_pro.py\")\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "\n",
    "            if fixed:\n",
    "                with open(squeeze_pro_path, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "                print(\"Successfully patched pandas_ta_classic/momentum/squeeze_pro.py\")\n",
    "            else:\n",
    "                print(\"Could not find the problematic import line in squeeze_pro.py\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error modifying squeeze_pro.py: {e}\")\n",
    "    else:\n",
    "        print(f\"Could not find squeeze_pro.py at {squeeze_pro_path}\")\n",
    "else:\n",
    "    print(\"Could not find the pandas_ta_classic library installation path.\")\n",
    "\n",
    "import pandas_ta_classic as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: torch==2.8.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torchvision) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install scikit-learn\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, matthews_corrcoef,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import random\n",
    "\n",
    "from unicodedata import bidirectional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seeds(42)\n",
    "\n",
    "# Datasets\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class OrdinalSequenceDataset(Dataset):\n",
    "    def __init__(self, X, T):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.T = torch.tensor(T, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.T[idx]\n",
    "\n",
    "def make_cumulative_targets(y_int, K):\n",
    "    y = y_int.reshape(-1, 1)\n",
    "    ks = np.arange(K-1).reshape(1, -1)\n",
    "    return (y > ks).astype(np.float32)\n",
    "\n",
    "def decode_ordinal(probs, thr=0.5):\n",
    "    return (probs >= thr).sum(axis=1)\n",
    "\n",
    "# Models\n",
    "class OrdinalHeadCORN(nn.Module):\n",
    "    def __init__(self, in_dim, K):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, K-1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        return self.fc(h)\n",
    "\n",
    "\n",
    "class OrdinalHeadCORAL(nn.Module):\n",
    "    def __init__(self, in_dim, K):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(in_dim, 1, bias=False)\n",
    "        self._beta = nn.Parameter(torch.zeros(K-1))\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, h):\n",
    "        base = self.w(h)\n",
    "        deltas = self.softplus(self._beta)\n",
    "        b = torch.cumsum(deltas, dim=0)\n",
    "        return base - b\n",
    "\n",
    "\n",
    "class RNNHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared head:\n",
    "      - RNN stack (LSTM/GRU, uni/bi)\n",
    "      - BatchNorm + Dense(32, ReLU) + Dropout(0.3)\n",
    "      - Output layer (1 unit): linear (regression) or logits (classification)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, rnn_type='LSTM', bidirectional=False, problem_type='regression', n_classes=6, ordinal_head='coral'):\n",
    "        super().__init__()\n",
    "        hidden1 = 128\n",
    "        hidden2 = 64\n",
    "        self.problem_type = problem_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "        self.n_classes = n_classes\n",
    "        self.ordinal_head = ordinal_head\n",
    "\n",
    "        rnn_cls = {'LSTM': nn.LSTM, 'GRU': nn.GRU}[('GRU' if 'GRU' in self.rnn_type else 'LSTM')]\n",
    "\n",
    "        self.rnn1 = rnn_cls(\n",
    "            input_size=input_size, hidden_size=hidden1, num_layers=1,\n",
    "            batch_first=True, dropout=0.0, bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        self.inter_rnn_drop = nn.Dropout(0.1)\n",
    "        \n",
    "        self.rnn2 = rnn_cls(\n",
    "            input_size=hidden1*(2 if bidirectional else 1), hidden_size=hidden2, num_layers=1,\n",
    "            batch_first=True, dropout=0.0, bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        feat_dim = hidden2*(2 if bidirectional else 1)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(feat_dim)\n",
    "        self.fc = nn.Linear(feat_dim, 32)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        if self.problem_type == 'multiclass':\n",
    "            head = self.ordinal_head.lower() if isinstance(self.ordinal_head, str) else 'coral'\n",
    "            if head == 'corn':\n",
    "                self.out = OrdinalHeadCORN(32, self.n_classes)\n",
    "            else:\n",
    "                self.out = OrdinalHeadCORAL(32, self.n_classes)\n",
    "        else:\n",
    "            self.out = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, F]\n",
    "        out, _ = self.rnn1(x)\n",
    "        out = self.inter_rnn_drop(out)   # inter-layer dropout (sequence-wise)\n",
    "        out, _ = self.rnn2(out)\n",
    "        # take last timestep: [B, T, H] -> [B, H]\n",
    "        out = out[:, -1, :]\n",
    "        out = self.bn(out)\n",
    "        out = F.relu(self.fc(out))\n",
    "        out = self.drop(out)\n",
    "        out = self.out(out)  # shape [B,1]\n",
    "        return out  # regression: raw; classification: logits\n",
    "\n",
    "def build_model(input_shape, model_type='LSTM', problem_type='regression', n_classes=6, ordinal_head='coral'):\n",
    "    seq_len, n_features = input_shape\n",
    "    model_type = model_type.upper()\n",
    "    if model_type == 'LSTM':\n",
    "        return RNNHead(n_features, rnn_type='LSTM', bidirectional=False, problem_type=problem_type, n_classes=n_classes, ordinal_head=ordinal_head)\n",
    "    elif model_type == 'BILSTM':\n",
    "        return RNNHead(n_features, rnn_type='LSTM', bidirectional=True, problem_type=problem_type, n_classes=n_classes, ordinal_head=ordinal_head)\n",
    "    elif model_type == 'GRU':\n",
    "        return RNNHead(n_features, rnn_type='GRU', bidirectional=False, problem_type=problem_type, n_classes=n_classes, ordinal_head=ordinal_head)\n",
    "    elif model_type == 'BIGRU':\n",
    "        return RNNHead(n_features, rnn_type='GRU', bidirectional=True, problem_type=problem_type, n_classes=n_classes, ordinal_head=ordinal_head)\n",
    "    else:\n",
    "        raise ValueError(\"Model type must be one of: ['LSTM','BiLSTM','GRU','BiGRU']\")\n",
    "\n",
    "# Early Stopping (PyTorch)\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=15, min_delta=0.0, restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best = restore_best\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        improved = (self.best_loss - val_loss) > self.min_delta\n",
    "        if improved:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best:\n",
    "                # Deep copy state dict\n",
    "                self.best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a473699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_labels_from_edges(edges, decimals=1):\n",
    "    labels = []\n",
    "    C = len(edges) - 1\n",
    "    for i in range(C):\n",
    "        lo, hi = edges[i], edges[i+1]\n",
    "        if i == 0:\n",
    "            labels.append(f\"≤ {hi*100:.{decimals}f}%\")\n",
    "        elif i == C - 1:\n",
    "            labels.append(f\"> {lo*100:.{decimals}f}%\")\n",
    "        else:\n",
    "            labels.append(f\"({lo*100:.{decimals}f}%,{hi*100:.{decimals}f}%]\")\n",
    "    return labels\n",
    "\n",
    "def pct_return(series, h):\n",
    "    return series.shift(-h) / series - 1.0\n",
    "\n",
    "def safe_quantile_edges(x, n_classes=6):\n",
    "    qs = np.linspace(0, 1, n_classes + 1)\n",
    "    edges = np.quantile(x, qs)\n",
    "    for i in range(1, len(edges)):\n",
    "        if edges[i] <= edges[i-1]:\n",
    "            edges[i] = np.nextafter(edges[i-1], np.inf)\n",
    "    return edges\n",
    "\n",
    "def bucketize_with_edges(x, edges):\n",
    "    inner = edges[1:-1]\n",
    "    return np.digitize(x, inner, right=True).astype(int)\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_logits(model, loader, device):\n",
    "    model.eval()\n",
    "    chunks = []\n",
    "    for xb, _ in loader:\n",
    "        xb = xb.to(device)\n",
    "        chunks.append(model(xb).detach().cpu())\n",
    "    return torch.cat(chunks, dim=0)\n",
    "\n",
    "def find_taus_per_threshold(Z_val, y_val_idx, grid=np.linspace(0, 1, 100)):\n",
    "    if isinstance(Z_val, torch.Tensor):\n",
    "        Z_val = Z_val.numpy()\n",
    "    P_val = 1.0 / (1.0 + np.exp(-Z_val))\n",
    "    P_rep = monotone_repair_numpy(P_val)\n",
    "    K_1 = P_rep.shape[1]\n",
    "    best_taus = np.full(K_1, 0.5, dtype=np.float32)\n",
    "    for k in range(K_1):\n",
    "        best_f1, best_tau = -1.0, 0.5\n",
    "        for tau in grid:\n",
    "            y_hat = decode_ordinal_with_taus(P_rep, taus_override={k: tau})\n",
    "            f1 = f1_score(y_val_idx, y_hat, average='macro', zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_tau = f1, tau\n",
    "        best_taus[k] = best_tau\n",
    "    return best_taus\n",
    "\n",
    "def monotone_repair_numpy(P):\n",
    "    P = np.asarray(P).copy()\n",
    "    for k in range(P.shape[1] - 2, -1, -1):\n",
    "        P[:, k] = np.maximum(P[:, k], P[:, k+1])\n",
    "    return P\n",
    "\n",
    "def decode_ordinal_with_taus(P_rep, taus=None, taus_override=None):\n",
    "    N, K_1 = P_rep.shape\n",
    "    if taus is None:\n",
    "        taus = np.full(K_1, 0.5, dtype=np.float32)\n",
    "    if taus_override:\n",
    "        taus = taus.copy()\n",
    "        for k, v in taus_override.items():\n",
    "            taus[k] = v\n",
    "    comp = (P_rep >= taus.reshape(1, -1)).astype(np.int32)\n",
    "    return comp.sum(axis=1).astype(np.int64)\n",
    "\n",
    "def ordinal_to_class_probs(P_rep):\n",
    "    N, K_1 = P_rep.shape\n",
    "    K = K_1 + 1\n",
    "    Pc = np.empty((N, K), dtype=np.float32)\n",
    "    Pc[:, 0] = 1.0 - P_rep[:, 0]\n",
    "    for c in range(1, K - 1):\n",
    "        Pc[:, c] = np.clip(P_rep[:, c-1] - P_rep[:, c], 0.0, 1.0)\n",
    "    Pc[:, K - 1] = P_rep[:, K_1 - 1]\n",
    "    s = Pc.sum(axis=1, keepdims=True)\n",
    "    return Pc / np.maximum(s, 1e-8)\n",
    "\n",
    "\n",
    "def best_threshold_from_val(y_true, y_scores, metric='f1', grid=None):\n",
    "    \"\"\"\n",
    "    Sweep probability thresholds on validation scores to maximize a metric.\n",
    "    metric can be 'f1', 'mcc', 'accuracy', or a callable(y_true,y_pred)->float.\n",
    "    Returns (best_threshold, best_metric_value).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_scores = np.asarray(y_scores).astype(float)\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 181)\n",
    "    metric_fn = None\n",
    "    if callable(metric):\n",
    "        metric_fn = metric\n",
    "    else:\n",
    "        name = str(metric).lower()\n",
    "        if name == 'f1':\n",
    "            metric_fn = lambda yt, yp: f1_score(yt, yp, zero_division=0)\n",
    "        elif name == 'mcc':\n",
    "            metric_fn = lambda yt, yp: matthews_corrcoef(yt, yp)\n",
    "        elif name in ('acc', 'accuracy'):\n",
    "            metric_fn = lambda yt, yp: (yt == yp).mean()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric '{metric}'\")\n",
    "    best_thr = 0.5\n",
    "    best_val = -np.inf\n",
    "    for thr in grid:\n",
    "        preds = (y_scores >= thr).astype(int)\n",
    "        val = metric_fn(y_true, preds)\n",
    "        if val > best_val + 1e-12 or (abs(val - best_val) <= 1e-12 and thr < best_thr):\n",
    "            best_val = float(val)\n",
    "            best_thr = float(thr)\n",
    "    return best_thr, best_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fGvgtiPTiBQ4"
   },
   "outputs": [],
   "source": [
    "class StockPredictionPipeline:\n",
    "    def __init__(self, df, feature_columns, model_type='LSTM', sequence_length=30, problem_type='regression', horizon_steps=1, n_classes=6, ordinal_head='coral', fixed_bucket_edges=None):\n",
    "        self.df = df.copy()\n",
    "        self.feature_columns = feature_columns\n",
    "        self.model_type = model_type\n",
    "        self.sequence_length = sequence_length\n",
    "        self.problem_type = problem_type\n",
    "        self.horizon_steps = horizon_steps\n",
    "        self.results = []\n",
    "        self.loss_curves = []\n",
    "        self.n_classes = n_classes\n",
    "        self.ordinal_head = ordinal_head\n",
    "        self.fixed_bucket_edges = None\n",
    "        if fixed_bucket_edges is not None:\n",
    "            edges = np.asarray(fixed_bucket_edges, dtype=float)\n",
    "            if edges.ndim != 1:\n",
    "                raise ValueError(\"fixed_bucket_edges must be a 1D sequence of monotonically increasing numbers\")\n",
    "            if edges.size < 2:\n",
    "                raise ValueError(\"fixed_bucket_edges must contain at least two values\")\n",
    "            if np.any(np.diff(edges) <= 0):\n",
    "                raise ValueError(\"fixed_bucket_edges must be strictly increasing\")\n",
    "            self.n_classes = int(edges.size - 1)\n",
    "            self.fixed_bucket_edges = edges\n",
    "\n",
    "        # Validate\n",
    "        self._validate_inputs()\n",
    "\n",
    "        # Device & precision\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mixed_precision = torch.cuda.is_available()\n",
    "\n",
    "        print(f\"Pipeline initialized for a '{self.problem_type}' problem \"\n",
    "              f\"with horizon {self.horizon_steps} steps. Device: {self.device}\")\n",
    "\n",
    "    def _validate_inputs(self):\n",
    "        missing_cols = [col for col in self.feature_columns if col not in self.df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_cols}\")\n",
    "\n",
    "        if 'close' not in self.df.columns and 'close_price' not in self.df.columns:\n",
    "            raise ValueError(\"No 'close' or 'close_price' column found in data\")\n",
    "\n",
    "        valid_models = ['LSTM', 'BiLSTM', 'GRU', 'BiGRU']\n",
    "        if self.model_type not in valid_models:\n",
    "            raise ValueError(f\"Model type must be one of: {valid_models}\")\n",
    "\n",
    "        if self.problem_type not in ['regression', 'classification', 'multiclass']:\n",
    "            raise ValueError(\"Problem type must be 'regression', 'classification', or 'multiclass'\")\n",
    "\n",
    "    def create_target_variable(self, company_data):\n",
    "        company_data = company_data.copy()\n",
    "        price_col = 'close' if 'close' in company_data.columns else 'close_price'\n",
    "        if 'date' in company_data.columns:\n",
    "            company_data = company_data.sort_values('date')\n",
    "            \n",
    "        h = self.horizon_steps\n",
    "\n",
    "        company_data['target_regression'] = (\n",
    "            np.log(company_data[price_col].shift(-h)) - np.log(company_data[price_col])\n",
    "        )\n",
    "        company_data['target_direction'] = (company_data['target_regression'] > 0).astype(int)\n",
    "        company_data['ret_h'] = pct_return(company_data[price_col], h)\n",
    "        if self.problem_type == 'multiclass':\n",
    "            company_data = company_data.dropna(subset=['ret_h'])\n",
    "        else:\n",
    "            company_data = company_data.dropna()\n",
    "        return company_data\n",
    "\n",
    "    def create_sequences(self, features, *targets):\n",
    "        X = []\n",
    "        y_sequences = [[] for _ in targets]\n",
    "        for i in range(self.sequence_length, len(features)):\n",
    "            X.append(features[i-self.sequence_length:i])\n",
    "            for j, target in enumerate(targets):\n",
    "                y_sequences[j].append(target[i])\n",
    "        return (np.array(X),) + tuple(np.array(y) for y in y_sequences)\n",
    "\n",
    "    def _train_one_epoch(self, model, loader, optimizer, loss_fn, scaler):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device).view(-1, 1)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            ctx = torch.amp.autocast('cuda') if self.mixed_precision else nullcontext()\n",
    "            with ctx:\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "\n",
    "            if self.mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        return total_loss / len(loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_one_epoch(self, model, loader, loss_fn):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device).view(-1, 1)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        return total_loss / len(loader.dataset)\n",
    "\n",
    "    def _train_one_epoch_multiclass(self, model, loader, optimizer, scaler, *, pos_weight=None):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            ctx = torch.amp.autocast('cuda') if self.mixed_precision else nullcontext()\n",
    "            with ctx:\n",
    "                logits = model(xb)\n",
    "                bces = []\n",
    "                for k in range(logits.shape[1]):\n",
    "                    w = None if pos_weight is None else pos_weight[k]\n",
    "                    bce_k = F.binary_cross_entropy_with_logits(logits[:, k], yb[:, k], pos_weight=w)\n",
    "                    bces.append(bce_k)\n",
    "                loss = torch.stack(bces).mean()\n",
    "            if self.mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            total += float(loss.item()) * xb.size(0)\n",
    "        return total / len(loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_one_epoch_multiclass(self, model, loader, pos_weight=None):\n",
    "        model.eval()\n",
    "        total = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device)\n",
    "            logits = model(xb)\n",
    "            bces = []\n",
    "            for k in range(logits.shape[1]):\n",
    "                w = None if pos_weight is None else pos_weight[k]\n",
    "                bce_k = F.binary_cross_entropy_with_logits(logits[:, k], yb[:, k], pos_weight=w)\n",
    "                bces.append(bce_k)\n",
    "            loss = torch.stack(bces).mean()\n",
    "            total += float(loss.item()) * xb.size(0)\n",
    "        return total / len(loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _predict(self, model, loader):\n",
    "        model.eval()\n",
    "        outs = []\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            logits = model(xb).squeeze(1).detach().cpu().numpy()\n",
    "            outs.append(logits)\n",
    "        return np.concatenate(outs, axis=0)\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        model = build_model(\n",
    "            input_shape,\n",
    "            model_type=self.model_type,\n",
    "            problem_type=self.problem_type,\n",
    "            n_classes=self.n_classes,\n",
    "            ordinal_head=self.ordinal_head\n",
    "        )\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def process_company(self, company_name, company_data, sector):\n",
    "        print(f\"\\nProcessing {company_name} ({sector})...\")\n",
    "        try:\n",
    "            company_data = self.create_target_variable(company_data)\n",
    "\n",
    "            # Min samples requirement (same heuristic)\n",
    "            min_samples = self.sequence_length + 150 + self.horizon_steps\n",
    "            if len(company_data) < min_samples:\n",
    "                print(f\"Insufficient data for {company_name} ({len(company_data)} < {min_samples}). Skipping...\")\n",
    "                return None\n",
    "\n",
    "            if company_data[self.feature_columns].isnull().any().any():\n",
    "                print(f\"Missing values in features for {company_name}. Skipping...\")\n",
    "                return None\n",
    "\n",
    "            features = company_data[self.feature_columns].values\n",
    "            target_reg = company_data['target_regression'].values\n",
    "            target_dir = company_data['target_direction'].values\n",
    "\n",
    "            # Create sequences\n",
    "            X_raw, y_reg, y_dir = self.create_sequences(features, target_reg, target_dir)\n",
    "\n",
    "            # TimeSeriesSplit\n",
    "            n_splits = min(5, len(X_raw) // 50)\n",
    "            if n_splits < 3:\n",
    "                print(f\"Insufficient data for proper time series validation for {company_name}. Skipping...\")\n",
    "                return None\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "            splits = list(tscv.split(X_raw))\n",
    "            train_idx, test_idx = splits[-1]\n",
    "\n",
    "            # Train/Val split (last 20% of train for val)\n",
    "            val_size = int(0.2 * len(train_idx))\n",
    "            final_train_idx = train_idx[:-val_size]\n",
    "            val_idx = train_idx[-val_size:]\n",
    "            \n",
    "            if self.horizon_steps > 1:\n",
    "                print(\"Adjusting for multi-step horizon...\")\n",
    "                gap = self.horizon_steps\n",
    "                if len(final_train_idx) > gap:\n",
    "                    final_train_idx = final_train_idx[:-gap]  # drop last h labels from train\n",
    "                if len(val_idx) > gap:\n",
    "                    val_idx = val_idx[gap:]  # drop last h labels from val\n",
    "\n",
    "            X_train_raw, X_val_raw, X_test_raw = X_raw[final_train_idx], X_raw[val_idx], X_raw[test_idx]\n",
    "            \n",
    "            F = X_raw.shape[-1]\n",
    "            feat_scaler = StandardScaler()\n",
    "            X_train = feat_scaler.fit_transform(X_train_raw.reshape(-1, F)).reshape(X_train_raw.shape)\n",
    "            X_val   = feat_scaler.transform(X_val_raw.reshape(-1, F)).reshape(X_val_raw.shape)\n",
    "            X_test  = feat_scaler.transform(X_test_raw.reshape(-1, F)).reshape(X_test_raw.shape)\n",
    "\n",
    "            if self.problem_type == 'multiclass':\n",
    "                if 'ret_h' not in company_data.columns:\n",
    "                    raise RuntimeError(\"Expected 'ret_h' for ordinal targets but it was missing.\")\n",
    "                ret_full = company_data['ret_h'].values\n",
    "                ret_seq_full = ret_full[self.sequence_length:]\n",
    "\n",
    "                ret_train = ret_seq_full[final_train_idx]\n",
    "                ret_val = ret_seq_full[val_idx]\n",
    "                ret_test = ret_seq_full[test_idx]\n",
    "\n",
    "                if self.fixed_bucket_edges is not None:\n",
    "                    edges = self.fixed_bucket_edges\n",
    "                    if int(edges.shape[0] - 1) != self.n_classes:\n",
    "                        raise ValueError(\"fixed_bucket_edges length must match n_classes+1\")\n",
    "                else:\n",
    "                    edges = safe_quantile_edges(ret_train, n_classes=self.n_classes)\n",
    "                edges = np.asarray(edges, dtype=float)\n",
    "                K = int(edges.shape[0] - 1)\n",
    "                label_names = edge_labels_from_edges(edges, decimals=1)\n",
    "\n",
    "                y_bucket = bucketize_with_edges(ret_seq_full, edges).astype(np.int64)\n",
    "                y_train = y_bucket[final_train_idx]\n",
    "                y_val = y_bucket[val_idx]\n",
    "                y_test = y_bucket[test_idx]\n",
    "\n",
    "                T_train = make_cumulative_targets(y_train.astype(np.int64), K)\n",
    "                T_val = make_cumulative_targets(y_val.astype(np.int64), K)\n",
    "                T_test = make_cumulative_targets(y_test.astype(np.int64), K)\n",
    "\n",
    "                train_ds = OrdinalSequenceDataset(X_train, T_train)\n",
    "                val_ds = OrdinalSequenceDataset(X_val, T_val)\n",
    "                test_ds = OrdinalSequenceDataset(X_test, T_test)\n",
    "\n",
    "                train_loader = DataLoader(train_ds, batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "                val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "                test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "\n",
    "                model = self.build_model((self.sequence_length, len(self.feature_columns)))\n",
    "\n",
    "                pos_rate = T_train.mean(axis=0)\n",
    "                pos_weight = (1.0 - pos_rate) / np.clip(pos_rate, 1e-6, 1.0)\n",
    "                pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float32).to(self.device)\n",
    "\n",
    "                optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-7, weight_decay=1e-5)\n",
    "                scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-7)\n",
    "                early_stopper = EarlyStopper(patience=15, min_delta=0.0, restore_best=True)\n",
    "                scaler = torch.amp.GradScaler('cuda', enabled=self.mixed_precision)\n",
    "\n",
    "                max_epochs = 100\n",
    "                best_val = float('inf')\n",
    "                epochs_trained = 0\n",
    "                company_loss_rows = []\n",
    "\n",
    "                for epoch in range(1, max_epochs + 1):\n",
    "                    train_loss = self._train_one_epoch_multiclass(model, train_loader, optimizer, scaler, pos_weight=pos_weight_tensor)\n",
    "                    val_loss = self._eval_one_epoch_multiclass(model, val_loader, pos_weight=pos_weight_tensor)\n",
    "                    scheduler.step(val_loss)\n",
    "                    stop = early_stopper.step(val_loss, model)\n",
    "                    epochs_trained = epoch\n",
    "\n",
    "                    row = {\n",
    "                        'company': company_name,\n",
    "                        'sector': sector,\n",
    "                        'model_type': self.model_type,\n",
    "                        'problem_type': self.problem_type,\n",
    "                        'sequence_length': self.sequence_length,\n",
    "                        'horizon_steps': self.horizon_steps,\n",
    "                        'epoch': epoch,\n",
    "                        'train_loss': float(train_loss),\n",
    "                        'val_loss': float(val_loss),\n",
    "                        'train_samples': len(X_train),\n",
    "                        'val_samples': len(X_val),\n",
    "                        'test_samples': len(X_test),\n",
    "                    }\n",
    "\n",
    "                    company_loss_rows.append(row)\n",
    "                    self.loss_curves.append(row)\n",
    "\n",
    "                    if epoch % 10 == 0 or stop:\n",
    "                        print(f\"  Epoch {epoch:03d} - train {train_loss:.5f} | val {val_loss:.5f}\")\n",
    "\n",
    "                    if stop:\n",
    "                        break\n",
    "\n",
    "                early_stopper.restore(model)\n",
    "\n",
    "                Z_val = collect_logits(model, val_loader, self.device)\n",
    "                taus = find_taus_per_threshold(Z_val, y_val.astype(np.int64))\n",
    "                P_cum_val = torch.sigmoid(Z_val).cpu().numpy()\n",
    "                P_rep_val = monotone_repair_numpy(P_cum_val)\n",
    "                P_class_val = ordinal_to_class_probs(P_rep_val)\n",
    "                mid_cut = (K // 2)\n",
    "                y_val_dir = (y_val >= mid_cut).astype(int)\n",
    "                prob_val_up = P_class_val[:, mid_cut:].sum(axis=1)\n",
    "                dir_thr, dir_thr_score = best_threshold_from_val(y_val_dir, prob_val_up, metric='f1')\n",
    "\n",
    "                Z_test = collect_logits(model, test_loader, self.device)\n",
    "                P_cum = torch.sigmoid(Z_test).cpu().numpy()\n",
    "                P_rep = monotone_repair_numpy(P_cum)\n",
    "                P_class = ordinal_to_class_probs(P_rep)\n",
    "\n",
    "                y_pred_labels = decode_ordinal_with_taus(P_rep, taus=taus)\n",
    "                y_true_labels = y_test\n",
    "\n",
    "                labels = list(range(K))\n",
    "                cm_counts = confusion_matrix(y_true_labels, y_pred_labels, labels=labels)\n",
    "                cm_norm = confusion_matrix(y_true_labels, y_pred_labels, labels=labels, normalize='true')\n",
    "\n",
    "                micro_acc = (y_true_labels == y_pred_labels).mean()\n",
    "                macro_f1 = f1_score(y_true_labels, y_pred_labels, average='macro', zero_division=0)\n",
    "\n",
    "                ret_seq_train = ret_seq_full[final_train_idx].astype(np.float32)\n",
    "                mu_c = np.array([\n",
    "                    ret_seq_train[y_train == c].mean() if np.any(y_train == c) else 0.0\n",
    "                    for c in range(K)\n",
    "                ], dtype=np.float32)\n",
    "\n",
    "                expected_ret = (P_class * mu_c[None, :]).sum(axis=1)\n",
    "                expected_ret_mean = float(expected_ret.mean())\n",
    "\n",
    "                prob_test_up = P_class[:, mid_cut:].sum(axis=1)\n",
    "                y_true_dir = (y_true_labels >= mid_cut).astype(int)\n",
    "                y_pred_dir = (prob_test_up >= dir_thr).astype(int)\n",
    "                precision = precision_score(y_true_dir, y_pred_dir, zero_division=0)\n",
    "                recall = recall_score(y_true_dir, y_pred_dir, zero_division=0)\n",
    "                f1 = f1_score(y_true_dir, y_pred_dir, zero_division=0)\n",
    "                mcc = matthews_corrcoef(y_true_dir, y_pred_dir)\n",
    "                directional_accuracy = (y_true_dir == y_pred_dir).mean()\n",
    "\n",
    "                result = {\n",
    "                    'company': company_name,\n",
    "                    'sector': sector,\n",
    "                    'model_type': self.model_type,\n",
    "                    'problem_type': self.problem_type,\n",
    "                    'horizon_steps': self.horizon_steps,\n",
    "                    'macro_f1': macro_f1,\n",
    "                    'micro_accuracy': micro_acc,\n",
    "                    'expected_return_mean': expected_ret_mean,\n",
    "                    'mse': np.nan,\n",
    "                    'mae': np.nan,\n",
    "                    'r2': np.nan,\n",
    "                    'mcc': mcc,\n",
    "                    'f1': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'directional_accuracy': directional_accuracy,\n",
    "                    'n_samples': int(X_raw.shape[0]),\n",
    "                    'train_samples': int(X_train.shape[0]),\n",
    "                    'val_samples': int(X_val.shape[0]),\n",
    "                    'test_samples': int(X_test.shape[0]),\n",
    "                    'epochs_trained': epochs_trained\n",
    "                }\n",
    "                result['confusion_matrix'] = cm_counts.tolist()\n",
    "                result['confusion_matrix_normalized'] = cm_norm.tolist()\n",
    "                result['bucket_edges'] = edges.tolist()\n",
    "                result['bucket_labels'] = label_names\n",
    "                result['taus'] = taus.astype(float).tolist()\n",
    "                result['direction_threshold'] = dir_thr\n",
    "                result['direction_threshold_metric'] = dir_thr_score\n",
    "\n",
    "                print(f\"  Multiclass -> Micro Acc: {micro_acc:.4f}, Macro F1: {macro_f1:.4f}, Expected Return: {expected_ret_mean:.6f}\")\n",
    "                print(f\"  Directional threshold -> τ={dir_thr:.3f} (val F1={dir_thr_score:.4f})\")\n",
    "\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "                return result\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                y_train, y_val, y_test = y_reg[final_train_idx], y_reg[val_idx], y_reg[test_idx]\n",
    "                target_scaler = StandardScaler()\n",
    "                y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "                y_val_scaled   = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "                train_target, val_target = y_train_scaled, y_val_scaled\n",
    "            else:\n",
    "                y_train, y_val, y_test = y_dir[final_train_idx], y_dir[val_idx], y_dir[test_idx]\n",
    "                train_target, val_target = y_train, y_val\n",
    "                target_scaler = None\n",
    "\n",
    "            # Class balance note\n",
    "            if self.problem_type == 'classification':\n",
    "                class_ratio = np.mean(y_train)\n",
    "                if class_ratio < 0.1 or class_ratio > 0.9:\n",
    "                    print(f\"Severe class imbalance for {company_name} ({class_ratio:.3f}). Consider using class weights.\")\n",
    "\n",
    "            # Datasets & loaders\n",
    "            train_ds = SequenceDataset(X_train, train_target)\n",
    "            val_ds   = SequenceDataset(X_val,   val_target)\n",
    "            test_ds  = SequenceDataset(X_test,  y_test)\n",
    "\n",
    "            train_loader = DataLoader(train_ds, batch_size=32, shuffle=False,  drop_last=False, num_workers=0)\n",
    "            val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "            test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "\n",
    "            # Build model\n",
    "            model = self.build_model((self.sequence_length, len(self.feature_columns)))\n",
    "\n",
    "            # Loss functions\n",
    "            if self.problem_type == 'regression':\n",
    "                loss_fn = nn.HuberLoss(delta=1.0)\n",
    "            else:\n",
    "                # Use BCEWithLogitsLoss for numerical stability (logits input)\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            # Optimizer & scheduler\n",
    "            optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-7, weight_decay=1e-5)\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-7)\n",
    "            early_stopper = EarlyStopper(patience=15, min_delta=0.0, restore_best=True)\n",
    "            scaler = torch.amp.GradScaler('cuda', enabled=self.mixed_precision)\n",
    "\n",
    "            # Training loop\n",
    "            max_epochs = 100\n",
    "            best_val = float('inf')\n",
    "            epochs_trained = 0\n",
    "            company_loss_rows = []  \n",
    "\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                train_loss = self._train_one_epoch(model, train_loader, optimizer, loss_fn, scaler)\n",
    "                val_loss = self._eval_one_epoch(model, val_loader, loss_fn)\n",
    "                scheduler.step(val_loss)\n",
    "                stop = early_stopper.step(val_loss, model)\n",
    "                epochs_trained = epoch\n",
    "                \n",
    "                row = {\n",
    "                    'company': company_name,\n",
    "                    'sector': sector,\n",
    "                    'model_type': self.model_type,\n",
    "                    'problem_type': self.problem_type,\n",
    "                    'sequence_length': self.sequence_length,\n",
    "                    'horizon_steps': self.horizon_steps,\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': float(train_loss),\n",
    "                    'val_loss': float(val_loss),\n",
    "                    'train_samples': len(X_train),\n",
    "                    'val_samples': len(X_val),\n",
    "                    'test_samples': len(X_test),\n",
    "                }\n",
    "                \n",
    "                company_loss_rows.append(row)\n",
    "                self.loss_curves.append(row)\n",
    "\n",
    "                if epoch % 10 == 0 or stop:\n",
    "                    print(f\"  Epoch {epoch:03d} - train {train_loss:.5f} | val {val_loss:.5f}\")\n",
    "\n",
    "                if stop:\n",
    "                    break\n",
    "\n",
    "            # Restore best model weights (like Keras restore_best_weights=True)\n",
    "            early_stopper.restore(model)\n",
    "\n",
    "            # Predictions\n",
    "            y_pred_raw = self._predict(model, test_loader)  # raw/regression or logits\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                y_pred_unscaled = target_scaler.inverse_transform(y_pred_raw.reshape(-1,1)).flatten() if target_scaler is not None else y_pred_raw\n",
    "                mse = mean_squared_error(y_test, y_pred_unscaled)\n",
    "                mae = mean_absolute_error(y_test, y_pred_unscaled)\n",
    "                r2  = r2_score(y_test, y_pred_unscaled)\n",
    "\n",
    "                # Directional metrics (derived)\n",
    "                y_test_dir = (y_reg[test_idx] > 0).astype(int)\n",
    "                y_pred_dir = (y_pred_unscaled > 0).astype(int)\n",
    "            else:\n",
    "                # logits -> probs via sigmoid -> learn best threshold on VAL\n",
    "                val_logits = self._predict(model, val_loader)\n",
    "                val_probs = 1.0 / (1.0 + np.exp(-val_logits))\n",
    "                best_thr, best_thr_score = best_threshold_from_val(y_val, val_probs, metric='f1')\n",
    "                probs = 1.0 / (1.0 + np.exp(-y_pred_raw))\n",
    "                y_pred_dir = (probs >= best_thr).astype(int)\n",
    "                y_test_dir = y_test\n",
    "                mse = mae = r2 = np.nan\n",
    "\n",
    "            precision = precision_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "            recall    = recall_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "            f1        = f1_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "            mcc       = matthews_corrcoef(y_test_dir, y_pred_dir)\n",
    "            directional_accuracy = np.mean(y_test_dir == y_pred_dir)\n",
    "\n",
    "            result = {\n",
    "                'company': company_name,\n",
    "                'sector': sector,\n",
    "                'model_type': self.model_type,\n",
    "                'problem_type': self.problem_type,\n",
    "                'horizon_steps': self.horizon_steps,\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'r2': r2,\n",
    "                'mcc': mcc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'directional_accuracy': directional_accuracy,\n",
    "                'n_samples': int(X_raw.shape[0]),\n",
    "                'train_samples': int(X_train.shape[0]),\n",
    "                'val_samples': int(X_val.shape[0]),\n",
    "                'test_samples': int(X_test.shape[0]),\n",
    "                'epochs_trained': epochs_trained\n",
    "            }\n",
    "            if self.problem_type == 'classification':\n",
    "                result['best_threshold'] = best_thr\n",
    "                result['best_threshold_metric'] = best_thr_score\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                print(f\"  Regression -> MSE: {mse:.6f}, MAE: {mae:.6f}, R²: {r2:.4f}\")\n",
    "            elif self.problem_type == 'classification':\n",
    "                print(f\"  Classification -> best τ={best_thr:.3f} (val F1={best_thr_score:.4f})\")\n",
    "            print(f\"  Directional -> Accuracy: {directional_accuracy:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "            # Explicit cleanup (PyTorch handles this, but keeps parity with Enrique2025)\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {company_name}: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return None\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        company_col = None\n",
    "        for col_name in ['ticker', 'company', 'symbol']:\n",
    "            if col_name in self.df.columns:\n",
    "                company_col = col_name\n",
    "                break\n",
    "        if company_col is None:\n",
    "            company_col = self.df.columns[0]\n",
    "            print(f\"Warning: Using '{company_col}' as company identifier column\")\n",
    "\n",
    "        companies = self.df[company_col].unique()\n",
    "        print(f\"Processing {len(companies)} companies with {self.model_type} model...\")\n",
    "        print(f\"Problem type: {self.problem_type}\")\n",
    "        print(f\"Sequence length: {self.sequence_length}\")\n",
    "        print(f\"Features: {self.feature_columns}\")\n",
    "\n",
    "        successful_companies = 0\n",
    "        for i, company in enumerate(companies, 1):\n",
    "            print(f\"\\n[{i}/{len(companies)}] Processing {company}...\")\n",
    "            company_data = self.df[self.df[company_col] == company].copy()\n",
    "            sector = company_data['sector'].iloc[0] if 'sector' in company_data.columns else 'Unknown'\n",
    "            result = self.process_company(company, company_data, sector)\n",
    "            if result:\n",
    "                self.results.append(result)\n",
    "                successful_companies += 1\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Pipeline completed: {successful_companies}/{len(companies)} companies processed successfully\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        if self.results:\n",
    "            self.results_df = pd.DataFrame(self.results)\n",
    "            return self.results_df\n",
    "        else:\n",
    "            print(\"No companies were processed successfully!\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "\n",
    "    def analyze_results(self):\n",
    "        if not hasattr(self, 'results_df') or self.results_df.empty:\n",
    "            print(\"No results to analyze!\")\n",
    "            return None\n",
    "\n",
    "        df = self.results_df\n",
    "        analysis = {}\n",
    "\n",
    "        print(\"\" + \"=\"*80)\n",
    "        print(\"STOCK PREDICTION PIPELINE RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: {self.model_type} | Problem: {self.problem_type}\")\n",
    "        print(f\"Companies analyzed: {len(df)}\")\n",
    "        print(f\"Average samples per company: {df['n_samples'].mean():.0f}\")\n",
    "\n",
    "        print(\"\" + \"=\"*50)\n",
    "        print(\"OVERALL PERFORMANCE\")\n",
    "        print(\"=\"*50)\n",
    "        if self.problem_type == 'regression':\n",
    "            print(f\"Mean Squared Error:     {df['mse'].mean():.6f} (±{df['mse'].std():.6f})\")\n",
    "            print(f\"Mean Absolute Error:    {df['mae'].mean():.6f} (±{df['mae'].std():.6f})\")\n",
    "            print(f\"R² Score:              {df['r2'].mean():.4f} (±{df['r2'].std():.4f})\")\n",
    "        if self.problem_type == 'multiclass' and 'micro_accuracy' in df.columns:\n",
    "            print(f\"Micro Accuracy:         {df['micro_accuracy'].mean():.4f} (±{df['micro_accuracy'].std():.4f})\")\n",
    "            print(f\"Macro F1 Score:         {df['macro_f1'].mean():.4f} (±{df['macro_f1'].std():.4f})\")\n",
    "            if 'expected_return_mean' in df.columns:\n",
    "                print(f\"Expected Return:        {df['expected_return_mean'].mean():.6f} (±{df['expected_return_mean'].std():.4f})\")\n",
    "\n",
    "        print(f\"Directional Accuracy:   {df['directional_accuracy'].mean():.4f} (±{df['directional_accuracy'].std():.4f})\")\n",
    "        print(f\"Matthews Correlation:   {df['mcc'].mean():.4f} (±{df['mcc'].std():.4f})\")\n",
    "        print(f\"F1 Score:              {df['f1'].mean():.4f} (±{df['f1'].std():.4f})\")\n",
    "        print(f\"Precision:             {df['precision'].mean():.4f} (±{df['precision'].std():.4f})\")\n",
    "        print(f\"Recall:                {df['recall'].mean():.4f} (±{df['recall'].std():.4f})\")\n",
    "\n",
    "        if self.problem_type == 'multiclass' and 'expected_return_mean' in df.columns:\n",
    "            print(\"\" + \"=\"*50)\n",
    "            print(\"TOP 10 BY EXPECTED RETURN (mean)\")\n",
    "            print(\"=\"*50)\n",
    "            top_er = df.nlargest(10, 'expected_return_mean')\n",
    "            for _, row in top_er.iterrows():\n",
    "                print(f\"{row['company']:<20} | {row['sector']:<15} | E[r]_mean: {row['expected_return_mean']:.4e} | Macro-F1: {row['macro_f1']:.3f}\")\n",
    "\n",
    "        if 'sector' in df.columns and df['sector'].nunique() > 1:\n",
    "            print(\"\" + \"=\"*50)\n",
    "            print(\"PERFORMANCE BY SECTOR\")\n",
    "            print(\"=\"*50)\n",
    "            sector_stats = df.groupby('sector').agg({\n",
    "                'directional_accuracy': ['mean', 'std', 'count'],\n",
    "                'mcc': ['mean', 'std'],\n",
    "                'r2': 'mean' if self.problem_type == 'regression' else lambda x: np.nan,\n",
    "                'mae': 'mean' if self.problem_type == 'regression' else lambda x: np.nan\n",
    "            }).round(4)\n",
    "            sector_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in sector_stats.columns]\n",
    "            sector_stats = sector_stats.sort_values('directional_accuracy_mean', ascending=False)\n",
    "            for sector, row in sector_stats.iterrows():\n",
    "                print(f\"{sector:<20} | Acc: {row['directional_accuracy_mean']:.3f}±{row['directional_accuracy_std']:.3f} | \"\n",
    "                      f\"MCC: {row['mcc_mean']:.3f} | Companies: {int(row['directional_accuracy_count'])}\")\n",
    "\n",
    "        print(\"\" + \"=\"*50)\n",
    "        print(\"TOP 10 PERFORMERS (by Directional Accuracy)\")\n",
    "        print(\"=\"*50)\n",
    "        top_performers = df.nlargest(10, 'directional_accuracy')\n",
    "        for _, row in top_performers.iterrows():\n",
    "            print(f\"{row['company']:<20} | {row['sector']:<15} | \"\n",
    "                  f\"Acc: {row['directional_accuracy']:.3f} | MCC: {row['mcc']:.3f}\")\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def save_results(self, results, output_dir='results/benchmarking'):\n",
    "        if results is not None and not results.empty:\n",
    "            model_name = self.model_type\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                out_dir = os.path.join(output_dir, 'regression')\n",
    "            elif self.problem_type == 'classification':\n",
    "                out_dir = os.path.join(output_dir, 'classification')\n",
    "            else:\n",
    "                out_dir = os.path.join(output_dir, 'multiclass')\n",
    "\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            output_path = os.path.join(out_dir, f\"{model_name}.csv\")\n",
    "\n",
    "            results.to_csv(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        else:\n",
    "            print(\"No results to save.\")\n",
    "            \n",
    "    def get_loss_curves_df(self):\n",
    "        if not self.loss_curves:\n",
    "            print(\"No loss curves logged yet.\")\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(self.loss_curves)\n",
    "\n",
    "    def save_loss_curves(self, out_path='results/benchmarking/'):\n",
    "        df = self.get_loss_curves_df()\n",
    "        if df.empty:\n",
    "            print(\"No loss curves to save.\")\n",
    "            return\n",
    "        if self.problem_type == 'regression':\n",
    "            out_path = os.path.join(out_path, 'regression', f\"{self.model_type}_loss_curves.csv\")\n",
    "        elif self.problem_type == 'classification':\n",
    "            out_path = os.path.join(out_path, 'classification', f\"{self.model_type}_loss_curves.csv\")\n",
    "        else:\n",
    "            out_path = os.path.join(out_path, 'multiclass', f\"{self.model_type}_loss_curves.csv\")\n",
    "            \n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        \n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\"Loss curves saved to {out_path}\")\n",
    "\n",
    "    def get_feature_importance_analysis(self):\n",
    "        print(\"Feature importance analysis not implemented yet.\")\n",
    "        print(\"Consider implementing SHAP values or permutation importance for better insights.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlCKxsDxHHFe",
    "outputId": "36aa45f2-ea60-4af0-f4ea-d3804481245f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/mdk91y8925ncdd32prjkcl_c0000gn/T/ipykernel_78442/1242874406.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  master_df[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of master_df before dropping NaNs: (108592, 21)\n",
      "Shape of master_df after dropping NaNs: (108592, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/mdk91y8925ncdd32prjkcl_c0000gn/T/ipykernel_78442/1242874406.py:95: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  master_df = master_df.groupby('ticker').apply(apply_ta_indicators)\n"
     ]
    }
   ],
   "source": [
    "companies = pd.read_parquet('stocknet-dataset/stock_table.parquet')\n",
    "tweets = pd.read_parquet('stocknet-dataset/stock_tweets_withsentiment_withemotion_withstance_nomerge.parquet')\n",
    "stocks = pd.read_parquet('stocknet-dataset/stock_prices.parquet')\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "companies.columns = [x.lower() for x in companies.columns]\n",
    "tweets.columns = [x.lower() for x in tweets.columns]\n",
    "stocks.columns = [x.lower() for x in stocks.columns]\n",
    "\n",
    "tweets['stance_positive'] = (tweets['stance_label'] == 'Positive').astype(int)\n",
    "tweets['stance_negative'] = (tweets['stance_label'] == 'Negative').astype(int)\n",
    "\n",
    "tweets_merged = tweets.groupby(['date', 'ticker'], as_index=False).agg({\n",
    "    'text': lambda x: ' '.join(x),\n",
    "    'sentiment': lambda x: x.mean(),\n",
    "    'emotion_anger': 'sum',\n",
    "    'emotion_disgust': 'sum',\n",
    "    'emotion_fear': 'sum',\n",
    "    'emotion_joy': 'sum',\n",
    "    'emotion_neutral': 'sum',\n",
    "    'emotion_sadness': 'sum',\n",
    "    'emotion_surprize': 'sum',\n",
    "    'stance_positive': 'sum',\n",
    "    'stance_negative': 'sum'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweets_merged['date'] = pd.to_datetime(tweets_merged['date'])\n",
    "stocks['date'] = pd.to_datetime(stocks['date'])\n",
    "\n",
    "\n",
    "\n",
    "master_df = pd.merge(\n",
    "    stocks,\n",
    "    tweets_merged,\n",
    "    on=[\"date\", \"ticker\"],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing tweet features with 0\n",
    "tweet_feature_cols = ['sentiment', 'emotion_anger', 'emotion_disgust', 'emotion_fear', 'emotion_joy', 'emotion_neutral', 'emotion_sadness', 'emotion_surprize', 'stance_positive', 'stance_negative']\n",
    "for col in tweet_feature_cols:\n",
    "    if col in master_df.columns:\n",
    "        master_df[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "master_df = pd.merge(master_df, companies[['ticker', 'sector', 'company']], on='ticker', how='left')\n",
    "\n",
    "\n",
    "feature_cols = ['open','high','low','volume']\n",
    "\n",
    "master_df = master_df.rename(columns={'close': 'close_price', 'company': 'company_name'})\n",
    "\n",
    "\n",
    "print(f\"Shape of master_df before dropping NaNs: {master_df.shape}\")\n",
    "print(f\"Shape of master_df after dropping NaNs: {master_df.shape}\")\n",
    "\n",
    "master_df.rename(columns={'close_price': 'close'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "master_df.sort_values(by=['ticker', 'date'], inplace=True)\n",
    "\n",
    "\n",
    "def apply_ta_indicators(df_group):\n",
    "    df_group.set_index(pd.DatetimeIndex(df_group['date']), inplace=True)\n",
    "    df_group.ta.ema(length=12, append=True)\n",
    "    df_group.ta.ema(length=26, append=True)\n",
    "    df_group.ta.ema(length=50, append=True)\n",
    "\n",
    "    df_group.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "\n",
    "\n",
    "    df_group.ta.rsi(length=14, append=True)\n",
    "    df_group.ta.stochrsi(length=14, append=True)\n",
    "\n",
    "    df_group.ta.atr(length=14, append=True)\n",
    "\n",
    "    bb = ta.bbands(df_group['close'], length=20, std=2)\n",
    "    df_group['BB_upper'] = bb['BBU_20_2.0']\n",
    "    df_group['BB_middle'] = bb['BBM_20_2.0']\n",
    "    df_group['BB_lower'] = bb['BBL_20_2.0']\n",
    "\n",
    "    df_group.ta.obv(append=True)\n",
    "    return df_group.reset_index(drop=True)\n",
    "\n",
    "master_df = master_df.groupby('ticker').apply(apply_ta_indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "12_qUQQGGxKm",
    "outputId": "41eb4670-181a-465b-f565-290ccae2d22b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>MACDh_12_26_9</th>\n",
       "      <th>MACDs_12_26_9</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>STOCHRSIk_14_14_3_3</th>\n",
       "      <th>STOCHRSId_14_14_3_3</th>\n",
       "      <th>ATRr_14</th>\n",
       "      <th>BB_upper</th>\n",
       "      <th>BB_middle</th>\n",
       "      <th>BB_lower</th>\n",
       "      <th>OBV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-11-14</td>\n",
       "      <td>77.928574</td>\n",
       "      <td>78.207146</td>\n",
       "      <td>76.597145</td>\n",
       "      <td>76.697144</td>\n",
       "      <td>69.613815</td>\n",
       "      <td>119292600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538077</td>\n",
       "      <td>-3.703588</td>\n",
       "      <td>25.771012</td>\n",
       "      <td>21.054629</td>\n",
       "      <td>19.582354</td>\n",
       "      <td>2.377852</td>\n",
       "      <td>94.648550</td>\n",
       "      <td>84.401357</td>\n",
       "      <td>74.154164</td>\n",
       "      <td>-1.014356e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-11-15</td>\n",
       "      <td>76.790001</td>\n",
       "      <td>77.071426</td>\n",
       "      <td>74.660004</td>\n",
       "      <td>75.088570</td>\n",
       "      <td>68.153778</td>\n",
       "      <td>197477700.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537725</td>\n",
       "      <td>-3.838019</td>\n",
       "      <td>23.573491</td>\n",
       "      <td>15.792949</td>\n",
       "      <td>19.993462</td>\n",
       "      <td>2.380310</td>\n",
       "      <td>93.761634</td>\n",
       "      <td>83.514428</td>\n",
       "      <td>73.267223</td>\n",
       "      <td>-1.211834e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>75.028572</td>\n",
       "      <td>75.714287</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>75.382858</td>\n",
       "      <td>68.420891</td>\n",
       "      <td>316723400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455544</td>\n",
       "      <td>-3.951905</td>\n",
       "      <td>24.836267</td>\n",
       "      <td>12.243346</td>\n",
       "      <td>16.363641</td>\n",
       "      <td>2.459547</td>\n",
       "      <td>92.716200</td>\n",
       "      <td>82.679214</td>\n",
       "      <td>72.642228</td>\n",
       "      <td>-8.951103e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-19</td>\n",
       "      <td>77.244286</td>\n",
       "      <td>81.071426</td>\n",
       "      <td>77.125717</td>\n",
       "      <td>80.818573</td>\n",
       "      <td>73.354591</td>\n",
       "      <td>205829400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>-3.951213</td>\n",
       "      <td>43.429047</td>\n",
       "      <td>39.692073</td>\n",
       "      <td>22.576123</td>\n",
       "      <td>2.695187</td>\n",
       "      <td>91.617665</td>\n",
       "      <td>82.201285</td>\n",
       "      <td>72.784906</td>\n",
       "      <td>-6.892809e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-11-20</td>\n",
       "      <td>81.701431</td>\n",
       "      <td>81.707146</td>\n",
       "      <td>79.225716</td>\n",
       "      <td>80.129997</td>\n",
       "      <td>72.729614</td>\n",
       "      <td>160688500.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281964</td>\n",
       "      <td>-3.880722</td>\n",
       "      <td>42.011353</td>\n",
       "      <td>69.373201</td>\n",
       "      <td>40.436207</td>\n",
       "      <td>2.679612</td>\n",
       "      <td>91.027780</td>\n",
       "      <td>81.851785</td>\n",
       "      <td>72.675790</td>\n",
       "      <td>-8.499694e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104215</th>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>76.940002</td>\n",
       "      <td>76.260002</td>\n",
       "      <td>76.470001</td>\n",
       "      <td>76.470001</td>\n",
       "      <td>8229700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107548</td>\n",
       "      <td>-0.972858</td>\n",
       "      <td>31.975492</td>\n",
       "      <td>35.117121</td>\n",
       "      <td>31.775404</td>\n",
       "      <td>0.786087</td>\n",
       "      <td>81.525829</td>\n",
       "      <td>78.243500</td>\n",
       "      <td>74.961171</td>\n",
       "      <td>-2.688251e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104216</th>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>76.209999</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.080002</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>7060400.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069077</td>\n",
       "      <td>-0.990127</td>\n",
       "      <td>31.851847</td>\n",
       "      <td>48.597552</td>\n",
       "      <td>38.712818</td>\n",
       "      <td>0.759224</td>\n",
       "      <td>81.303475</td>\n",
       "      <td>78.057500</td>\n",
       "      <td>74.811525</td>\n",
       "      <td>-2.758855e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104217</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>76.239998</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>76.059998</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>8218000.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054652</td>\n",
       "      <td>-1.003790</td>\n",
       "      <td>29.688704</td>\n",
       "      <td>55.025431</td>\n",
       "      <td>46.246701</td>\n",
       "      <td>0.732850</td>\n",
       "      <td>80.964170</td>\n",
       "      <td>77.832500</td>\n",
       "      <td>74.700830</td>\n",
       "      <td>-2.841035e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104218</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>76.269997</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.050003</td>\n",
       "      <td>76.330002</td>\n",
       "      <td>76.330002</td>\n",
       "      <td>15641700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018917</td>\n",
       "      <td>-1.008519</td>\n",
       "      <td>32.913052</td>\n",
       "      <td>73.940933</td>\n",
       "      <td>59.187972</td>\n",
       "      <td>0.711932</td>\n",
       "      <td>80.569554</td>\n",
       "      <td>77.624500</td>\n",
       "      <td>74.679446</td>\n",
       "      <td>-2.684618e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104219</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>76.849998</td>\n",
       "      <td>76.320000</td>\n",
       "      <td>76.570000</td>\n",
       "      <td>76.570000</td>\n",
       "      <td>7340800.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028984</td>\n",
       "      <td>-1.001273</td>\n",
       "      <td>36.200731</td>\n",
       "      <td>85.986580</td>\n",
       "      <td>71.650981</td>\n",
       "      <td>0.698937</td>\n",
       "      <td>80.167620</td>\n",
       "      <td>77.442500</td>\n",
       "      <td>74.717380</td>\n",
       "      <td>-2.611210e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104220 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date       open       high        low      close  adj close  \\\n",
       "0      2012-11-14  77.928574  78.207146  76.597145  76.697144  69.613815   \n",
       "1      2012-11-15  76.790001  77.071426  74.660004  75.088570  68.153778   \n",
       "2      2012-11-16  75.028572  75.714287  72.250000  75.382858  68.420891   \n",
       "3      2012-11-19  77.244286  81.071426  77.125717  80.818573  73.354591   \n",
       "4      2012-11-20  81.701431  81.707146  79.225716  80.129997  72.729614   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "104215 2017-08-28  76.900002  76.940002  76.260002  76.470001  76.470001   \n",
       "104216 2017-08-29  76.209999  76.489998  76.080002  76.449997  76.449997   \n",
       "104217 2017-08-30  76.239998  76.449997  76.059998  76.099998  76.099998   \n",
       "104218 2017-08-31  76.269997  76.489998  76.050003  76.330002  76.330002   \n",
       "104219 2017-09-01  76.370003  76.849998  76.320000  76.570000  76.570000   \n",
       "\n",
       "             volume ticker text  sentiment  ...  MACDh_12_26_9  MACDs_12_26_9  \\\n",
       "0       119292600.0   AAPL  NaN        0.0  ...      -0.538077      -3.703588   \n",
       "1       197477700.0   AAPL  NaN        0.0  ...      -0.537725      -3.838019   \n",
       "2       316723400.0   AAPL  NaN        0.0  ...      -0.455544      -3.951905   \n",
       "3       205829400.0   AAPL  NaN        0.0  ...       0.002769      -3.951213   \n",
       "4       160688500.0   AAPL  NaN        0.0  ...       0.281964      -3.880722   \n",
       "...             ...    ...  ...        ...  ...            ...            ...   \n",
       "104215    8229700.0    XOM  NaN        0.0  ...      -0.107548      -0.972858   \n",
       "104216    7060400.0    XOM  NaN        0.0  ...      -0.069077      -0.990127   \n",
       "104217    8218000.0    XOM  NaN        0.0  ...      -0.054652      -1.003790   \n",
       "104218   15641700.0    XOM  NaN        0.0  ...      -0.018917      -1.008519   \n",
       "104219    7340800.0    XOM  NaN        0.0  ...       0.028984      -1.001273   \n",
       "\n",
       "           RSI_14  STOCHRSIk_14_14_3_3  STOCHRSId_14_14_3_3   ATRr_14  \\\n",
       "0       25.771012            21.054629            19.582354  2.377852   \n",
       "1       23.573491            15.792949            19.993462  2.380310   \n",
       "2       24.836267            12.243346            16.363641  2.459547   \n",
       "3       43.429047            39.692073            22.576123  2.695187   \n",
       "4       42.011353            69.373201            40.436207  2.679612   \n",
       "...           ...                  ...                  ...       ...   \n",
       "104215  31.975492            35.117121            31.775404  0.786087   \n",
       "104216  31.851847            48.597552            38.712818  0.759224   \n",
       "104217  29.688704            55.025431            46.246701  0.732850   \n",
       "104218  32.913052            73.940933            59.187972  0.711932   \n",
       "104219  36.200731            85.986580            71.650981  0.698937   \n",
       "\n",
       "         BB_upper  BB_middle   BB_lower           OBV  \n",
       "0       94.648550  84.401357  74.154164 -1.014356e+09  \n",
       "1       93.761634  83.514428  73.267223 -1.211834e+09  \n",
       "2       92.716200  82.679214  72.642228 -8.951103e+08  \n",
       "3       91.617665  82.201285  72.784906 -6.892809e+08  \n",
       "4       91.027780  81.851785  72.675790 -8.499694e+08  \n",
       "...           ...        ...        ...           ...  \n",
       "104215  81.525829  78.243500  74.961171 -2.688251e+08  \n",
       "104216  81.303475  78.057500  74.811525 -2.758855e+08  \n",
       "104217  80.964170  77.832500  74.700830 -2.841035e+08  \n",
       "104218  80.569554  77.624500  74.679446 -2.684618e+08  \n",
       "104219  80.167620  77.442500  74.717380 -2.611210e+08  \n",
       "\n",
       "[104220 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_check = ['EMA_12', 'EMA_26','EMA_50','MACD_12_26_9','MACDh_12_26_9','MACDs_12_26_9','RSI_14','ATRr_14','STOCHRSIk_14_14_3_3','STOCHRSId_14_14_3_3','ATRr_14','BB_upper','BB_middle','BB_lower','OBV']\n",
    "master_df = master_df.dropna(subset=columns_to_check)\n",
    "\n",
    "\n",
    "master_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4D6EfvoYDrGY",
    "outputId": "a09ae550-5787-45c6-81aa-4bf74311f09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'open', 'high', 'low', 'close', 'adj close', 'volume', 'ticker',\n",
      "       'text', 'sentiment', 'emotion_anger', 'emotion_disgust', 'emotion_fear',\n",
      "       'emotion_joy', 'emotion_neutral', 'emotion_sadness', 'emotion_surprize',\n",
      "       'stance_positive', 'stance_negative', 'sector', 'company_name',\n",
      "       'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9',\n",
      "       'MACDs_12_26_9', 'RSI_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3',\n",
      "       'ATRr_14', 'BB_upper', 'BB_middle', 'BB_lower', 'OBV'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(master_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ytoycwxUDtS3"
   },
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'open', 'high', 'low', 'close', 'volume'\n",
    "    # 'stance_positive', 'stance_negative',\n",
    "    # 'sentiment'\n",
    "]\n",
    "\n",
    "new_indicator_columns = [\n",
    "    'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',\n",
    "    'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3',\n",
    "    'BB_upper', 'BB_middle', 'BB_lower', 'OBV'\n",
    "]\n",
    "feature_columns.extend(new_indicator_columns)\n",
    "\n",
    "sequence_length=12\n",
    "\n",
    "\n",
    "\n",
    "all_pipelines = {}\n",
    "all_results_dfs = {}\n",
    "all_analyses = {}\n",
    "fixed_bucket_edges = np.array([-0.08, -0.03, -0.01, 0.0, 0.01, 0.03, 0.08], dtype=float)\n",
    "n_classes = len(fixed_bucket_edges) - 1\n",
    "ordinal_head = 'corn'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RBV6cWZNDuCW",
    "outputId": "07c614de-47f3-4115-a4f2-0a8e11405f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "  RUNNING PIPELINE FOR: BiGRU\n",
      "=========================\n",
      "\n",
      "Pipeline initialized for a 'classification' problem with horizon 5 steps. Device: cpu\n",
      "Processing 88 companies with LSTM model...\n",
      "Problem type: classification\n",
      "Sequence length: 12\n",
      "Features: ['open', 'high', 'low', 'close', 'volume', 'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3', 'BB_upper', 'BB_middle', 'BB_lower', 'OBV']\n",
      "\n",
      "[1/88] Processing AAPL...\n",
      "\n",
      "Processing AAPL (Consumer Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.40582 | val 1.47090\n",
      "  Epoch 016 - train 0.33046 | val 1.84425\n",
      "  Classification -> best τ=0.515 (val F1=0.5600)\n",
      "  Directional -> Accuracy: 0.6026, MCC: 0.1989, F1: 0.5634\n",
      "\n",
      "[2/88] Processing ABB...\n",
      "\n",
      "Processing ABB (Industrial Goods)...\n",
      "Insufficient data for ABB (62 < 167). Skipping...\n",
      "\n",
      "[3/88] Processing ABBV...\n",
      "\n",
      "Processing ABBV (Healthcare)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.52528 | val 0.80829\n",
      "  Epoch 016 - train 0.48504 | val 1.11458\n",
      "  Classification -> best τ=0.050 (val F1=0.6230)\n",
      "  Directional -> Accuracy: 0.4468, MCC: 0.0000, F1: 0.6176\n",
      "\n",
      "[4/88] Processing AEP...\n",
      "\n",
      "Processing AEP (Utilities)...\n",
      "Insufficient data for AEP (109 < 167). Skipping...\n",
      "\n",
      "[5/88] Processing AGFS...\n",
      "\n",
      "Processing AGFS (Conglomerates)...\n",
      "Insufficient data for AGFS (2 < 167). Skipping...\n",
      "\n",
      "[6/88] Processing AMGN...\n",
      "\n",
      "Processing AMGN (Healthcare)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.51593 | val 0.74226\n",
      "  Epoch 020 - train 0.38726 | val 1.40919\n",
      "  Epoch 022 - train 0.36886 | val 1.40261\n",
      "  Classification -> best τ=0.460 (val F1=0.7391)\n",
      "  Directional -> Accuracy: 0.6667, MCC: 0.2944, F1: 0.7419\n",
      "\n",
      "[7/88] Processing AMZN...\n",
      "\n",
      "Processing AMZN (Services)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.50338 | val 0.74074\n",
      "  Epoch 020 - train 0.37008 | val 0.99241\n",
      "  Epoch 029 - train 0.29424 | val 1.33710\n",
      "  Classification -> best τ=0.050 (val F1=0.8167)\n",
      "  Directional -> Accuracy: 0.6974, MCC: 0.0000, F1: 0.8217\n",
      "\n",
      "[8/88] Processing BA...\n",
      "\n",
      "Processing BA (Industrial Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.48762 | val 1.59761\n",
      "  Epoch 017 - train 0.44020 | val 2.82425\n",
      "  Classification -> best τ=0.485 (val F1=0.7385)\n",
      "  Directional -> Accuracy: 0.4423, MCC: 0.0000, F1: 0.6133\n",
      "\n",
      "[9/88] Processing BABA...\n",
      "\n",
      "Processing BABA (Services)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.44023 | val 0.80193\n",
      "  Epoch 020 - train 0.26860 | val 4.01359\n",
      "  Epoch 021 - train 0.27521 | val 4.55967\n",
      "  Classification -> best τ=0.610 (val F1=0.7879)\n",
      "  Directional -> Accuracy: 0.4490, MCC: 0.0000, F1: 0.0000\n",
      "\n",
      "[10/88] Processing BAC...\n",
      "\n",
      "Processing BAC (Financial)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.49170 | val 1.02383\n",
      "  Epoch 019 - train 0.37311 | val 1.26678\n",
      "  Classification -> best τ=0.340 (val F1=0.6944)\n",
      "  Directional -> Accuracy: 0.5143, MCC: -0.0370, F1: 0.6458\n",
      "\n",
      "[11/88] Processing BBL...\n",
      "\n",
      "Processing BBL (Basic Matierials)...\n",
      "Insufficient data for BBL (69 < 167). Skipping...\n",
      "\n",
      "[12/88] Processing BCH...\n",
      "\n",
      "Processing BCH (Financial)...\n",
      "Insufficient data for BCH (5 < 167). Skipping...\n",
      "\n",
      "[13/88] Processing BHP...\n",
      "\n",
      "Processing BHP (Basic Matierials)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.54083 | val 0.77136\n",
      "  Epoch 016 - train 0.48903 | val 0.93066\n",
      "  Classification -> best τ=0.515 (val F1=0.5556)\n",
      "  Directional -> Accuracy: 0.7111, MCC: 0.2166, F1: 0.3810\n",
      "\n",
      "[14/88] Processing BP...\n",
      "\n",
      "Processing BP (Basic Matierials)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.41336 | val 1.78531\n",
      "  Epoch 017 - train 0.36245 | val 2.83763\n",
      "  Classification -> best τ=0.490 (val F1=0.6538)\n",
      "  Directional -> Accuracy: 0.3095, MCC: 0.0000, F1: 0.4727\n",
      "\n",
      "[15/88] Processing BRK-A...\n",
      "\n",
      "Processing BRK-A (Financial)...\n",
      "Insufficient data for BRK-A (13 < 167). Skipping...\n",
      "\n",
      "[16/88] Processing BSAC...\n",
      "\n",
      "Processing BSAC (Financial)...\n",
      "Insufficient data for BSAC (11 < 167). Skipping...\n",
      "\n",
      "[17/88] Processing BUD...\n",
      "\n",
      "Processing BUD (Consumer Goods)...\n",
      "Insufficient data for BUD (96 < 167). Skipping...\n",
      "\n",
      "[18/88] Processing C...\n",
      "\n",
      "Processing C (Financial)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.41850 | val 1.65801\n",
      "  Epoch 019 - train 0.31273 | val 1.58892\n",
      "  Classification -> best τ=0.360 (val F1=0.6500)\n",
      "  Directional -> Accuracy: 0.5211, MCC: 0.0000, F1: 0.6852\n",
      "\n",
      "[19/88] Processing CAT...\n",
      "\n",
      "Processing CAT (Industrial Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.52245 | val 1.96794\n",
      "  Epoch 016 - train 0.47461 | val 2.50891\n",
      "  Classification -> best τ=0.530 (val F1=0.6122)\n",
      "  Directional -> Accuracy: 0.5490, MCC: 0.0000, F1: 0.7089\n",
      "\n",
      "[20/88] Processing CELG...\n",
      "\n",
      "Processing CELG (Healthcare)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.45424 | val 1.17276\n",
      "  Epoch 020 - train 0.33542 | val 1.60478\n",
      "  Epoch 021 - train 0.33801 | val 1.58487\n",
      "  Classification -> best τ=0.245 (val F1=0.8525)\n",
      "  Directional -> Accuracy: 0.5085, MCC: 0.0000, F1: 0.6742\n",
      "\n",
      "[21/88] Processing CHL...\n",
      "\n",
      "Processing CHL (Technology)...\n",
      "Insufficient data for CHL (96 < 167). Skipping...\n",
      "\n",
      "[22/88] Processing CHTR...\n",
      "\n",
      "Processing CHTR (Services)...\n",
      "Insufficient data for CHTR (137 < 167). Skipping...\n",
      "\n",
      "[23/88] Processing CMCSA...\n",
      "\n",
      "Processing CMCSA (Services)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.50558 | val 1.90589\n",
      "  Epoch 016 - train 0.44465 | val 3.09839\n",
      "  Classification -> best τ=0.445 (val F1=0.8333)\n",
      "  Directional -> Accuracy: 0.5000, MCC: 0.0000, F1: 0.6207\n",
      "\n",
      "[24/88] Processing CODI...\n",
      "\n",
      "Processing CODI (Conglomerates)...\n",
      "Insufficient data for CODI (29 < 167). Skipping...\n",
      "\n",
      "[25/88] Processing CSCO...\n",
      "\n",
      "Processing CSCO (Technology)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.48346 | val 0.89202\n",
      "  Epoch 020 - train 0.33948 | val 0.98477\n",
      "  Classification -> best τ=0.415 (val F1=0.7179)\n",
      "  Directional -> Accuracy: 0.5645, MCC: 0.1702, F1: 0.6494\n",
      "\n",
      "[26/88] Processing CVX...\n",
      "\n",
      "Processing CVX (Basic Matierials)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.46101 | val 2.42425\n",
      "  Epoch 017 - train 0.40219 | val 3.22359\n",
      "  Classification -> best τ=0.550 (val F1=0.7458)\n",
      "  Directional -> Accuracy: 0.5660, MCC: 0.1221, F1: 0.4889\n",
      "\n",
      "[27/88] Processing D...\n",
      "\n",
      "Processing D (Utilities)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.43793 | val 0.90383\n",
      "  Epoch 020 - train 0.36068 | val 1.22162\n",
      "  Classification -> best τ=0.265 (val F1=0.8200)\n",
      "  Directional -> Accuracy: 0.5634, MCC: 0.2191, F1: 0.7048\n",
      "\n",
      "[28/88] Processing DHR...\n",
      "\n",
      "Processing DHR (Industrial Goods)...\n",
      "Insufficient data for DHR (91 < 167). Skipping...\n",
      "\n",
      "[29/88] Processing DIS...\n",
      "\n",
      "Processing DIS (Services)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.49754 | val 1.31433\n",
      "  Epoch 016 - train 0.42700 | val 2.36914\n",
      "  Classification -> best τ=0.050 (val F1=0.6977)\n",
      "  Directional -> Accuracy: 0.5833, MCC: 0.0000, F1: 0.7368\n",
      "\n",
      "[30/88] Processing DUK...\n",
      "\n",
      "Processing DUK (Utilities)...\n",
      "Insufficient data for DUK (117 < 167). Skipping...\n",
      "\n",
      "[31/88] Processing EXC...\n",
      "\n",
      "Processing EXC (Utilities)...\n",
      "Insufficient data for EXC (136 < 167). Skipping...\n",
      "\n",
      "[32/88] Processing FB...\n",
      "\n",
      "Processing FB (Technology)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.48021 | val 2.10147\n",
      "  Epoch 017 - train 0.39537 | val 2.41948\n",
      "  Classification -> best τ=0.475 (val F1=0.7573)\n",
      "  Directional -> Accuracy: 0.5325, MCC: 0.3340, F1: 0.4000\n",
      "\n",
      "[33/88] Processing GD...\n",
      "\n",
      "Processing GD (Industrial Goods)...\n",
      "Insufficient data for GD (153 < 167). Skipping...\n",
      "\n",
      "[34/88] Processing GE...\n",
      "\n",
      "Processing GE (Industrial Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.45769 | val 1.06432\n",
      "  Epoch 020 - train 0.35511 | val 2.58720\n",
      "  Classification -> best τ=0.250 (val F1=0.6316)\n",
      "  Directional -> Accuracy: 0.5303, MCC: 0.1188, F1: 0.5867\n",
      "\n",
      "[35/88] Processing GMRE...\n",
      "\n",
      "Processing GMRE (Conglomerates)...\n",
      "Insufficient data for GMRE (0 < 167). Skipping...\n",
      "\n",
      "[36/88] Processing GOOG...\n",
      "\n",
      "Processing GOOG (Technology)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.45466 | val 1.33546\n",
      "  Epoch 019 - train 0.31019 | val 2.02900\n",
      "  Classification -> best τ=0.295 (val F1=0.7429)\n",
      "  Directional -> Accuracy: 0.5789, MCC: 0.1842, F1: 0.6279\n",
      "\n",
      "[37/88] Processing HD...\n",
      "\n",
      "Processing HD (Services)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.48617 | val 0.82669\n",
      "  Epoch 016 - train 0.44015 | val 1.17482\n",
      "  Classification -> best τ=0.050 (val F1=0.6866)\n",
      "  Directional -> Accuracy: 0.6531, MCC: 0.0000, F1: 0.7901\n",
      "\n",
      "[38/88] Processing HON...\n",
      "\n",
      "Processing HON (Industrial Goods)...\n",
      "Insufficient data for HON (142 < 167). Skipping...\n",
      "\n",
      "[39/88] Processing HRG...\n",
      "\n",
      "Processing HRG (Conglomerates)...\n",
      "Insufficient data for HRG (30 < 167). Skipping...\n",
      "\n",
      "[40/88] Processing HSBC...\n",
      "\n",
      "Processing HSBC (Financial)...\n",
      "Insufficient data for HSBC (71 < 167). Skipping...\n",
      "\n",
      "[41/88] Processing IEP...\n",
      "\n",
      "Processing IEP (Conglomerates)...\n",
      "Insufficient data for IEP (58 < 167). Skipping...\n",
      "\n",
      "[42/88] Processing INTC...\n",
      "\n",
      "Processing INTC (Technology)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.52485 | val 0.85045\n",
      "  Epoch 016 - train 0.47584 | val 0.81837\n",
      "  Classification -> best τ=0.050 (val F1=0.6522)\n",
      "  Directional -> Accuracy: 0.5672, MCC: 0.0000, F1: 0.7238\n",
      "\n",
      "[43/88] Processing JNJ...\n",
      "\n",
      "Processing JNJ (Healthcare)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.41465 | val 1.89793\n",
      "  Epoch 016 - train 0.36756 | val 2.21105\n",
      "  Classification -> best τ=0.520 (val F1=0.6582)\n",
      "  Directional -> Accuracy: 0.6780, MCC: 0.3938, F1: 0.7164\n",
      "\n",
      "[44/88] Processing JPM...\n",
      "\n",
      "Processing JPM (Financial)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.43500 | val 1.31544\n",
      "  Epoch 017 - train 0.37000 | val 1.51289\n",
      "  Classification -> best τ=0.465 (val F1=0.6970)\n",
      "  Directional -> Accuracy: 0.5970, MCC: 0.0000, F1: 0.7477\n",
      "\n",
      "[45/88] Processing KO...\n",
      "\n",
      "Processing KO (Consumer Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "Error processing KO: Expected more than 1 value per channel when training, got input size torch.Size([1, 64])\n",
      "\n",
      "[46/88] Processing LMT...\n",
      "\n",
      "Processing LMT (Industrial Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.44318 | val 0.77826\n",
      "  Epoch 016 - train 0.38830 | val 0.99945\n",
      "  Classification -> best τ=0.050 (val F1=0.8837)\n",
      "  Directional -> Accuracy: 0.4792, MCC: 0.0000, F1: 0.6479\n",
      "\n",
      "[47/88] Processing MA...\n",
      "\n",
      "Processing MA (Financial)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.51052 | val 0.72897\n",
      "  Epoch 016 - train 0.49233 | val 0.84659\n",
      "  Classification -> best τ=0.050 (val F1=0.7619)\n",
      "  Directional -> Accuracy: 0.5682, MCC: 0.0000, F1: 0.7246\n",
      "\n",
      "[48/88] Processing MCD...\n",
      "\n",
      "Processing MCD (Services)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.42438 | val 0.76968\n",
      "  Epoch 020 - train 0.31935 | val 1.18008\n",
      "  Epoch 021 - train 0.31514 | val 1.17529\n",
      "  Classification -> best τ=0.380 (val F1=0.7407)\n",
      "  Directional -> Accuracy: 0.5862, MCC: 0.2103, F1: 0.6571\n",
      "\n",
      "[49/88] Processing MDT...\n",
      "\n",
      "Processing MDT (Healthcare)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.53389 | val 0.70552\n",
      "  Epoch 020 - train 0.44943 | val 1.53575\n",
      "  Epoch 022 - train 0.43660 | val 2.10837\n",
      "  Classification -> best τ=0.465 (val F1=0.6154)\n",
      "  Directional -> Accuracy: 0.4419, MCC: -0.2261, F1: 0.6129\n",
      "\n",
      "[50/88] Processing MMM...\n",
      "\n",
      "Processing MMM (Industrial Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.49071 | val 0.85815\n",
      "  Epoch 016 - train 0.42212 | val 1.42806\n",
      "  Classification -> best τ=0.050 (val F1=0.5238)\n",
      "  Directional -> Accuracy: 0.6000, MCC: 0.0000, F1: 0.7500\n",
      "\n",
      "[51/88] Processing MO...\n",
      "\n",
      "Processing MO (Consumer Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.47131 | val 0.95457\n",
      "  Epoch 016 - train 0.43167 | val 1.55530\n",
      "  Classification -> best τ=0.050 (val F1=0.8929)\n",
      "  Directional -> Accuracy: 0.6222, MCC: 0.0000, F1: 0.7671\n",
      "\n",
      "[52/88] Processing MRK...\n",
      "\n",
      "Processing MRK (Healthcare)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.49953 | val 0.81961\n",
      "  Epoch 020 - train 0.39333 | val 1.02594\n",
      "  Epoch 021 - train 0.37269 | val 1.06723\n",
      "  Classification -> best τ=0.425 (val F1=0.6667)\n",
      "  Directional -> Accuracy: 0.5294, MCC: 0.0000, F1: 0.6923\n",
      "\n",
      "[53/88] Processing MSFT...\n",
      "\n",
      "Processing MSFT (Technology)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.40995 | val 1.80808\n",
      "  Epoch 018 - train 0.33527 | val 2.66014\n",
      "  Classification -> best τ=0.505 (val F1=0.6875)\n",
      "  Directional -> Accuracy: 0.5811, MCC: 0.2268, F1: 0.5753\n",
      "\n",
      "[54/88] Processing NEE...\n",
      "\n",
      "Processing NEE (Utilities)...\n",
      "Insufficient data for NEE (119 < 167). Skipping...\n",
      "\n",
      "[55/88] Processing NGG...\n",
      "\n",
      "Processing NGG (Utilities)...\n",
      "Insufficient data for NGG (28 < 167). Skipping...\n",
      "\n",
      "[56/88] Processing NVS...\n",
      "\n",
      "Processing NVS (Healthcare)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.51024 | val 0.72821\n",
      "  Epoch 016 - train 0.46439 | val 0.97516\n",
      "  Classification -> best τ=0.050 (val F1=0.6667)\n",
      "  Directional -> Accuracy: 0.4615, MCC: 0.0000, F1: 0.6316\n",
      "\n",
      "[57/88] Processing ORCL...\n",
      "\n",
      "Processing ORCL (Technology)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.47156 | val 0.97385\n",
      "  Epoch 016 - train 0.41316 | val 1.46362\n",
      "  Classification -> best τ=0.490 (val F1=0.4444)\n",
      "  Directional -> Accuracy: 0.5556, MCC: 0.0000, F1: 0.7143\n",
      "\n",
      "[58/88] Processing PCG...\n",
      "\n",
      "Processing PCG (Utilities)...\n",
      "Insufficient data for PCG (101 < 167). Skipping...\n",
      "\n",
      "[59/88] Processing PCLN...\n",
      "\n",
      "Processing PCLN (Services)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.44566 | val 1.48573\n",
      "  Epoch 018 - train 0.32445 | val 1.88126\n",
      "  Classification -> best τ=0.375 (val F1=0.6429)\n",
      "  Directional -> Accuracy: 0.6176, MCC: 0.1888, F1: 0.7111\n",
      "\n",
      "[60/88] Processing PEP...\n",
      "\n",
      "Processing PEP (Consumer Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.50656 | val 0.69603\n",
      "  Epoch 020 - train 0.36618 | val 1.05679\n",
      "  Epoch 026 - train 0.32948 | val 1.05342\n",
      "  Classification -> best τ=0.410 (val F1=0.8000)\n",
      "  Directional -> Accuracy: 0.5455, MCC: 0.0925, F1: 0.5833\n",
      "\n",
      "[61/88] Processing PFE...\n",
      "\n",
      "Processing PFE (Healthcare)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.43865 | val 1.11888\n",
      "  Epoch 017 - train 0.38972 | val 2.52119\n",
      "  Classification -> best τ=0.050 (val F1=0.6750)\n",
      "  Directional -> Accuracy: 0.3966, MCC: 0.0000, F1: 0.5679\n",
      "\n",
      "[62/88] Processing PG...\n",
      "\n",
      "Processing PG (Consumer Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.53617 | val 0.95001\n",
      "  Epoch 016 - train 0.49427 | val 1.21680\n",
      "  Classification -> best τ=0.050 (val F1=0.6349)\n",
      "  Directional -> Accuracy: 0.5417, MCC: 0.0000, F1: 0.7027\n",
      "\n",
      "[63/88] Processing PICO...\n",
      "\n",
      "Processing PICO (Conglomerates)...\n",
      "Insufficient data for PICO (16 < 167). Skipping...\n",
      "\n",
      "[64/88] Processing PM...\n",
      "\n",
      "Processing PM (Consumer Goods)...\n",
      "Insufficient data for PM (145 < 167). Skipping...\n",
      "\n",
      "[65/88] Processing PPL...\n",
      "\n",
      "Processing PPL (Utilities)...\n",
      "Insufficient data for PPL (108 < 167). Skipping...\n",
      "\n",
      "[66/88] Processing PTR...\n",
      "\n",
      "Processing PTR (Basic Matierials)...\n",
      "Insufficient data for PTR (44 < 167). Skipping...\n",
      "\n",
      "[67/88] Processing RDS-B...\n",
      "\n",
      "Processing RDS-B (Basic Matierials)...\n",
      "Insufficient data for RDS-B (3 < 167). Skipping...\n",
      "\n",
      "[68/88] Processing REX...\n",
      "\n",
      "Processing REX (Conglomerates)...\n",
      "Insufficient data for REX (70 < 167). Skipping...\n",
      "\n",
      "[69/88] Processing SLB...\n",
      "\n",
      "Processing SLB (Basic Matierials)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.38719 | val 1.11547\n",
      "  Epoch 016 - train 0.33679 | val 2.32589\n",
      "  Classification -> best τ=0.050 (val F1=0.4571)\n",
      "  Directional -> Accuracy: 0.4250, MCC: 0.0000, F1: 0.5965\n",
      "\n",
      "[70/88] Processing SNP...\n",
      "\n",
      "Processing SNP (Basic Matierials)...\n",
      "Insufficient data for SNP (23 < 167). Skipping...\n",
      "\n",
      "[71/88] Processing SNY...\n",
      "\n",
      "Processing SNY (Healthcare)...\n",
      "Insufficient data for SNY (143 < 167). Skipping...\n",
      "\n",
      "[72/88] Processing SO...\n",
      "\n",
      "Processing SO (Utilities)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.56288 | val 0.69784\n",
      "  Epoch 020 - train 0.44610 | val 0.74267\n",
      "  Epoch 024 - train 0.40537 | val 0.76450\n",
      "  Classification -> best τ=0.505 (val F1=0.6667)\n",
      "  Directional -> Accuracy: 0.4615, MCC: -0.2711, F1: 0.6182\n",
      "\n",
      "[73/88] Processing SPLP...\n",
      "\n",
      "Processing SPLP (Conglomerates)...\n",
      "Insufficient data for SPLP (5 < 167). Skipping...\n",
      "\n",
      "[74/88] Processing SRE...\n",
      "\n",
      "Processing SRE (Utilities)...\n",
      "Insufficient data for SRE (69 < 167). Skipping...\n",
      "\n",
      "[75/88] Processing T...\n",
      "\n",
      "Processing T (Technology)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.39995 | val 1.08566\n",
      "  Epoch 019 - train 0.27219 | val 1.46304\n",
      "  Classification -> best τ=0.380 (val F1=0.6957)\n",
      "  Directional -> Accuracy: 0.5395, MCC: 0.0000, F1: 0.7009\n",
      "\n",
      "[76/88] Processing TM...\n",
      "\n",
      "Processing TM (Consumer Goods)...\n",
      "Insufficient data for TM (117 < 167). Skipping...\n",
      "\n",
      "[77/88] Processing TOT...\n",
      "\n",
      "Processing TOT (Basic Matierials)...\n",
      "Insufficient data for TOT (93 < 167). Skipping...\n",
      "\n",
      "[78/88] Processing TSM...\n",
      "\n",
      "Processing TSM (Technology)...\n",
      "Insufficient data for TSM (87 < 167). Skipping...\n",
      "\n",
      "[79/88] Processing UL...\n",
      "\n",
      "Processing UL (Consumer Goods)...\n",
      "Insufficient data for UL (61 < 167). Skipping...\n",
      "\n",
      "[80/88] Processing UN...\n",
      "\n",
      "Processing UN (Consumer Goods)...\n",
      "Insufficient data for UN (54 < 167). Skipping...\n",
      "\n",
      "[81/88] Processing UNH...\n",
      "\n",
      "Processing UNH (Healthcare)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.44591 | val 0.65611\n",
      "  Epoch 020 - train 0.31442 | val 1.38847\n",
      "  Epoch 026 - train 0.28725 | val 1.38357\n",
      "  Classification -> best τ=0.350 (val F1=0.7600)\n",
      "  Directional -> Accuracy: 0.5625, MCC: 0.0000, F1: 0.7200\n",
      "\n",
      "[82/88] Processing UPS...\n",
      "\n",
      "Processing UPS (Services)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.48283 | val 0.69261\n",
      "  Epoch 016 - train 0.42216 | val 0.80791\n",
      "  Classification -> best τ=0.050 (val F1=0.5806)\n",
      "  Directional -> Accuracy: 0.4889, MCC: 0.0000, F1: 0.6567\n",
      "\n",
      "[83/88] Processing UTX...\n",
      "\n",
      "Processing UTX (Industrial Goods)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.46113 | val 1.14297\n",
      "  Epoch 016 - train 0.42462 | val 2.34797\n",
      "  Classification -> best τ=0.510 (val F1=0.5000)\n",
      "  Directional -> Accuracy: 0.4390, MCC: 0.0000, F1: 0.6102\n",
      "\n",
      "[84/88] Processing V...\n",
      "\n",
      "Processing V (Financial)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.47367 | val 2.88189\n",
      "  Epoch 016 - train 0.43155 | val 3.78720\n",
      "  Classification -> best τ=0.050 (val F1=0.7619)\n",
      "  Directional -> Accuracy: 0.5789, MCC: 0.0000, F1: 0.7333\n",
      "\n",
      "[85/88] Processing VZ...\n",
      "\n",
      "Processing VZ (Technology)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.47197 | val 1.27408\n",
      "  Epoch 016 - train 0.39565 | val 1.62445\n",
      "  Classification -> best τ=0.050 (val F1=0.6410)\n",
      "  Directional -> Accuracy: 0.5345, MCC: 0.0000, F1: 0.6966\n",
      "\n",
      "[86/88] Processing WFC...\n",
      "\n",
      "Processing WFC (Financial)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.44635 | val 3.21266\n",
      "  Epoch 016 - train 0.40326 | val 4.67642\n",
      "  Classification -> best τ=0.050 (val F1=0.7013)\n",
      "  Directional -> Accuracy: 0.6000, MCC: 0.0000, F1: 0.7500\n",
      "\n",
      "[87/88] Processing WMT...\n",
      "\n",
      "Processing WMT (Services)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.51781 | val 1.18840\n",
      "  Epoch 016 - train 0.47000 | val 1.17479\n",
      "  Classification -> best τ=0.050 (val F1=0.5143)\n",
      "  Directional -> Accuracy: 0.5439, MCC: 0.0000, F1: 0.7045\n",
      "\n",
      "[88/88] Processing XOM...\n",
      "\n",
      "Processing XOM (Basic Matierials)...\n",
      "Adjusting for multi-step horizon...\n",
      "  Epoch 010 - train 0.50103 | val 1.53017\n",
      "  Epoch 016 - train 0.45532 | val 1.98914\n",
      "  Classification -> best τ=0.050 (val F1=0.6098)\n",
      "  Directional -> Accuracy: 0.6452, MCC: 0.0000, F1: 0.7843\n",
      "\n",
      "================================================================================\n",
      "Pipeline completed: 49/88 companies processed successfully\n",
      "================================================================================\n",
      "Loss curves saved to results/benchmarking/classification/LSTM_loss_curves.csv\n",
      "================================================================================\n",
      "STOCK PREDICTION PIPELINE RESULTS\n",
      "================================================================================\n",
      "Model: LSTM | Problem: classification\n",
      "Companies analyzed: 49\n",
      "Average samples per company: 319\n",
      "==================================================\n",
      "OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Directional Accuracy:   0.5444 (±0.0801)\n",
      "Matthews Correlation:   0.0497 (±0.1228)\n",
      "F1 Score:              0.6455 (±0.1319)\n",
      "Precision:             0.5634 (±0.1471)\n",
      "Recall:                0.8462 (±0.2492)\n",
      "==================================================\n",
      "PERFORMANCE BY SECTOR\n",
      "==================================================\n",
      "Consumer Goods       | Acc: 0.578±0.041 | MCC: 0.073 | Companies: 4\n",
      "Services             | Acc: 0.569±0.081 | MCC: 0.044 | Companies: 9\n",
      "Financial            | Acc: 0.563±0.037 | MCC: -0.006 | Companies: 6\n",
      "Technology           | Acc: 0.557±0.019 | MCC: 0.114 | Companies: 8\n",
      "Basic Matierials     | Acc: 0.531±0.164 | MCC: 0.068 | Companies: 5\n",
      "Healthcare           | Acc: 0.521±0.099 | MCC: 0.051 | Companies: 9\n",
      "Utilities            | Acc: 0.512±0.072 | MCC: -0.026 | Companies: 2\n",
      "Industrial Goods     | Acc: 0.507±0.064 | MCC: 0.020 | Companies: 6\n",
      "==================================================\n",
      "TOP 10 PERFORMERS (by Directional Accuracy)\n",
      "==================================================\n",
      "BHP                  | Basic Matierials | Acc: 0.711 | MCC: 0.217\n",
      "AMZN                 | Services        | Acc: 0.697 | MCC: 0.000\n",
      "JNJ                  | Healthcare      | Acc: 0.678 | MCC: 0.394\n",
      "AMGN                 | Healthcare      | Acc: 0.667 | MCC: 0.294\n",
      "HD                   | Services        | Acc: 0.653 | MCC: 0.000\n",
      "XOM                  | Basic Matierials | Acc: 0.645 | MCC: 0.000\n",
      "MO                   | Consumer Goods  | Acc: 0.622 | MCC: 0.000\n",
      "PCLN                 | Services        | Acc: 0.618 | MCC: 0.189\n",
      "AAPL                 | Consumer Goods  | Acc: 0.603 | MCC: 0.199\n",
      "MMM                  | Industrial Goods | Acc: 0.600 | MCC: 0.000\n",
      "Results saved to results/benchmarking/classification/LSTM.csv\n",
      "\n",
      "Displaying first 5 rows of BiGRU results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>sector</th>\n",
       "      <th>model_type</th>\n",
       "      <th>problem_type</th>\n",
       "      <th>horizon_steps</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "      <th>mcc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>directional_accuracy</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>train_samples</th>\n",
       "      <th>val_samples</th>\n",
       "      <th>test_samples</th>\n",
       "      <th>epochs_trained</th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>best_threshold_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>classification</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.198874</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>468</td>\n",
       "      <td>307</td>\n",
       "      <td>73</td>\n",
       "      <td>78</td>\n",
       "      <td>16</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>classification</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>283</td>\n",
       "      <td>184</td>\n",
       "      <td>42</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.622951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMGN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>classification</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.294406</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>293</td>\n",
       "      <td>191</td>\n",
       "      <td>44</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.739130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Services</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>classification</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>458</td>\n",
       "      <td>301</td>\n",
       "      <td>71</td>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BA</td>\n",
       "      <td>Industrial Goods</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>classification</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>315</td>\n",
       "      <td>206</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "      <td>17</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.738462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company            sector model_type    problem_type  horizon_steps  mse  \\\n",
       "0    AAPL    Consumer Goods       LSTM  classification              5  NaN   \n",
       "1    ABBV        Healthcare       LSTM  classification              5  NaN   \n",
       "2    AMGN        Healthcare       LSTM  classification              5  NaN   \n",
       "3    AMZN          Services       LSTM  classification              5  NaN   \n",
       "4      BA  Industrial Goods       LSTM  classification              5  NaN   \n",
       "\n",
       "   mae  r2       mcc        f1  precision    recall  directional_accuracy  \\\n",
       "0  NaN NaN  0.198874  0.563380   0.571429  0.555556              0.602564   \n",
       "1  NaN NaN  0.000000  0.617647   0.446809  1.000000              0.446809   \n",
       "2  NaN NaN  0.294406  0.741935   0.676471  0.821429              0.666667   \n",
       "3  NaN NaN  0.000000  0.821705   0.697368  1.000000              0.697368   \n",
       "4  NaN NaN  0.000000  0.613333   0.442308  1.000000              0.442308   \n",
       "\n",
       "   n_samples  train_samples  val_samples  test_samples  epochs_trained  \\\n",
       "0        468            307           73            78              16   \n",
       "1        283            184           42            47              16   \n",
       "2        293            191           44            48              22   \n",
       "3        458            301           71            76              29   \n",
       "4        315            206           47            52              17   \n",
       "\n",
       "   best_threshold  best_threshold_metric  \n",
       "0           0.515               0.560000  \n",
       "1           0.050               0.622951  \n",
       "2           0.460               0.739130  \n",
       "3           0.050               0.816667  \n",
       "4           0.485               0.738462  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n{'='*25}\\n  RUNNING PIPELINE FOR: BiGRU\\n{'='*25}\\n\")\n",
    "\n",
    "pipeline_BiGRU = StockPredictionPipeline(\n",
    "    df=master_df,\n",
    "    feature_columns=feature_columns,\n",
    "    model_type='LSTM',\n",
    "    sequence_length=sequence_length,\n",
    "    problem_type='classification',\n",
    "    horizon_steps=5,\n",
    "    n_classes=n_classes,\n",
    "    ordinal_head=ordinal_head,\n",
    "    fixed_bucket_edges=fixed_bucket_edges\n",
    ")\n",
    "\n",
    "results_BiGRU = pipeline_BiGRU.run_pipeline()\n",
    "\n",
    "loss_df = pipeline_BiGRU.get_loss_curves_df()\n",
    "\n",
    "pipeline_BiGRU.save_loss_curves('results/benchmarking/')\n",
    "\n",
    "if results_BiGRU is not None and not results_BiGRU.empty:\n",
    "    analysis_BiGRU = pipeline_BiGRU.analyze_results()\n",
    "    pipeline_BiGRU.save_results(results_BiGRU, output_dir='results/benchmarking/')\n",
    "\n",
    "    all_pipelines[\"BiGRU\"] = pipeline_BiGRU\n",
    "    all_results_dfs[\"BiGRU\"] = results_BiGRU\n",
    "    all_analyses[\"BiGRU\"] = analysis_BiGRU\n",
    "\n",
    "    print(\"\\nDisplaying first 5 rows of BiGRU results:\")\n",
    "    display(results_BiGRU.head())\n",
    "else:\n",
    "    print(f\"\\n[FAILED] Pipeline for BiGRU did not produce any results.\")\n",
    "\n",
    "del pipeline_BiGRU"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
