{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Results from Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgZ_ylAADNb6",
    "outputId": "6134cc02-4bf2-4dbb-a201-b8fd4d0df03c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_ta_classic in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.3.36)\n",
      "Requirement already satisfied: numpy>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.1.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2025.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (1.16.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas>=2.0.0->pandas_ta_classic) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas_ta_classic in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.3.36)\n",
      "Requirement already satisfied: numpy>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.1.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (2025.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas_ta_classic) (1.16.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from pandas>=2.0.0->pandas_ta_classic) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (21.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas_ta_classic\n",
    "%pip install --upgrade pandas_ta_classic\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "befeuglqHy8f",
    "outputId": "a2bcece3-930c-4c77-c6ff-3473d1fde791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the problematic import line in squeeze_pro.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import site\n",
    "import os\n",
    "\n",
    "# Find the path to the pandas_ta_classic library and patch it\n",
    "pandas_ta_classic_path = None\n",
    "for sp in site.getsitepackages():\n",
    "    pandas_ta_classic_path = os.path.join(sp, 'pandas_ta_classic')\n",
    "    if os.path.exists(pandas_ta_classic_path):\n",
    "        break\n",
    "\n",
    "if pandas_ta_classic_path:\n",
    "    squeeze_pro_path = os.path.join(pandas_ta_classic_path, 'momentum', 'squeeze_pro.py')\n",
    "    if os.path.exists(squeeze_pro_path):\n",
    "        try:\n",
    "            with open(squeeze_pro_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            new_lines = []\n",
    "            fixed = False\n",
    "            for line in lines:\n",
    "                if \"from numpy import NaN as npNaN\" in line:\n",
    "                    new_lines.append(line.replace(\"from numpy import NaN as npNaN\", \"# from numpy import NaN as npNaN\\nimport numpy as np\\n\"))\n",
    "                    fixed = True\n",
    "                    print(\"Modified import statement in squeeze_pro.py\")\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "\n",
    "            if fixed:\n",
    "                with open(squeeze_pro_path, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "                print(\"Successfully patched pandas_ta_classic/momentum/squeeze_pro.py\")\n",
    "            else:\n",
    "                print(\"Could not find the problematic import line in squeeze_pro.py\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error modifying squeeze_pro.py: {e}\")\n",
    "    else:\n",
    "        print(f\"Could not find squeeze_pro.py at {squeeze_pro_path}\")\n",
    "else:\n",
    "    print(\"Could not find the pandas_ta_classic library installation path.\")\n",
    "\n",
    "import pandas_ta_classic as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: torch==2.8.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torchvision) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from torch==2.8.0->torchvision) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dylanhuang/micromamba/envs/df_ae2/lib/python3.13/site-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install scikit-learn\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, matthews_corrcoef,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import random\n",
    "\n",
    "from unicodedata import bidirectional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Optional: stricter determinism (may slow down training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seeds(42)\n",
    "\n",
    "# Datasets\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Models\n",
    "class RNNHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared head:\n",
    "      - RNN stack (LSTM/GRU, uni/bi)\n",
    "      - BatchNorm + Dense(32, ReLU) + Dropout(0.3)\n",
    "      - Output layer (1 unit): linear (regression) or logits (classification)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, rnn_type='LSTM', bidirectional=False, problem_type='regression'):\n",
    "        super().__init__()\n",
    "        hidden1 = 128\n",
    "        hidden2 = 64\n",
    "        self.problem_type = problem_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "\n",
    "        rnn_cls = {'LSTM': nn.LSTM, 'GRU': nn.GRU}[('GRU' if 'GRU' in self.rnn_type else 'LSTM')]\n",
    "\n",
    "        self.rnn1 = rnn_cls(\n",
    "            input_size=input_size, hidden_size=hidden1, num_layers=1,\n",
    "            batch_first=True, dropout=0.0, bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        self.inter_rnn_drop = nn.Dropout(0.1)\n",
    "        \n",
    "        self.rnn2 = rnn_cls(\n",
    "            input_size=hidden1*(2 if bidirectional else 1), hidden_size=hidden2, num_layers=1,\n",
    "            batch_first=True, dropout=0.0, bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        feat_dim = hidden2*(2 if bidirectional else 1)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(feat_dim)\n",
    "        self.fc = nn.Linear(feat_dim, 32)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, F]\n",
    "        out, _ = self.rnn1(x)\n",
    "        out = self.inter_rnn_drop(out)   # inter-layer dropout (sequence-wise)\n",
    "        out, _ = self.rnn2(out)\n",
    "        # take last timestep: [B, T, H] -> [B, H]\n",
    "        out = out[:, -1, :]\n",
    "        out = self.bn(out)\n",
    "        out = F.relu(self.fc(out))\n",
    "        out = self.drop(out)\n",
    "        out = self.out(out)  # shape [B,1]\n",
    "        return out  # regression: raw; classification: logits\n",
    "\n",
    "def build_model(input_shape, model_type='LSTM', problem_type='regression'):\n",
    "    seq_len, n_features = input_shape\n",
    "    model_type = model_type.upper()\n",
    "    if model_type == 'LSTM':\n",
    "        return RNNHead(n_features, rnn_type='LSTM', bidirectional=False, problem_type=problem_type)\n",
    "    elif model_type == 'BILSTM':\n",
    "        return RNNHead(n_features, rnn_type='LSTM', bidirectional=True, problem_type=problem_type)\n",
    "    elif model_type == 'GRU':\n",
    "        return RNNHead(n_features, rnn_type='GRU', bidirectional=False, problem_type=problem_type)\n",
    "    elif model_type == 'BIGRU':\n",
    "        return RNNHead(n_features, rnn_type='GRU', bidirectional=True, problem_type=problem_type)\n",
    "    else:\n",
    "        raise ValueError(\"Model type must be one of: ['LSTM','BiLSTM','GRU','BiGRU']\")\n",
    "\n",
    "# Early Stopping (PyTorch)\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=15, min_delta=0.0, restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best = restore_best\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        improved = (self.best_loss - val_loss) > self.min_delta\n",
    "        if improved:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best:\n",
    "                # Deep copy state dict\n",
    "                self.best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGvgtiPTiBQ4"
   },
   "outputs": [],
   "source": [
    "class StockPredictionPipeline:\n",
    "    def __init__(self, df, feature_columns, model_type='LSTM', sequence_length=30, problem_type='regression', horizon_steps=1):\n",
    "        self.df = df.copy()\n",
    "        self.feature_columns = feature_columns\n",
    "        self.model_type = model_type\n",
    "        self.sequence_length = sequence_length\n",
    "        self.problem_type = problem_type\n",
    "        self.horizon_steps = horizon_steps\n",
    "        self.results = []\n",
    "        self.loss_curves = []\n",
    "\n",
    "        # Validate\n",
    "        self._validate_inputs()\n",
    "\n",
    "        # Device & precision\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mixed_precision = torch.cuda.is_available()\n",
    "\n",
    "        print(f\"Pipeline initialized for a '{self.problem_type}' problem \"\n",
    "              f\"with horizon {self.horizon_steps} steps. Device: {self.device}\")\n",
    "\n",
    "    def _validate_inputs(self):\n",
    "        missing_cols = [col for col in self.feature_columns if col not in self.df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_cols}\")\n",
    "\n",
    "        if 'close' not in self.df.columns and 'close_price' not in self.df.columns:\n",
    "            raise ValueError(\"No 'close' or 'close_price' column found in data\")\n",
    "\n",
    "        valid_models = ['LSTM', 'BiLSTM', 'GRU', 'BiGRU']\n",
    "        if self.model_type not in valid_models:\n",
    "            raise ValueError(f\"Model type must be one of: {valid_models}\")\n",
    "\n",
    "        if self.problem_type not in ['regression', 'classification']:\n",
    "            raise ValueError(\"Problem type must be 'regression' or 'classification'\")\n",
    "\n",
    "    def create_target_variable(self, company_data):\n",
    "        company_data = company_data.copy()\n",
    "        price_col = 'close' if 'close' in company_data.columns else 'close_price'\n",
    "        if 'date' in company_data.columns:\n",
    "            company_data = company_data.sort_values('date')\n",
    "            \n",
    "        h = self.horizon_steps\n",
    "\n",
    "        company_data['target_regression'] = (\n",
    "            np.log(company_data[price_col].shift(-h)) - np.log(company_data[price_col])\n",
    "        )\n",
    "        company_data['target_direction'] = (company_data['target_regression'] > 0).astype(int)\n",
    "        company_data = company_data.dropna()\n",
    "        return company_data\n",
    "\n",
    "    def create_sequences(self, features, *targets):\n",
    "        X = []\n",
    "        y_sequences = [[] for _ in targets]\n",
    "        for i in range(self.sequence_length, len(features)):\n",
    "            X.append(features[i-self.sequence_length:i])\n",
    "            for j, target in enumerate(targets):\n",
    "                y_sequences[j].append(target[i])\n",
    "        return (np.array(X),) + tuple(np.array(y) for y in y_sequences)\n",
    "\n",
    "    def _train_one_epoch(self, model, loader, optimizer, loss_fn, scaler):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device).view(-1, 1)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            ctx = torch.amp.autocast('cuda') if self.mixed_precision else nullcontext()\n",
    "            with ctx:\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "\n",
    "            if self.mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        return total_loss / len(loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_one_epoch(self, model, loader, loss_fn):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            yb = yb.to(self.device).view(-1, 1)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "        return total_loss / len(loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _predict(self, model, loader):\n",
    "        model.eval()\n",
    "        outs = []\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(self.device)\n",
    "            logits = model(xb).squeeze(1).detach().cpu().numpy()\n",
    "            outs.append(logits)\n",
    "        return np.concatenate(outs, axis=0)\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        model = build_model(input_shape, model_type=self.model_type, problem_type=self.problem_type)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def process_company(self, company_name, company_data, sector):\n",
    "        print(f\"\\nProcessing {company_name} ({sector})...\")\n",
    "        try:\n",
    "            company_data = self.create_target_variable(company_data)\n",
    "\n",
    "            # Min samples requirement (same heuristic)\n",
    "            min_samples = self.sequence_length + 150 + self.horizon_steps\n",
    "            if len(company_data) < min_samples:\n",
    "                print(f\"Insufficient data for {company_name} ({len(company_data)} < {min_samples}). Skipping...\")\n",
    "                return None\n",
    "\n",
    "            if company_data[self.feature_columns].isnull().any().any():\n",
    "                print(f\"Missing values in features for {company_name}. Skipping...\")\n",
    "                return None\n",
    "\n",
    "            features = company_data[self.feature_columns].values\n",
    "            target_reg = company_data['target_regression'].values\n",
    "            target_dir = company_data['target_direction'].values\n",
    "\n",
    "            # Create sequences\n",
    "            X_raw, y_reg, y_dir = self.create_sequences(features, target_reg, target_dir)\n",
    "\n",
    "            # TimeSeriesSplit\n",
    "            n_splits = min(5, len(X_raw) // 50)\n",
    "            if n_splits < 3:\n",
    "                print(f\"Insufficient data for proper time series validation for {company_name}. Skipping...\")\n",
    "                return None\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "            splits = list(tscv.split(X_raw))\n",
    "            train_idx, test_idx = splits[-1]\n",
    "\n",
    "            # Train/Val split (last 20% of train for val)\n",
    "            val_size = int(0.2 * len(train_idx))\n",
    "            final_train_idx = train_idx[:-val_size]\n",
    "            val_idx = train_idx[-val_size:]\n",
    "            \n",
    "            if self.horizon_steps > 1:\n",
    "                print(\"Adjusting for multi-step horizon...\")\n",
    "                gap = self.horizon_steps\n",
    "                if len(final_train_idx) > gap:\n",
    "                    final_train_idx = final_train_idx[:-gap]  # drop last h labels from train\n",
    "                if len(val_idx) > gap:\n",
    "                    val_idx = val_idx[gap:]  # drop last h labels from val\n",
    "\n",
    "            X_train_raw, X_val_raw, X_test_raw = X_raw[final_train_idx], X_raw[val_idx], X_raw[test_idx]\n",
    "            \n",
    "            F = X_raw.shape[-1]\n",
    "            feat_scaler = StandardScaler()\n",
    "            X_train = feat_scaler.fit_transform(X_train_raw.reshape(-1, F)).reshape(X_train_raw.shape)\n",
    "            X_val   = feat_scaler.transform(X_val_raw.reshape(-1, F)).reshape(X_val_raw.shape)\n",
    "            X_test  = feat_scaler.transform(X_test_raw.reshape(-1, F)).reshape(X_test_raw.shape)\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                y_train, y_val, y_test = y_reg[final_train_idx], y_reg[val_idx], y_reg[test_idx]\n",
    "                target_scaler = StandardScaler()\n",
    "                y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "                y_val_scaled   = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "                train_target, val_target = y_train_scaled, y_val_scaled\n",
    "            else:\n",
    "                y_train, y_val, y_test = y_dir[final_train_idx], y_dir[val_idx], y_dir[test_idx]\n",
    "                train_target, val_target = y_train, y_val\n",
    "                target_scaler = None\n",
    "\n",
    "            # Class balance note\n",
    "            if self.problem_type == 'classification':\n",
    "                class_ratio = np.mean(y_train)\n",
    "                if class_ratio < 0.1 or class_ratio > 0.9:\n",
    "                    print(f\"Severe class imbalance for {company_name} ({class_ratio:.3f}). Consider using class weights.\")\n",
    "\n",
    "            # Datasets & loaders\n",
    "            train_ds = SequenceDataset(X_train, train_target)\n",
    "            val_ds   = SequenceDataset(X_val,   val_target)\n",
    "            test_ds  = SequenceDataset(X_test,  y_test)\n",
    "\n",
    "            train_loader = DataLoader(train_ds, batch_size=32, shuffle=False,  drop_last=False, num_workers=0)\n",
    "            val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "            test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, drop_last=False, num_workers=0)\n",
    "\n",
    "            # Build model\n",
    "            model = self.build_model((self.sequence_length, len(self.feature_columns)))\n",
    "\n",
    "            # Loss functions\n",
    "            if self.problem_type == 'regression':\n",
    "                loss_fn = nn.HuberLoss(delta=1.0)\n",
    "            else:\n",
    "                # Use BCEWithLogitsLoss for numerical stability (logits input)\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            # Optimizer & scheduler\n",
    "            optimizer = Adam(model.parameters(), lr=1e-3, eps=1e-7, weight_decay=1e-5)\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-7)\n",
    "            early_stopper = EarlyStopper(patience=15, min_delta=0.0, restore_best=True)\n",
    "            scaler = torch.amp.GradScaler('cuda', enabled=self.mixed_precision)\n",
    "\n",
    "            # Training loop\n",
    "            max_epochs = 100\n",
    "            best_val = float('inf')\n",
    "            epochs_trained = 0\n",
    "            company_loss_rows = []  \n",
    "\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                train_loss = self._train_one_epoch(model, train_loader, optimizer, loss_fn, scaler)\n",
    "                val_loss = self._eval_one_epoch(model, val_loader, loss_fn)\n",
    "                scheduler.step(val_loss)\n",
    "                stop = early_stopper.step(val_loss, model)\n",
    "                epochs_trained = epoch\n",
    "                \n",
    "                row = {\n",
    "                    'company': company_name,\n",
    "                    'sector': sector,\n",
    "                    'model_type': self.model_type,\n",
    "                    'problem_type': self.problem_type,\n",
    "                    'sequence_length': self.sequence_length,\n",
    "                    'horizon_steps': self.horizon_steps,\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': float(train_loss),\n",
    "                    'val_loss': float(val_loss),\n",
    "                    'train_samples': len(X_train),\n",
    "                    'val_samples': len(X_val),\n",
    "                    'test_samples': len(X_test),\n",
    "                }\n",
    "                \n",
    "                company_loss_rows.append(row)\n",
    "                self.loss_curves.append(row)\n",
    "\n",
    "                if epoch % 10 == 0 or stop:\n",
    "                    print(f\"  Epoch {epoch:03d} - train {train_loss:.5f} | val {val_loss:.5f}\")\n",
    "\n",
    "                if stop:\n",
    "                    break\n",
    "\n",
    "            # Restore best model weights (like Keras restore_best_weights=True)\n",
    "            early_stopper.restore(model)\n",
    "\n",
    "            # Predictions\n",
    "            y_pred_raw = self._predict(model, test_loader)  # raw/regression or logits\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                y_pred_unscaled = target_scaler.inverse_transform(y_pred_raw.reshape(-1,1)).flatten() if target_scaler is not None else y_pred_raw\n",
    "                mse = mean_squared_error(y_test, y_pred_unscaled)\n",
    "                mae = mean_absolute_error(y_test, y_pred_unscaled)\n",
    "                r2  = r2_score(y_test, y_pred_unscaled)\n",
    "\n",
    "                # Directional metrics (derived)\n",
    "                y_test_dir = (y_reg[test_idx] > 0).astype(int)\n",
    "                y_pred_dir = (y_pred_unscaled > 0).astype(int)\n",
    "            else:\n",
    "                # logits -> probs via sigmoid -> threshold 0.5\n",
    "                probs = 1.0 / (1.0 + np.exp(-y_pred_raw))\n",
    "                y_pred_dir = (probs > 0.5).astype(int)\n",
    "                y_test_dir = y_test\n",
    "                mse = mae = r2 = np.nan\n",
    "\n",
    "            precision = precision_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "            recall    = recall_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "            f1        = f1_score(y_test_dir, y_pred_dir, zero_division=0)\n",
    "            mcc       = matthews_corrcoef(y_test_dir, y_pred_dir)\n",
    "            directional_accuracy = np.mean(y_test_dir == y_pred_dir)\n",
    "\n",
    "            result = {\n",
    "                'company': company_name,\n",
    "                'sector': sector,\n",
    "                'model_type': self.model_type,\n",
    "                'problem_type': self.problem_type,\n",
    "                'horizon_steps': self.horizon_steps,\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'r2': r2,\n",
    "                'mcc': mcc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'directional_accuracy': directional_accuracy,\n",
    "                'n_samples': int(X_raw.shape[0]),\n",
    "                'train_samples': int(X_train.shape[0]),\n",
    "                'val_samples': int(X_val.shape[0]),\n",
    "                'test_samples': int(X_test.shape[0]),\n",
    "                'epochs_trained': epochs_trained\n",
    "            }\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                print(f\"  Regression -> MSE: {mse:.6f}, MAE: {mae:.6f}, R²: {r2:.4f}\")\n",
    "            print(f\"  Directional -> Accuracy: {directional_accuracy:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "            # Explicit cleanup (PyTorch handles this, but keeps parity with Enrique2025)\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {company_name}: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return None\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        company_col = None\n",
    "        for col_name in ['ticker', 'company', 'symbol']:\n",
    "            if col_name in self.df.columns:\n",
    "                company_col = col_name\n",
    "                break\n",
    "        if company_col is None:\n",
    "            company_col = self.df.columns[0]\n",
    "            print(f\"Warning: Using '{company_col}' as company identifier column\")\n",
    "\n",
    "        companies = self.df[company_col].unique()\n",
    "        print(f\"Processing {len(companies)} companies with {self.model_type} model...\")\n",
    "        print(f\"Problem type: {self.problem_type}\")\n",
    "        print(f\"Sequence length: {self.sequence_length}\")\n",
    "        print(f\"Features: {self.feature_columns}\")\n",
    "\n",
    "        successful_companies = 0\n",
    "        for i, company in enumerate(companies, 1):\n",
    "            print(f\"\\n[{i}/{len(companies)}] Processing {company}...\")\n",
    "            company_data = self.df[self.df[company_col] == company].copy()\n",
    "            sector = company_data['sector'].iloc[0] if 'sector' in company_data.columns else 'Unknown'\n",
    "            result = self.process_company(company, company_data, sector)\n",
    "            if result:\n",
    "                self.results.append(result)\n",
    "                successful_companies += 1\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Pipeline completed: {successful_companies}/{len(companies)} companies processed successfully\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        if self.results:\n",
    "            self.results_df = pd.DataFrame(self.results)\n",
    "            return self.results_df\n",
    "        else:\n",
    "            print(\"No companies were processed successfully!\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def analyze_results(self):\n",
    "        if not hasattr(self, 'results_df') or self.results_df.empty:\n",
    "            print(\"No results to analyze!\")\n",
    "            return None\n",
    "\n",
    "        df = self.results_df\n",
    "        analysis = {}\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STOCK PREDICTION PIPELINE RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: {self.model_type} | Problem: {self.problem_type}\")\n",
    "        print(f\"Companies analyzed: {len(df)}\")\n",
    "        print(f\"Average samples per company: {df['n_samples'].mean():.0f}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"OVERALL PERFORMANCE\")\n",
    "        print(\"=\"*50)\n",
    "        if self.problem_type == 'regression':\n",
    "            print(f\"Mean Squared Error:     {df['mse'].mean():.6f} (±{df['mse'].std():.6f})\")\n",
    "            print(f\"Mean Absolute Error:    {df['mae'].mean():.6f} (±{df['mae'].std():.6f})\")\n",
    "            print(f\"R² Score:              {df['r2'].mean():.4f} (±{df['r2'].std():.4f})\")\n",
    "\n",
    "        print(f\"Directional Accuracy:   {df['directional_accuracy'].mean():.4f} (±{df['directional_accuracy'].std():.4f})\")\n",
    "        print(f\"Matthews Correlation:   {df['mcc'].mean():.4f} (±{df['mcc'].std():.4f})\")\n",
    "        print(f\"F1 Score:              {df['f1'].mean():.4f} (±{df['f1'].std():.4f})\")\n",
    "        print(f\"Precision:             {df['precision'].mean():.4f} (±{df['precision'].std():.4f})\")\n",
    "        print(f\"Recall:                {df['recall'].mean():.4f} (±{df['recall'].std():.4f})\")\n",
    "\n",
    "        if 'sector' in df.columns and df['sector'].nunique() > 1:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"PERFORMANCE BY SECTOR\")\n",
    "            print(\"=\"*50)\n",
    "            sector_stats = df.groupby('sector').agg({\n",
    "                'directional_accuracy': ['mean', 'std', 'count'],\n",
    "                'mcc': ['mean', 'std'],\n",
    "                'r2': 'mean' if self.problem_type == 'regression' else lambda x: np.nan,\n",
    "                'mae': 'mean' if self.problem_type == 'regression' else lambda x: np.nan\n",
    "            }).round(4)\n",
    "            sector_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in sector_stats.columns]\n",
    "            sector_stats = sector_stats.sort_values('directional_accuracy_mean', ascending=False)\n",
    "            for sector, row in sector_stats.iterrows():\n",
    "                print(f\"{sector:<20} | Acc: {row['directional_accuracy_mean']:.3f}±{row['directional_accuracy_std']:.3f} | \"\n",
    "                      f\"MCC: {row['mcc_mean']:.3f} | Companies: {int(row['directional_accuracy_count'])}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TOP 10 PERFORMERS (by Directional Accuracy)\")\n",
    "        print(\"=\"*50)\n",
    "        top_performers = df.nlargest(10, 'directional_accuracy')\n",
    "        for _, row in top_performers.iterrows():\n",
    "            print(f\"{row['company']:<20} | {row['sector']:<15} | \"\n",
    "                  f\"Acc: {row['directional_accuracy']:.3f} | MCC: {row['mcc']:.3f}\")\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def save_results(self, results, output_dir='results/benchmarking'):\n",
    "        if results is not None and not results.empty:\n",
    "            model_name = self.model_type\n",
    "\n",
    "            if self.problem_type == 'regression':\n",
    "                out_dir = os.path.join(output_dir, 'regression')\n",
    "            else:\n",
    "                out_dir = os.path.join(output_dir, 'classification')\n",
    "\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            output_path = os.path.join(out_dir, f\"{model_name}.csv\")\n",
    "\n",
    "            results.to_csv(output_path, index=False)\n",
    "            print(f\"Results saved to {output_path}\")\n",
    "        else:\n",
    "            print(\"No results to save.\")\n",
    "            \n",
    "    def get_loss_curves_df(self):\n",
    "        if not self.loss_curves:\n",
    "            print(\"No loss curves logged yet.\")\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(self.loss_curves)\n",
    "\n",
    "    def save_loss_curves(self, out_path='results/benchmarking/'):\n",
    "        df = self.get_loss_curves_df()\n",
    "        if df.empty:\n",
    "            print(\"No loss curves to save.\")\n",
    "            return\n",
    "        if self.problem_type == 'regression':\n",
    "            out_path = os.path.join(out_path, 'regression', f\"{self.model_type}_loss_curves.csv\")\n",
    "        else:\n",
    "            out_path = os.path.join(out_path, 'classification', f\"{self.model_type}_loss_curves.csv\")\n",
    "            \n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        \n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\"Loss curves saved to {out_path}\")\n",
    "\n",
    "    def get_feature_importance_analysis(self):\n",
    "        print(\"Feature importance analysis not implemented yet.\")\n",
    "        print(\"Consider implementing SHAP values or permutation importance for better insights.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlCKxsDxHHFe",
    "outputId": "36aa45f2-ea60-4af0-f4ea-d3804481245f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/mdk91y8925ncdd32prjkcl_c0000gn/T/ipykernel_90145/1242874406.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  master_df[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of master_df before dropping NaNs: (108592, 21)\n",
      "Shape of master_df after dropping NaNs: (108592, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/mdk91y8925ncdd32prjkcl_c0000gn/T/ipykernel_90145/1242874406.py:95: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  master_df = master_df.groupby('ticker').apply(apply_ta_indicators)\n"
     ]
    }
   ],
   "source": [
    "companies = pd.read_parquet('stocknet-dataset/stock_table.parquet')\n",
    "tweets = pd.read_parquet('stocknet-dataset/stock_tweets_withsentiment_withemotion_withstance_nomerge.parquet')\n",
    "stocks = pd.read_parquet('stocknet-dataset/stock_prices.parquet')\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "companies.columns = [x.lower() for x in companies.columns]\n",
    "tweets.columns = [x.lower() for x in tweets.columns]\n",
    "stocks.columns = [x.lower() for x in stocks.columns]\n",
    "\n",
    "tweets['stance_positive'] = (tweets['stance_label'] == 'Positive').astype(int)\n",
    "tweets['stance_negative'] = (tweets['stance_label'] == 'Negative').astype(int)\n",
    "\n",
    "tweets_merged = tweets.groupby(['date', 'ticker'], as_index=False).agg({\n",
    "    'text': lambda x: ' '.join(x),\n",
    "    'sentiment': lambda x: x.mean(),\n",
    "    'emotion_anger': 'sum',\n",
    "    'emotion_disgust': 'sum',\n",
    "    'emotion_fear': 'sum',\n",
    "    'emotion_joy': 'sum',\n",
    "    'emotion_neutral': 'sum',\n",
    "    'emotion_sadness': 'sum',\n",
    "    'emotion_surprize': 'sum',\n",
    "    'stance_positive': 'sum',\n",
    "    'stance_negative': 'sum'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweets_merged['date'] = pd.to_datetime(tweets_merged['date'])\n",
    "stocks['date'] = pd.to_datetime(stocks['date'])\n",
    "\n",
    "\n",
    "\n",
    "master_df = pd.merge(\n",
    "    stocks,\n",
    "    tweets_merged,\n",
    "    on=[\"date\", \"ticker\"],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing tweet features with 0\n",
    "tweet_feature_cols = ['sentiment', 'emotion_anger', 'emotion_disgust', 'emotion_fear', 'emotion_joy', 'emotion_neutral', 'emotion_sadness', 'emotion_surprize', 'stance_positive', 'stance_negative']\n",
    "for col in tweet_feature_cols:\n",
    "    if col in master_df.columns:\n",
    "        master_df[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "companies = companies.rename(columns={'symbol': 'ticker'})\n",
    "\n",
    "master_df = pd.merge(master_df, companies[['ticker', 'sector', 'company']], on='ticker', how='left')\n",
    "\n",
    "\n",
    "feature_cols = ['open','high','low','volume']\n",
    "\n",
    "master_df = master_df.rename(columns={'close': 'close_price', 'company': 'company_name'})\n",
    "\n",
    "\n",
    "print(f\"Shape of master_df before dropping NaNs: {master_df.shape}\")\n",
    "print(f\"Shape of master_df after dropping NaNs: {master_df.shape}\")\n",
    "\n",
    "master_df.rename(columns={'close_price': 'close'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "master_df.sort_values(by=['ticker', 'date'], inplace=True)\n",
    "\n",
    "\n",
    "def apply_ta_indicators(df_group):\n",
    "    df_group.set_index(pd.DatetimeIndex(df_group['date']), inplace=True)\n",
    "    df_group.ta.ema(length=12, append=True)\n",
    "    df_group.ta.ema(length=26, append=True)\n",
    "    df_group.ta.ema(length=50, append=True)\n",
    "\n",
    "    df_group.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "\n",
    "\n",
    "    df_group.ta.rsi(length=14, append=True)\n",
    "    df_group.ta.stochrsi(length=14, append=True)\n",
    "\n",
    "    df_group.ta.atr(length=14, append=True)\n",
    "\n",
    "    bb = ta.bbands(df_group['close'], length=20, std=2)\n",
    "    df_group['BB_upper'] = bb['BBU_20_2.0']\n",
    "    df_group['BB_middle'] = bb['BBM_20_2.0']\n",
    "    df_group['BB_lower'] = bb['BBL_20_2.0']\n",
    "\n",
    "    df_group.ta.obv(append=True)\n",
    "    return df_group.reset_index(drop=True)\n",
    "\n",
    "master_df = master_df.groupby('ticker').apply(apply_ta_indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "12_qUQQGGxKm",
    "outputId": "41eb4670-181a-465b-f565-290ccae2d22b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>MACDh_12_26_9</th>\n",
       "      <th>MACDs_12_26_9</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>STOCHRSIk_14_14_3_3</th>\n",
       "      <th>STOCHRSId_14_14_3_3</th>\n",
       "      <th>ATRr_14</th>\n",
       "      <th>BB_upper</th>\n",
       "      <th>BB_middle</th>\n",
       "      <th>BB_lower</th>\n",
       "      <th>OBV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-11-14</td>\n",
       "      <td>77.928574</td>\n",
       "      <td>78.207146</td>\n",
       "      <td>76.597145</td>\n",
       "      <td>76.697144</td>\n",
       "      <td>69.613815</td>\n",
       "      <td>119292600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538077</td>\n",
       "      <td>-3.703588</td>\n",
       "      <td>25.771012</td>\n",
       "      <td>21.054629</td>\n",
       "      <td>19.582354</td>\n",
       "      <td>2.377852</td>\n",
       "      <td>94.648550</td>\n",
       "      <td>84.401357</td>\n",
       "      <td>74.154164</td>\n",
       "      <td>-1.014356e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-11-15</td>\n",
       "      <td>76.790001</td>\n",
       "      <td>77.071426</td>\n",
       "      <td>74.660004</td>\n",
       "      <td>75.088570</td>\n",
       "      <td>68.153778</td>\n",
       "      <td>197477700.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537725</td>\n",
       "      <td>-3.838019</td>\n",
       "      <td>23.573491</td>\n",
       "      <td>15.792949</td>\n",
       "      <td>19.993462</td>\n",
       "      <td>2.380310</td>\n",
       "      <td>93.761634</td>\n",
       "      <td>83.514428</td>\n",
       "      <td>73.267223</td>\n",
       "      <td>-1.211834e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>75.028572</td>\n",
       "      <td>75.714287</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>75.382858</td>\n",
       "      <td>68.420891</td>\n",
       "      <td>316723400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455544</td>\n",
       "      <td>-3.951905</td>\n",
       "      <td>24.836267</td>\n",
       "      <td>12.243346</td>\n",
       "      <td>16.363641</td>\n",
       "      <td>2.459547</td>\n",
       "      <td>92.716200</td>\n",
       "      <td>82.679214</td>\n",
       "      <td>72.642228</td>\n",
       "      <td>-8.951103e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-19</td>\n",
       "      <td>77.244286</td>\n",
       "      <td>81.071426</td>\n",
       "      <td>77.125717</td>\n",
       "      <td>80.818573</td>\n",
       "      <td>73.354591</td>\n",
       "      <td>205829400.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>-3.951213</td>\n",
       "      <td>43.429047</td>\n",
       "      <td>39.692073</td>\n",
       "      <td>22.576123</td>\n",
       "      <td>2.695187</td>\n",
       "      <td>91.617665</td>\n",
       "      <td>82.201285</td>\n",
       "      <td>72.784906</td>\n",
       "      <td>-6.892809e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-11-20</td>\n",
       "      <td>81.701431</td>\n",
       "      <td>81.707146</td>\n",
       "      <td>79.225716</td>\n",
       "      <td>80.129997</td>\n",
       "      <td>72.729614</td>\n",
       "      <td>160688500.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281964</td>\n",
       "      <td>-3.880722</td>\n",
       "      <td>42.011353</td>\n",
       "      <td>69.373201</td>\n",
       "      <td>40.436207</td>\n",
       "      <td>2.679612</td>\n",
       "      <td>91.027780</td>\n",
       "      <td>81.851785</td>\n",
       "      <td>72.675790</td>\n",
       "      <td>-8.499694e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104215</th>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>76.940002</td>\n",
       "      <td>76.260002</td>\n",
       "      <td>76.470001</td>\n",
       "      <td>76.470001</td>\n",
       "      <td>8229700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107548</td>\n",
       "      <td>-0.972858</td>\n",
       "      <td>31.975492</td>\n",
       "      <td>35.117121</td>\n",
       "      <td>31.775404</td>\n",
       "      <td>0.786087</td>\n",
       "      <td>81.525829</td>\n",
       "      <td>78.243500</td>\n",
       "      <td>74.961171</td>\n",
       "      <td>-2.688251e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104216</th>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>76.209999</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.080002</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>7060400.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069077</td>\n",
       "      <td>-0.990127</td>\n",
       "      <td>31.851847</td>\n",
       "      <td>48.597552</td>\n",
       "      <td>38.712818</td>\n",
       "      <td>0.759224</td>\n",
       "      <td>81.303475</td>\n",
       "      <td>78.057500</td>\n",
       "      <td>74.811525</td>\n",
       "      <td>-2.758855e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104217</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>76.239998</td>\n",
       "      <td>76.449997</td>\n",
       "      <td>76.059998</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>8218000.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054652</td>\n",
       "      <td>-1.003790</td>\n",
       "      <td>29.688704</td>\n",
       "      <td>55.025431</td>\n",
       "      <td>46.246701</td>\n",
       "      <td>0.732850</td>\n",
       "      <td>80.964170</td>\n",
       "      <td>77.832500</td>\n",
       "      <td>74.700830</td>\n",
       "      <td>-2.841035e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104218</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>76.269997</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>76.050003</td>\n",
       "      <td>76.330002</td>\n",
       "      <td>76.330002</td>\n",
       "      <td>15641700.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018917</td>\n",
       "      <td>-1.008519</td>\n",
       "      <td>32.913052</td>\n",
       "      <td>73.940933</td>\n",
       "      <td>59.187972</td>\n",
       "      <td>0.711932</td>\n",
       "      <td>80.569554</td>\n",
       "      <td>77.624500</td>\n",
       "      <td>74.679446</td>\n",
       "      <td>-2.684618e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104219</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>76.849998</td>\n",
       "      <td>76.320000</td>\n",
       "      <td>76.570000</td>\n",
       "      <td>76.570000</td>\n",
       "      <td>7340800.0</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028984</td>\n",
       "      <td>-1.001273</td>\n",
       "      <td>36.200731</td>\n",
       "      <td>85.986580</td>\n",
       "      <td>71.650981</td>\n",
       "      <td>0.698937</td>\n",
       "      <td>80.167620</td>\n",
       "      <td>77.442500</td>\n",
       "      <td>74.717380</td>\n",
       "      <td>-2.611210e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104220 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date       open       high        low      close  adj close  \\\n",
       "0      2012-11-14  77.928574  78.207146  76.597145  76.697144  69.613815   \n",
       "1      2012-11-15  76.790001  77.071426  74.660004  75.088570  68.153778   \n",
       "2      2012-11-16  75.028572  75.714287  72.250000  75.382858  68.420891   \n",
       "3      2012-11-19  77.244286  81.071426  77.125717  80.818573  73.354591   \n",
       "4      2012-11-20  81.701431  81.707146  79.225716  80.129997  72.729614   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "104215 2017-08-28  76.900002  76.940002  76.260002  76.470001  76.470001   \n",
       "104216 2017-08-29  76.209999  76.489998  76.080002  76.449997  76.449997   \n",
       "104217 2017-08-30  76.239998  76.449997  76.059998  76.099998  76.099998   \n",
       "104218 2017-08-31  76.269997  76.489998  76.050003  76.330002  76.330002   \n",
       "104219 2017-09-01  76.370003  76.849998  76.320000  76.570000  76.570000   \n",
       "\n",
       "             volume ticker text  sentiment  ...  MACDh_12_26_9  MACDs_12_26_9  \\\n",
       "0       119292600.0   AAPL  NaN        0.0  ...      -0.538077      -3.703588   \n",
       "1       197477700.0   AAPL  NaN        0.0  ...      -0.537725      -3.838019   \n",
       "2       316723400.0   AAPL  NaN        0.0  ...      -0.455544      -3.951905   \n",
       "3       205829400.0   AAPL  NaN        0.0  ...       0.002769      -3.951213   \n",
       "4       160688500.0   AAPL  NaN        0.0  ...       0.281964      -3.880722   \n",
       "...             ...    ...  ...        ...  ...            ...            ...   \n",
       "104215    8229700.0    XOM  NaN        0.0  ...      -0.107548      -0.972858   \n",
       "104216    7060400.0    XOM  NaN        0.0  ...      -0.069077      -0.990127   \n",
       "104217    8218000.0    XOM  NaN        0.0  ...      -0.054652      -1.003790   \n",
       "104218   15641700.0    XOM  NaN        0.0  ...      -0.018917      -1.008519   \n",
       "104219    7340800.0    XOM  NaN        0.0  ...       0.028984      -1.001273   \n",
       "\n",
       "           RSI_14  STOCHRSIk_14_14_3_3  STOCHRSId_14_14_3_3   ATRr_14  \\\n",
       "0       25.771012            21.054629            19.582354  2.377852   \n",
       "1       23.573491            15.792949            19.993462  2.380310   \n",
       "2       24.836267            12.243346            16.363641  2.459547   \n",
       "3       43.429047            39.692073            22.576123  2.695187   \n",
       "4       42.011353            69.373201            40.436207  2.679612   \n",
       "...           ...                  ...                  ...       ...   \n",
       "104215  31.975492            35.117121            31.775404  0.786087   \n",
       "104216  31.851847            48.597552            38.712818  0.759224   \n",
       "104217  29.688704            55.025431            46.246701  0.732850   \n",
       "104218  32.913052            73.940933            59.187972  0.711932   \n",
       "104219  36.200731            85.986580            71.650981  0.698937   \n",
       "\n",
       "         BB_upper  BB_middle   BB_lower           OBV  \n",
       "0       94.648550  84.401357  74.154164 -1.014356e+09  \n",
       "1       93.761634  83.514428  73.267223 -1.211834e+09  \n",
       "2       92.716200  82.679214  72.642228 -8.951103e+08  \n",
       "3       91.617665  82.201285  72.784906 -6.892809e+08  \n",
       "4       91.027780  81.851785  72.675790 -8.499694e+08  \n",
       "...           ...        ...        ...           ...  \n",
       "104215  81.525829  78.243500  74.961171 -2.688251e+08  \n",
       "104216  81.303475  78.057500  74.811525 -2.758855e+08  \n",
       "104217  80.964170  77.832500  74.700830 -2.841035e+08  \n",
       "104218  80.569554  77.624500  74.679446 -2.684618e+08  \n",
       "104219  80.167620  77.442500  74.717380 -2.611210e+08  \n",
       "\n",
       "[104220 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_check = ['EMA_12', 'EMA_26','EMA_50','MACD_12_26_9','MACDh_12_26_9','MACDs_12_26_9','RSI_14','ATRr_14','STOCHRSIk_14_14_3_3','STOCHRSId_14_14_3_3','ATRr_14','BB_upper','BB_middle','BB_lower','OBV']\n",
    "master_df = master_df.dropna(subset=columns_to_check)\n",
    "\n",
    "\n",
    "master_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4D6EfvoYDrGY",
    "outputId": "a09ae550-5787-45c6-81aa-4bf74311f09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'open', 'high', 'low', 'close', 'adj close', 'volume', 'ticker',\n",
      "       'text', 'sentiment', 'emotion_anger', 'emotion_disgust', 'emotion_fear',\n",
      "       'emotion_joy', 'emotion_neutral', 'emotion_sadness', 'emotion_surprize',\n",
      "       'stance_positive', 'stance_negative', 'sector', 'company_name',\n",
      "       'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9',\n",
      "       'MACDs_12_26_9', 'RSI_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3',\n",
      "       'ATRr_14', 'BB_upper', 'BB_middle', 'BB_lower', 'OBV'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(master_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ytoycwxUDtS3"
   },
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'open', 'high', 'low', 'close', 'volume'\n",
    "    # 'stance_positive', 'stance_negative',\n",
    "    # 'sentiment'\n",
    "]\n",
    "\n",
    "new_indicator_columns = [\n",
    "    'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',\n",
    "    'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3',\n",
    "    'BB_upper', 'BB_middle', 'BB_lower', 'OBV'\n",
    "]\n",
    "feature_columns.extend(new_indicator_columns)\n",
    "\n",
    "sequence_length=12\n",
    "\n",
    "\n",
    "\n",
    "all_pipelines = {}\n",
    "all_results_dfs = {}\n",
    "all_analyses = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RBV6cWZNDuCW",
    "outputId": "07c614de-47f3-4115-a4f2-0a8e11405f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "  RUNNING PIPELINE FOR: BiGRU\n",
      "=========================\n",
      "\n",
      "Pipeline initialized for a 'regression' problem with horizon 1 steps. Device: cpu\n",
      "Processing 88 companies with BiGRU model...\n",
      "Problem type: regression\n",
      "Sequence length: 12\n",
      "Features: ['open', 'high', 'low', 'close', 'volume', 'EMA_12', 'EMA_26', 'EMA_50', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'RSI_14', 'ATRr_14', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3', 'BB_upper', 'BB_middle', 'BB_lower', 'OBV']\n",
      "\n",
      "[1/88] Processing AAPL...\n",
      "\n",
      "Processing AAPL (Consumer Goods)...\n",
      "  Epoch 010 - train 0.31381 | val 0.67597\n",
      "  Epoch 017 - train 0.29533 | val 0.66951\n",
      "  Regression -> MSE: 0.000253, MAE: 0.012697, R²: -0.0320\n",
      "  Directional -> Accuracy: 0.4744, MCC: 0.0047, F1: 0.5684\n",
      "\n",
      "[2/88] Processing ABB...\n",
      "\n",
      "Processing ABB (Industrial Goods)...\n",
      "Insufficient data for ABB (62 < 163). Skipping...\n",
      "\n",
      "[3/88] Processing ABBV...\n",
      "\n",
      "Processing ABBV (Healthcare)...\n",
      "  Epoch 010 - train 0.31834 | val 0.37897\n",
      "  Epoch 020 - train 0.29989 | val 0.42078\n",
      "  Regression -> MSE: 0.000668, MAE: 0.018892, R²: 0.0342\n",
      "  Directional -> Accuracy: 0.5532, MCC: 0.0485, F1: 0.6769\n",
      "\n",
      "[4/88] Processing AEP...\n",
      "\n",
      "Processing AEP (Utilities)...\n",
      "Insufficient data for AEP (109 < 163). Skipping...\n",
      "\n",
      "[5/88] Processing AGFS...\n",
      "\n",
      "Processing AGFS (Conglomerates)...\n",
      "Insufficient data for AGFS (2 < 163). Skipping...\n",
      "\n",
      "[6/88] Processing AMGN...\n",
      "\n",
      "Processing AMGN (Healthcare)...\n",
      "  Epoch 010 - train 0.37801 | val 0.83840\n",
      "  Epoch 018 - train 0.34511 | val 0.90207\n",
      "  Regression -> MSE: 0.000347, MAE: 0.015233, R²: -0.0052\n",
      "  Directional -> Accuracy: 0.5417, MCC: 0.0000, F1: 0.7027\n",
      "\n",
      "[7/88] Processing AMZN...\n",
      "\n",
      "Processing AMZN (Services)...\n",
      "  Epoch 010 - train 0.28221 | val 0.79060\n",
      "  Epoch 016 - train 0.26604 | val 1.02304\n",
      "  Regression -> MSE: 0.000424, MAE: 0.016105, R²: -0.2287\n",
      "  Directional -> Accuracy: 0.4605, MCC: 0.0000, F1: 0.0000\n",
      "\n",
      "[8/88] Processing BA...\n",
      "\n",
      "Processing BA (Industrial Goods)...\n",
      "  Epoch 010 - train 0.33419 | val 0.36642\n",
      "  Epoch 019 - train 0.29744 | val 0.37661\n",
      "  Regression -> MSE: 0.000188, MAE: 0.009059, R²: 0.0615\n",
      "  Directional -> Accuracy: 0.7885, MCC: 0.5745, F1: 0.8136\n",
      "\n",
      "[9/88] Processing BABA...\n",
      "\n",
      "Processing BABA (Services)...\n",
      "  Epoch 010 - train 0.30725 | val 0.90613\n",
      "  Epoch 019 - train 0.29992 | val 0.88004\n",
      "  Regression -> MSE: 0.000580, MAE: 0.018480, R²: -0.0143\n",
      "  Directional -> Accuracy: 0.4694, MCC: -0.0578, F1: 0.3810\n",
      "\n",
      "[10/88] Processing BAC...\n",
      "\n",
      "Processing BAC (Financial)...\n",
      "  Epoch 010 - train 0.33606 | val 0.98003\n",
      "  Epoch 016 - train 0.31807 | val 0.84863\n",
      "  Regression -> MSE: 0.000248, MAE: 0.011950, R²: -0.0076\n",
      "  Directional -> Accuracy: 0.6000, MCC: 0.0000, F1: 0.0000\n",
      "\n",
      "[11/88] Processing BBL...\n",
      "\n",
      "Processing BBL (Basic Matierials)...\n",
      "Insufficient data for BBL (69 < 163). Skipping...\n",
      "\n",
      "[12/88] Processing BCH...\n",
      "\n",
      "Processing BCH (Financial)...\n",
      "Insufficient data for BCH (5 < 163). Skipping...\n",
      "\n",
      "[13/88] Processing BHP...\n",
      "\n",
      "Processing BHP (Basic Matierials)...\n",
      "  Epoch 010 - train 0.29394 | val 1.19658\n",
      "  Epoch 016 - train 0.29121 | val 1.57524\n",
      "  Regression -> MSE: 0.000639, MAE: 0.019862, R²: -0.0936\n",
      "  Directional -> Accuracy: 0.4222, MCC: 0.0000, F1: 0.5938\n",
      "\n",
      "[14/88] Processing BP...\n",
      "\n",
      "Processing BP (Basic Matierials)...\n",
      "  Epoch 010 - train 0.32379 | val 0.82871\n",
      "  Epoch 016 - train 0.28245 | val 0.96741\n",
      "  Regression -> MSE: 0.000340, MAE: 0.015373, R²: -0.0051\n",
      "  Directional -> Accuracy: 0.3810, MCC: 0.0000, F1: 0.5517\n",
      "\n",
      "[15/88] Processing BRK-A...\n",
      "\n",
      "Processing BRK-A (Financial)...\n",
      "Insufficient data for BRK-A (13 < 163). Skipping...\n",
      "\n",
      "[16/88] Processing BSAC...\n",
      "\n",
      "Processing BSAC (Financial)...\n",
      "Insufficient data for BSAC (11 < 163). Skipping...\n",
      "\n",
      "[17/88] Processing BUD...\n",
      "\n",
      "Processing BUD (Consumer Goods)...\n",
      "Insufficient data for BUD (96 < 163). Skipping...\n",
      "\n",
      "[18/88] Processing C...\n",
      "\n",
      "Processing C (Financial)...\n",
      "  Epoch 010 - train 0.31989 | val 0.86018\n",
      "  Epoch 016 - train 0.30227 | val 1.18279\n",
      "  Regression -> MSE: 0.000237, MAE: 0.011791, R²: 0.0048\n",
      "  Directional -> Accuracy: 0.4366, MCC: 0.0000, F1: 0.6078\n",
      "\n",
      "[19/88] Processing CAT...\n",
      "\n",
      "Processing CAT (Industrial Goods)...\n",
      "  Epoch 010 - train 0.29889 | val 1.51029\n",
      "  Epoch 016 - train 0.30021 | val 1.63389\n",
      "  Regression -> MSE: 0.000309, MAE: 0.013872, R²: -0.0098\n",
      "  Directional -> Accuracy: 0.4706, MCC: 0.0000, F1: 0.6400\n",
      "\n",
      "[20/88] Processing CELG...\n",
      "\n",
      "Processing CELG (Healthcare)...\n",
      "  Epoch 010 - train 0.35853 | val 0.58029\n",
      "  Epoch 017 - train 0.33398 | val 0.69348\n",
      "  Regression -> MSE: 0.000655, MAE: 0.019132, R²: 0.0211\n",
      "  Directional -> Accuracy: 0.6441, MCC: 0.2942, F1: 0.6182\n",
      "\n",
      "[21/88] Processing CHL...\n",
      "\n",
      "Processing CHL (Technology)...\n",
      "Insufficient data for CHL (96 < 163). Skipping...\n",
      "\n",
      "[22/88] Processing CHTR...\n",
      "\n",
      "Processing CHTR (Services)...\n",
      "Insufficient data for CHTR (137 < 163). Skipping...\n",
      "\n",
      "[23/88] Processing CMCSA...\n",
      "\n",
      "Processing CMCSA (Services)...\n",
      "  Epoch 010 - train 0.31670 | val 0.57563\n",
      "  Epoch 017 - train 0.30633 | val 0.86701\n",
      "  Regression -> MSE: 0.000150, MAE: 0.009502, R²: -0.0027\n",
      "  Directional -> Accuracy: 0.5455, MCC: 0.1000, F1: 0.5455\n",
      "\n",
      "[24/88] Processing CODI...\n",
      "\n",
      "Processing CODI (Conglomerates)...\n",
      "Insufficient data for CODI (29 < 163). Skipping...\n",
      "\n",
      "[25/88] Processing CSCO...\n",
      "\n",
      "Processing CSCO (Technology)...\n",
      "  Epoch 010 - train 0.29715 | val 0.70621\n",
      "  Epoch 017 - train 0.26876 | val 0.74091\n",
      "  Regression -> MSE: 0.000222, MAE: 0.010670, R²: 0.0139\n",
      "  Directional -> Accuracy: 0.5484, MCC: 0.1130, F1: 0.6410\n",
      "\n",
      "[26/88] Processing CVX...\n",
      "\n",
      "Processing CVX (Basic Matierials)...\n",
      "  Epoch 010 - train 0.31632 | val 1.77077\n",
      "  Epoch 017 - train 0.28300 | val 1.79209\n",
      "  Regression -> MSE: 0.000374, MAE: 0.016516, R²: 0.0039\n",
      "  Directional -> Accuracy: 0.4717, MCC: 0.0000, F1: 0.6410\n",
      "\n",
      "[27/88] Processing D...\n",
      "\n",
      "Processing D (Utilities)...\n",
      "  Epoch 010 - train 0.33930 | val 0.43788\n",
      "  Epoch 017 - train 0.31348 | val 0.57760\n",
      "  Regression -> MSE: 0.000102, MAE: 0.007808, R²: 0.0377\n",
      "  Directional -> Accuracy: 0.5493, MCC: 0.0874, F1: 0.6000\n",
      "\n",
      "[28/88] Processing DHR...\n",
      "\n",
      "Processing DHR (Industrial Goods)...\n",
      "Insufficient data for DHR (91 < 163). Skipping...\n",
      "\n",
      "[29/88] Processing DIS...\n",
      "\n",
      "Processing DIS (Services)...\n",
      "  Epoch 010 - train 0.32747 | val 1.10397\n",
      "  Epoch 017 - train 0.31145 | val 1.41747\n",
      "  Regression -> MSE: 0.000159, MAE: 0.010034, R²: 0.0099\n",
      "  Directional -> Accuracy: 0.6000, MCC: 0.2691, F1: 0.6000\n",
      "\n",
      "[30/88] Processing DUK...\n",
      "\n",
      "Processing DUK (Utilities)...\n",
      "Insufficient data for DUK (117 < 163). Skipping...\n",
      "\n",
      "[31/88] Processing EXC...\n",
      "\n",
      "Processing EXC (Utilities)...\n",
      "Insufficient data for EXC (136 < 163). Skipping...\n",
      "\n",
      "[32/88] Processing FB...\n",
      "\n",
      "Processing FB (Technology)...\n",
      "  Epoch 010 - train 0.33127 | val 0.58723\n",
      "  Epoch 016 - train 0.30135 | val 0.74472\n",
      "  Regression -> MSE: 0.000269, MAE: 0.012800, R²: -0.0034\n",
      "  Directional -> Accuracy: 0.6104, MCC: 0.0000, F1: 0.7581\n",
      "\n",
      "[33/88] Processing GD...\n",
      "\n",
      "Processing GD (Industrial Goods)...\n",
      "Insufficient data for GD (153 < 163). Skipping...\n",
      "\n",
      "[34/88] Processing GE...\n",
      "\n",
      "Processing GE (Industrial Goods)...\n",
      "  Epoch 010 - train 0.29312 | val 0.61186\n",
      "  Epoch 020 - train 0.26719 | val 0.63076\n",
      "  Epoch 026 - train 0.26001 | val 0.64329\n",
      "  Regression -> MSE: 0.000207, MAE: 0.010839, R²: -0.4056\n",
      "  Directional -> Accuracy: 0.4242, MCC: 0.1032, F1: 0.0500\n",
      "\n",
      "[35/88] Processing GMRE...\n",
      "\n",
      "Processing GMRE (Conglomerates)...\n",
      "Insufficient data for GMRE (0 < 163). Skipping...\n",
      "\n",
      "[36/88] Processing GOOG...\n",
      "\n",
      "Processing GOOG (Technology)...\n",
      "  Epoch 010 - train 0.33399 | val 0.69709\n",
      "  Epoch 020 - train 0.29973 | val 0.72941\n",
      "  Regression -> MSE: 0.000309, MAE: 0.013043, R²: -0.3622\n",
      "  Directional -> Accuracy: 0.4605, MCC: 0.0000, F1: 0.0000\n",
      "\n",
      "[37/88] Processing HD...\n",
      "\n",
      "Processing HD (Services)...\n",
      "  Epoch 010 - train 0.31392 | val 0.60330\n",
      "  Epoch 020 - train 0.28321 | val 1.00603\n",
      "  Epoch 029 - train 0.26365 | val 0.57977\n",
      "  Regression -> MSE: 0.000378, MAE: 0.016058, R²: -1.5458\n",
      "  Directional -> Accuracy: 0.3673, MCC: 0.0000, F1: 0.0000\n",
      "\n",
      "[38/88] Processing HON...\n",
      "\n",
      "Processing HON (Industrial Goods)...\n",
      "Insufficient data for HON (142 < 163). Skipping...\n",
      "\n",
      "[39/88] Processing HRG...\n",
      "\n",
      "Processing HRG (Conglomerates)...\n",
      "Insufficient data for HRG (30 < 163). Skipping...\n",
      "\n",
      "[40/88] Processing HSBC...\n",
      "\n",
      "Processing HSBC (Financial)...\n",
      "Insufficient data for HSBC (71 < 163). Skipping...\n",
      "\n",
      "[41/88] Processing IEP...\n",
      "\n",
      "Processing IEP (Conglomerates)...\n",
      "Insufficient data for IEP (58 < 163). Skipping...\n",
      "\n",
      "[42/88] Processing INTC...\n",
      "\n",
      "Processing INTC (Technology)...\n",
      "  Epoch 010 - train 0.31044 | val 0.56162\n",
      "  Epoch 016 - train 0.31085 | val 0.53811\n",
      "  Regression -> MSE: 0.000196, MAE: 0.010612, R²: -0.1050\n",
      "  Directional -> Accuracy: 0.4925, MCC: -0.0026, F1: 0.0556\n",
      "\n",
      "[43/88] Processing JNJ...\n",
      "\n",
      "Processing JNJ (Healthcare)...\n",
      "  Epoch 010 - train 0.34660 | val 0.86212\n",
      "  Epoch 016 - train 0.33059 | val 0.98922\n",
      "  Regression -> MSE: 0.000083, MAE: 0.006887, R²: 0.0133\n",
      "  Directional -> Accuracy: 0.5932, MCC: 0.1861, F1: 0.6250\n",
      "\n",
      "[44/88] Processing JPM...\n",
      "\n",
      "Processing JPM (Financial)...\n",
      "  Epoch 010 - train 0.32026 | val 1.92182\n",
      "  Epoch 016 - train 0.30796 | val 1.37394\n",
      "  Regression -> MSE: 0.000220, MAE: 0.011211, R²: -0.0074\n",
      "  Directional -> Accuracy: 0.5522, MCC: 0.0946, F1: 0.4000\n",
      "\n",
      "[45/88] Processing KO...\n",
      "\n",
      "Processing KO (Consumer Goods)...\n",
      "  Epoch 010 - train 0.28532 | val 0.30508\n",
      "  Epoch 019 - train 0.25267 | val 0.29016\n",
      "  Regression -> MSE: 0.000101, MAE: 0.008268, R²: -0.1051\n",
      "  Directional -> Accuracy: 0.4737, MCC: 0.0228, F1: 0.3182\n",
      "\n",
      "[46/88] Processing LMT...\n",
      "\n",
      "Processing LMT (Industrial Goods)...\n",
      "  Epoch 010 - train 0.33416 | val 0.27823\n",
      "  Epoch 016 - train 0.32204 | val 0.45087\n",
      "  Regression -> MSE: 0.000130, MAE: 0.009442, R²: -0.0379\n",
      "  Directional -> Accuracy: 0.4167, MCC: 0.0000, F1: 0.5882\n",
      "\n",
      "[47/88] Processing MA...\n",
      "\n",
      "Processing MA (Financial)...\n",
      "  Epoch 010 - train 0.34634 | val 0.54518\n",
      "  Epoch 016 - train 0.32214 | val 0.59622\n",
      "  Regression -> MSE: 0.000263, MAE: 0.013497, R²: -0.1042\n",
      "  Directional -> Accuracy: 0.4091, MCC: 0.0000, F1: 0.0000\n",
      "\n",
      "[48/88] Processing MCD...\n",
      "\n",
      "Processing MCD (Services)...\n",
      "  Epoch 010 - train 0.32352 | val 0.48089\n",
      "  Epoch 018 - train 0.29660 | val 0.49635\n",
      "  Regression -> MSE: 0.000233, MAE: 0.009860, R²: -0.4201\n",
      "  Directional -> Accuracy: 0.4483, MCC: 0.0000, F1: 0.0000\n",
      "\n",
      "[49/88] Processing MDT...\n",
      "\n",
      "Processing MDT (Healthcare)...\n",
      "  Epoch 010 - train 0.32746 | val 0.23709\n",
      "  Epoch 020 - train 0.29789 | val 0.24422\n",
      "  Epoch 022 - train 0.29868 | val 0.24027\n",
      "  Regression -> MSE: 0.000224, MAE: 0.011842, R²: 0.0453\n",
      "  Directional -> Accuracy: 0.5814, MCC: 0.1829, F1: 0.7097\n",
      "\n",
      "[50/88] Processing MMM...\n",
      "\n",
      "Processing MMM (Industrial Goods)...\n",
      "  Epoch 010 - train 0.34084 | val 0.82759\n",
      "  Epoch 016 - train 0.31907 | val 0.86139\n",
      "  Regression -> MSE: 0.000249, MAE: 0.010443, R²: 0.0065\n",
      "  Directional -> Accuracy: 0.5111, MCC: 0.0000, F1: 0.6765\n",
      "\n",
      "[51/88] Processing MO...\n",
      "\n",
      "Processing MO (Consumer Goods)...\n",
      "  Epoch 010 - train 0.32139 | val 0.94554\n",
      "  Epoch 017 - train 0.30300 | val 1.13902\n",
      "  Regression -> MSE: 0.000136, MAE: 0.008151, R²: -0.0190\n",
      "  Directional -> Accuracy: 0.5333, MCC: 0.0000, F1: 0.6957\n",
      "\n",
      "[52/88] Processing MRK...\n",
      "\n",
      "Processing MRK (Healthcare)...\n",
      "  Epoch 010 - train 0.31071 | val 0.68101\n",
      "  Epoch 019 - train 0.27734 | val 0.69848\n",
      "  Regression -> MSE: 0.000195, MAE: 0.010843, R²: -0.0350\n",
      "  Directional -> Accuracy: 0.5294, MCC: 0.0000, F1: 0.6923\n",
      "\n",
      "[53/88] Processing MSFT...\n",
      "\n",
      "Processing MSFT (Technology)...\n",
      "  Epoch 010 - train 0.29214 | val 0.51540\n",
      "  Epoch 018 - train 0.26903 | val 0.52913\n",
      "  Regression -> MSE: 0.000315, MAE: 0.011872, R²: -0.2172\n",
      "  Directional -> Accuracy: 0.5135, MCC: 0.2161, F1: 0.2800\n",
      "\n",
      "[54/88] Processing NEE...\n",
      "\n",
      "Processing NEE (Utilities)...\n",
      "Insufficient data for NEE (119 < 163). Skipping...\n",
      "\n",
      "[55/88] Processing NGG...\n",
      "\n",
      "Processing NGG (Utilities)...\n",
      "Insufficient data for NGG (28 < 163). Skipping...\n",
      "\n",
      "[56/88] Processing NVS...\n",
      "\n",
      "Processing NVS (Healthcare)...\n",
      "  Epoch 010 - train 0.29899 | val 0.35367\n",
      "  Epoch 018 - train 0.28809 | val 0.43111\n",
      "  Regression -> MSE: 0.000159, MAE: 0.010098, R²: 0.0206\n",
      "  Directional -> Accuracy: 0.4872, MCC: 0.0000, F1: 0.6552\n",
      "\n",
      "[57/88] Processing ORCL...\n",
      "\n",
      "Processing ORCL (Technology)...\n",
      "  Epoch 010 - train 0.29562 | val 0.66411\n",
      "  Epoch 017 - train 0.26995 | val 0.75154\n",
      "  Regression -> MSE: 0.000193, MAE: 0.010952, R²: -0.0576\n",
      "  Directional -> Accuracy: 0.4667, MCC: 0.0000, F1: 0.6364\n",
      "\n",
      "[58/88] Processing PCG...\n",
      "\n",
      "Processing PCG (Utilities)...\n",
      "Insufficient data for PCG (101 < 163). Skipping...\n",
      "\n",
      "[59/88] Processing PCLN...\n",
      "\n",
      "Processing PCLN (Services)...\n",
      "  Epoch 010 - train 0.29878 | val 0.46788\n",
      "  Epoch 018 - train 0.30043 | val 0.47526\n",
      "  Regression -> MSE: 0.000565, MAE: 0.019483, R²: -0.6187\n",
      "  Directional -> Accuracy: 0.4265, MCC: 0.0000, F1: 0.0000\n",
      "\n",
      "[60/88] Processing PEP...\n",
      "\n",
      "Processing PEP (Consumer Goods)...\n",
      "  Epoch 010 - train 0.32344 | val 0.26724\n",
      "  Epoch 020 - train 0.27131 | val 0.36346\n",
      "  Regression -> MSE: 0.000131, MAE: 0.008206, R²: -0.2601\n",
      "  Directional -> Accuracy: 0.6136, MCC: 0.2624, F1: 0.4848\n",
      "\n",
      "[61/88] Processing PFE...\n",
      "\n",
      "Processing PFE (Healthcare)...\n",
      "  Epoch 010 - train 0.33490 | val 0.93304\n",
      "  Epoch 016 - train 0.31716 | val 1.03498\n",
      "  Regression -> MSE: 0.000216, MAE: 0.011223, R²: -0.0045\n",
      "  Directional -> Accuracy: 0.4828, MCC: -0.0988, F1: 0.2105\n",
      "\n",
      "[62/88] Processing PG...\n",
      "\n",
      "Processing PG (Consumer Goods)...\n",
      "  Epoch 010 - train 0.30630 | val 1.24632\n",
      "  Epoch 016 - train 0.29194 | val 1.25758\n",
      "  Regression -> MSE: 0.000086, MAE: 0.007290, R²: -0.0871\n",
      "  Directional -> Accuracy: 0.6042, MCC: 0.0896, F1: 0.7164\n",
      "\n",
      "[63/88] Processing PICO...\n",
      "\n",
      "Processing PICO (Conglomerates)...\n",
      "Insufficient data for PICO (16 < 163). Skipping...\n",
      "\n",
      "[64/88] Processing PM...\n",
      "\n",
      "Processing PM (Consumer Goods)...\n",
      "Insufficient data for PM (145 < 163). Skipping...\n",
      "\n",
      "[65/88] Processing PPL...\n",
      "\n",
      "Processing PPL (Utilities)...\n",
      "Insufficient data for PPL (108 < 163). Skipping...\n",
      "\n",
      "[66/88] Processing PTR...\n",
      "\n",
      "Processing PTR (Basic Matierials)...\n",
      "Insufficient data for PTR (44 < 163). Skipping...\n",
      "\n",
      "[67/88] Processing RDS-B...\n",
      "\n",
      "Processing RDS-B (Basic Matierials)...\n",
      "Insufficient data for RDS-B (3 < 163). Skipping...\n",
      "\n",
      "[68/88] Processing REX...\n",
      "\n",
      "Processing REX (Conglomerates)...\n",
      "Insufficient data for REX (70 < 163). Skipping...\n",
      "\n",
      "[69/88] Processing SLB...\n",
      "\n",
      "Processing SLB (Basic Matierials)...\n",
      "  Epoch 010 - train 0.32217 | val 0.32311\n",
      "  Epoch 019 - train 0.30997 | val 0.33773\n",
      "  Regression -> MSE: 0.000341, MAE: 0.015294, R²: -0.0383\n",
      "  Directional -> Accuracy: 0.3500, MCC: 0.0000, F1: 0.5185\n",
      "\n",
      "[70/88] Processing SNP...\n",
      "\n",
      "Processing SNP (Basic Matierials)...\n",
      "Insufficient data for SNP (23 < 163). Skipping...\n",
      "\n",
      "[71/88] Processing SNY...\n",
      "\n",
      "Processing SNY (Healthcare)...\n",
      "Insufficient data for SNY (143 < 163). Skipping...\n",
      "\n",
      "[72/88] Processing SO...\n",
      "\n",
      "Processing SO (Utilities)...\n",
      "  Epoch 010 - train 0.31160 | val 0.23988\n",
      "  Epoch 018 - train 0.26983 | val 0.27862\n",
      "  Regression -> MSE: 0.000247, MAE: 0.011194, R²: -0.0365\n",
      "  Directional -> Accuracy: 0.4872, MCC: -0.0882, F1: 0.6154\n",
      "\n",
      "[73/88] Processing SPLP...\n",
      "\n",
      "Processing SPLP (Conglomerates)...\n",
      "Insufficient data for SPLP (5 < 163). Skipping...\n",
      "\n",
      "[74/88] Processing SRE...\n",
      "\n",
      "Processing SRE (Utilities)...\n",
      "Insufficient data for SRE (69 < 163). Skipping...\n",
      "\n",
      "[75/88] Processing T...\n",
      "\n",
      "Processing T (Technology)...\n",
      "  Epoch 010 - train 0.32333 | val 0.81693\n",
      "  Epoch 018 - train 0.28668 | val 1.01467\n",
      "  Regression -> MSE: 0.000079, MAE: 0.006995, R²: 0.0121\n",
      "  Directional -> Accuracy: 0.5263, MCC: 0.1179, F1: 0.6727\n",
      "\n",
      "[76/88] Processing TM...\n",
      "\n",
      "Processing TM (Consumer Goods)...\n",
      "Insufficient data for TM (117 < 163). Skipping...\n",
      "\n",
      "[77/88] Processing TOT...\n",
      "\n",
      "Processing TOT (Basic Matierials)...\n",
      "Insufficient data for TOT (93 < 163). Skipping...\n",
      "\n",
      "[78/88] Processing TSM...\n",
      "\n",
      "Processing TSM (Technology)...\n",
      "Insufficient data for TSM (87 < 163). Skipping...\n",
      "\n",
      "[79/88] Processing UL...\n",
      "\n",
      "Processing UL (Consumer Goods)...\n",
      "Insufficient data for UL (61 < 163). Skipping...\n",
      "\n",
      "[80/88] Processing UN...\n",
      "\n",
      "Processing UN (Consumer Goods)...\n",
      "Insufficient data for UN (54 < 163). Skipping...\n",
      "\n",
      "[81/88] Processing UNH...\n",
      "\n",
      "Processing UNH (Healthcare)...\n",
      "  Epoch 010 - train 0.34030 | val 0.45700\n",
      "  Epoch 018 - train 0.33192 | val 0.46123\n",
      "  Regression -> MSE: 0.000357, MAE: 0.015222, R²: 0.0071\n",
      "  Directional -> Accuracy: 0.5417, MCC: 0.1508, F1: 0.2143\n",
      "\n",
      "[82/88] Processing UPS...\n",
      "\n",
      "Processing UPS (Services)...\n",
      "  Epoch 010 - train 0.29795 | val 0.51767\n",
      "  Epoch 019 - train 0.26900 | val 0.54413\n",
      "  Regression -> MSE: 0.000098, MAE: 0.007146, R²: -0.0179\n",
      "  Directional -> Accuracy: 0.5556, MCC: 0.0000, F1: 0.7143\n",
      "\n",
      "[83/88] Processing UTX...\n",
      "\n",
      "Processing UTX (Industrial Goods)...\n",
      "  Epoch 010 - train 0.32802 | val 0.78155\n",
      "  Epoch 017 - train 0.29710 | val 0.81660\n",
      "  Regression -> MSE: 0.000201, MAE: 0.011335, R²: -0.1099\n",
      "  Directional -> Accuracy: 0.3659, MCC: 0.0000, F1: 0.5357\n",
      "\n",
      "[84/88] Processing V...\n",
      "\n",
      "Processing V (Financial)...\n",
      "  Epoch 010 - train 0.29386 | val 1.03410\n",
      "  Epoch 016 - train 0.29421 | val 0.89552\n",
      "  Regression -> MSE: 0.000252, MAE: 0.012434, R²: -0.1477\n",
      "  Directional -> Accuracy: 0.4561, MCC: 0.0000, F1: 0.0000\n",
      "\n",
      "[85/88] Processing VZ...\n",
      "\n",
      "Processing VZ (Technology)...\n",
      "  Epoch 010 - train 0.33509 | val 1.06753\n",
      "  Epoch 016 - train 0.28763 | val 1.35440\n",
      "  Regression -> MSE: 0.000096, MAE: 0.007954, R²: -0.0513\n",
      "  Directional -> Accuracy: 0.4828, MCC: 0.0000, F1: 0.6512\n",
      "\n",
      "[86/88] Processing WFC...\n",
      "\n",
      "Processing WFC (Financial)...\n",
      "  Epoch 010 - train 0.36613 | val 1.49075\n",
      "  Epoch 016 - train 0.34478 | val 1.40764\n",
      "  Regression -> MSE: 0.000159, MAE: 0.009910, R²: 0.0087\n",
      "  Directional -> Accuracy: 0.6000, MCC: 0.2013, F1: 0.6071\n",
      "\n",
      "[87/88] Processing WMT...\n",
      "\n",
      "Processing WMT (Services)...\n",
      "  Epoch 010 - train 0.32741 | val 1.29091\n",
      "  Epoch 016 - train 0.30460 | val 1.01825\n",
      "  Regression -> MSE: 0.000115, MAE: 0.008812, R²: -0.0106\n",
      "  Directional -> Accuracy: 0.5088, MCC: 0.0000, F1: 0.6744\n",
      "\n",
      "[88/88] Processing XOM...\n",
      "\n",
      "Processing XOM (Basic Matierials)...\n",
      "  Epoch 010 - train 0.35127 | val 0.92293\n",
      "  Epoch 016 - train 0.33393 | val 0.84482\n",
      "  Regression -> MSE: 0.000233, MAE: 0.012150, R²: 0.0006\n",
      "  Directional -> Accuracy: 0.5645, MCC: 0.1507, F1: 0.3077\n",
      "\n",
      "================================================================================\n",
      "Pipeline completed: 50/88 companies processed successfully\n",
      "================================================================================\n",
      "Loss curves saved to results/benchmarking/regression/BiGRU_loss_curves.csv\n",
      "\n",
      "================================================================================\n",
      "STOCK PREDICTION PIPELINE RESULTS\n",
      "================================================================================\n",
      "Model: BiGRU | Problem: regression\n",
      "Companies analyzed: 50\n",
      "Average samples per company: 320\n",
      "\n",
      "==================================================\n",
      "OVERALL PERFORMANCE\n",
      "==================================================\n",
      "Mean Squared Error:     0.000259 (±0.000150)\n",
      "Mean Absolute Error:    0.011967 (±0.003492)\n",
      "R² Score:              -0.0981 (±0.2490)\n",
      "Directional Accuracy:   0.5079 (±0.0822)\n",
      "Matthews Correlation:   0.0604 (±0.1171)\n",
      "F1 Score:              0.4649 (±0.2644)\n",
      "Precision:             0.4685 (±0.2424)\n",
      "Recall:                0.6028 (±0.4066)\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE BY SECTOR\n",
      "==================================================\n",
      "Healthcare           | Acc: 0.550±0.051 | MCC: 0.085 | Companies: 9\n",
      "Consumer Goods       | Acc: 0.540±0.068 | MCC: 0.076 | Companies: 5\n",
      "Utilities            | Acc: 0.518±0.044 | MCC: -0.000 | Companies: 2\n",
      "Technology           | Acc: 0.513±0.050 | MCC: 0.056 | Companies: 8\n",
      "Financial            | Acc: 0.509±0.085 | MCC: 0.049 | Companies: 6\n",
      "Industrial Goods     | Acc: 0.496±0.151 | MCC: 0.113 | Companies: 6\n",
      "Services             | Acc: 0.487±0.072 | MCC: 0.035 | Companies: 9\n",
      "Basic Matierials     | Acc: 0.438±0.084 | MCC: 0.030 | Companies: 5\n",
      "\n",
      "==================================================\n",
      "TOP 10 PERFORMERS (by Directional Accuracy)\n",
      "==================================================\n",
      "BA                   | Industrial Goods | Acc: 0.788 | MCC: 0.575\n",
      "CELG                 | Healthcare      | Acc: 0.644 | MCC: 0.294\n",
      "PEP                  | Consumer Goods  | Acc: 0.614 | MCC: 0.262\n",
      "FB                   | Technology      | Acc: 0.610 | MCC: 0.000\n",
      "PG                   | Consumer Goods  | Acc: 0.604 | MCC: 0.090\n",
      "BAC                  | Financial       | Acc: 0.600 | MCC: 0.000\n",
      "DIS                  | Services        | Acc: 0.600 | MCC: 0.269\n",
      "WFC                  | Financial       | Acc: 0.600 | MCC: 0.201\n",
      "JNJ                  | Healthcare      | Acc: 0.593 | MCC: 0.186\n",
      "MDT                  | Healthcare      | Acc: 0.581 | MCC: 0.183\n",
      "Results saved to results/benchmarking/regression/BiGRU.csv\n",
      "\n",
      "Displaying first 5 rows of BiGRU results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>sector</th>\n",
       "      <th>model_type</th>\n",
       "      <th>problem_type</th>\n",
       "      <th>horizon_steps</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "      <th>mcc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>directional_accuracy</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>train_samples</th>\n",
       "      <th>val_samples</th>\n",
       "      <th>test_samples</th>\n",
       "      <th>epochs_trained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>BiGRU</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.012697</td>\n",
       "      <td>-0.032047</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>468</td>\n",
       "      <td>312</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>BiGRU</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.018892</td>\n",
       "      <td>0.034178</td>\n",
       "      <td>0.048457</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>283</td>\n",
       "      <td>189</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMGN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>BiGRU</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.015233</td>\n",
       "      <td>-0.005155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>293</td>\n",
       "      <td>196</td>\n",
       "      <td>49</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Services</td>\n",
       "      <td>BiGRU</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>-0.228660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460526</td>\n",
       "      <td>458</td>\n",
       "      <td>306</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BA</td>\n",
       "      <td>Industrial Goods</td>\n",
       "      <td>BiGRU</td>\n",
       "      <td>regression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>0.574524</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>315</td>\n",
       "      <td>211</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company            sector model_type problem_type  horizon_steps       mse  \\\n",
       "0    AAPL    Consumer Goods      BiGRU   regression              1  0.000253   \n",
       "1    ABBV        Healthcare      BiGRU   regression              1  0.000668   \n",
       "2    AMGN        Healthcare      BiGRU   regression              1  0.000347   \n",
       "3    AMZN          Services      BiGRU   regression              1  0.000424   \n",
       "4      BA  Industrial Goods      BiGRU   regression              1  0.000188   \n",
       "\n",
       "        mae        r2       mcc        f1  precision    recall  \\\n",
       "0  0.012697 -0.032047  0.004706  0.568421   0.450000  0.771429   \n",
       "1  0.018892  0.034178  0.048457  0.676923   0.564103  0.846154   \n",
       "2  0.015233 -0.005155  0.000000  0.702703   0.541667  1.000000   \n",
       "3  0.016105 -0.228660  0.000000  0.000000   0.000000  0.000000   \n",
       "4  0.009059  0.061475  0.574524  0.813559   0.774194  0.857143   \n",
       "\n",
       "   directional_accuracy  n_samples  train_samples  val_samples  test_samples  \\\n",
       "0              0.474359        468            312           78            78   \n",
       "1              0.553191        283            189           47            47   \n",
       "2              0.541667        293            196           49            48   \n",
       "3              0.460526        458            306           76            76   \n",
       "4              0.788462        315            211           52            52   \n",
       "\n",
       "   epochs_trained  \n",
       "0              17  \n",
       "1              20  \n",
       "2              18  \n",
       "3              16  \n",
       "4              19  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n{'='*25}\\n  RUNNING PIPELINE FOR: BiGRU\\n{'='*25}\\n\")\n",
    "\n",
    "pipeline_BiGRU = StockPredictionPipeline(\n",
    "    df=master_df,\n",
    "    feature_columns=feature_columns,\n",
    "    model_type='BiGRU',\n",
    "    sequence_length=sequence_length,\n",
    "    problem_type='regression',\n",
    "    horizon_steps=1\n",
    ")\n",
    "\n",
    "results_BiGRU = pipeline_BiGRU.run_pipeline()\n",
    "\n",
    "loss_df = pipeline_BiGRU.get_loss_curves_df()\n",
    "\n",
    "pipeline_BiGRU.save_loss_curves('results/benchmarking/')\n",
    "\n",
    "if results_BiGRU is not None and not results_BiGRU.empty:\n",
    "    analysis_BiGRU = pipeline_BiGRU.analyze_results()\n",
    "    pipeline_BiGRU.save_results(results_BiGRU, output_dir='results/benchmarking/')\n",
    "\n",
    "    all_pipelines[\"BiGRU\"] = pipeline_BiGRU\n",
    "    all_results_dfs[\"BiGRU\"] = results_BiGRU\n",
    "    all_analyses[\"BiGRU\"] = analysis_BiGRU\n",
    "\n",
    "    print(\"\\nDisplaying first 5 rows of BiGRU results:\")\n",
    "    display(results_BiGRU.head())\n",
    "else:\n",
    "    print(f\"\\n[FAILED] Pipeline for BiGRU did not produce any results.\")\n",
    "\n",
    "del pipeline_BiGRU"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
